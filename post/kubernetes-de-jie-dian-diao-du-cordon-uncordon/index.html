<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title> kubernetes的节点调度cordon uncordon drain | OrochW&#39;s Blog</title>
<link rel="shortcut icon" href="https://orochw.github.io/favicon.ico?v=1757257179686">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://orochw.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title=" kubernetes的节点调度cordon uncordon drain | OrochW&#39;s Blog - Atom Feed" href="https://orochw.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-133741629-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-133741629-1');
</script>


    <meta name="description" content="1.cordon的使用
使用此命令后。被cordon的节点不参与以后的pod调度，如果需要重新开始调度需要uncordon命令来解除
[root@master-etcd1-234-101 ~]# kubectl get nodes
NAME..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://orochw.github.io">
  <img class="avatar" src="https://orochw.github.io/images/avatar.png?v=1757257179686" alt="">
  </a>
  <h1 class="site-title">
    OrochW&#39;s Blog
  </h1>
  <p class="site-description">
    
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/OrochW" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
               kubernetes的节点调度cordon uncordon drain
            </h2>
            <div class="post-info">
              <span>
                2022-09-16
              </span>
              <span>
                11 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h2 id="1cordon的使用">1.cordon的使用</h2>
<p>使用此命令后。被cordon的节点不参与以后的pod调度，如果需要重新开始调度需要uncordon命令来解除</p>
<pre><code class="language-shell">[root@master-etcd1-234-101 ~]# kubectl get nodes
NAME              STATUS                     ROLES    AGE     VERSION
192.168.234.101   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.102   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.103   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.111   Ready                      node     2d19h   v1.22.2
192.168.234.112   Ready                      node     2d19h   v1.22.2
192.168.234.113   Ready                      node     2d19h   v1.22.2
[root@master-etcd1-234-101 ~]# kubectl cordon  192.168.234.113
node/192.168.234.113 cordoned
[root@master-etcd1-234-101 ~]# kubectl get nodes
NAME              STATUS                     ROLES    AGE     VERSION
192.168.234.101   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.102   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.103   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.111   Ready                      node     2d19h   v1.22.2
192.168.234.112   Ready                      node     2d19h   v1.22.2
192.168.234.113   Ready,SchedulingDisabled   node     2d19h   v1.22.2
[root@master-etcd1-234-101 ~]# kubectl uncordon  192.168.234.113
node/192.168.234.113 uncordoned
[root@master-etcd1-234-101 ~]# kubectl get nodes
NAME              STATUS                     ROLES    AGE     VERSION
192.168.234.101   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.102   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.103   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.111   Ready                      node     2d19h   v1.22.2
192.168.234.112   Ready                      node     2d19h   v1.22.2
192.168.234.113   Ready                      node     2d19h   v1.22.2
[root@master-etcd1-234-101 ~]#

</code></pre>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20220916174549.png" alt="" loading="lazy"></figure>
<h2 id="2drain驱逐命令的使用">2.drain驱逐命令的使用</h2>
<pre><code class="language-shell">[root@master-etcd1-234-101 yaml]# kubectl get pod -o wide -n n60
NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE              NOMINATED NODE   READINESS GATES
n60-nginx-deployment-6b85b6d695-49zxx   1/1     Running   0          53s     10.200.199.131   192.168.234.113   &lt;none&gt;           &lt;none&gt;
n60-nginx-deployment-6b85b6d695-6gjrf   1/1     Running   0          2m14s   10.200.215.193   192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60-nginx-deployment-6b85b6d695-nclps   1/1     Running   0          53s     10.200.144.195   192.168.234.111   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]# kubectl drain 192.168.234.113
node/192.168.234.113 cordoned
DEPRECATED WARNING: Aborting the drain command in a list of nodes will be deprecated in v1.23.
The new behavior will make the drain command go through all nodes even if one or more nodes failed during the drain.
For now, users can try such experience via: --ignore-errors
error: unable to drain node &quot;192.168.234.113&quot;, aborting command...

There are pending nodes to be drained:
 192.168.234.113
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-gzptk, kube-system/node-local-dns-fhd2m
cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/kubernetes-dashboard-65b659dd64-9v77k
[root@master-etcd1-234-101 yaml]# kubectl drain 192.168.234.113 --ignore-daemonsets
node/192.168.234.113 already cordoned
DEPRECATED WARNING: Aborting the drain command in a list of nodes will be deprecated in v1.23.
The new behavior will make the drain command go through all nodes even if one or more nodes failed during the drain.
For now, users can try such experience via: --ignore-errors
error: unable to drain node &quot;192.168.234.113&quot;, aborting command...

There are pending nodes to be drained:
 192.168.234.113
error: cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/kubernetes-dashboard-65b659dd64-9v77k
[root@master-etcd1-234-101 yaml]# kubectl drain 192.168.234.113 --ignore-daemonsets  --delete-emptydir-data
node/192.168.234.113 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-gzptk, kube-system/node-local-dns-fhd2m
evicting pod n60/n60-nginx-deployment-6b85b6d695-49zxx
evicting pod kube-system/coredns-67cb59d684-2x9gb
evicting pod kube-system/kubernetes-dashboard-65b659dd64-9v77k
pod/n60-nginx-deployment-6b85b6d695-49zxx evicted
pod/kubernetes-dashboard-65b659dd64-9v77k evicted
pod/coredns-67cb59d684-2x9gb evicted
node/192.168.234.113 evicted
[root@master-etcd1-234-101 yaml]# kubectl get pod -o wide -n n60
NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE              NOMINATED NODE   READINESS GATES
n60-nginx-deployment-6b85b6d695-6gjrf   1/1     Running   0          7m25s   10.200.215.193   192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60-nginx-deployment-6b85b6d695-g97bt   1/1     Running   0          14s     10.200.215.194   192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60-nginx-deployment-6b85b6d695-nclps   1/1     Running   0          6m4s    10.200.144.195   192.168.234.111   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]# kubectl get nodes
NAME              STATUS                     ROLES    AGE     VERSION
192.168.234.101   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.102   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.103   Ready,SchedulingDisabled   master   2d19h   v1.22.2
192.168.234.111   Ready                      node     2d19h   v1.22.2
192.168.234.112   Ready                      node     2d19h   v1.22.2
192.168.234.113   Ready,SchedulingDisabled   node     2d19h   v1.22.2
[root@master-etcd1-234-101 yaml]# kubectl get pod -A -o wide
NAMESPACE     NAME                                         READY   STATUS    RESTARTS      AGE     IP                NODE              NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-59df8b6856-qrv46     1/1     Running   1 (29h ago)   2d20h   192.168.234.112   192.168.234.112   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-2pwn9                            1/1     Running   0             2d20h   192.168.234.111   192.168.234.111   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-4crxn                            1/1     Running   0             2d20h   192.168.234.102   192.168.234.102   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-6tttd                            1/1     Running   4 (29h ago)   2d20h   192.168.234.101   192.168.234.101   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-gzptk                            1/1     Running   0             2d20h   192.168.234.113   192.168.234.113   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-lbspc                            1/1     Running   0             2d20h   192.168.234.112   192.168.234.112   &lt;none&gt;           &lt;none&gt;
kube-system   calico-node-tw8nr                            1/1     Running   0             2d20h   192.168.234.103   192.168.234.103   &lt;none&gt;           &lt;none&gt;
kube-system   coredns-67cb59d684-zk7x2                     1/1     Running   0             31m     10.200.215.195    192.168.234.112   &lt;none&gt;           &lt;none&gt;
kube-system   dashboard-metrics-scraper-856586f554-c97wb   1/1     Running   0             29h     10.200.144.194    192.168.234.111   &lt;none&gt;           &lt;none&gt;
kube-system   kubernetes-dashboard-65b659dd64-g9nkz        1/1     Running   0             31m     10.200.144.196    192.168.234.111   &lt;none&gt;           &lt;none&gt;
kube-system   metrics-server-7d567f6489-njqv5              1/1     Running   0             29h     10.200.144.193    192.168.234.111   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-5gz22                         1/1     Running   0             29h     192.168.234.103   192.168.234.103   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-bbqkh                         1/1     Running   0             29h     192.168.234.102   192.168.234.102   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-fhd2m                         1/1     Running   0             29h     192.168.234.113   192.168.234.113   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-j8cgt                         1/1     Running   0             29h     192.168.234.101   192.168.234.101   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-sh6f5                         1/1     Running   0             29h     192.168.234.111   192.168.234.111   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-z72np                         1/1     Running   0             29h     192.168.234.112   192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60           n60-nginx-deployment-6b85b6d695-6gjrf        1/1     Running   0             38m     10.200.215.193    192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60           n60-nginx-deployment-6b85b6d695-g97bt        1/1     Running   0             31m     10.200.215.194    192.168.234.112   &lt;none&gt;           &lt;none&gt;
n60           n60-nginx-deployment-6b85b6d695-nclps        1/1     Running   0             37m     10.200.144.195    192.168.234.111   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]# kubectl get pod -A -o wide |grep 113
kube-system   calico-node-gzptk                            1/1     Running   0             2d20h   192.168.234.113   192.168.234.113   &lt;none&gt;           &lt;none&gt;
kube-system   node-local-dns-fhd2m                         1/1     Running   0             29h     192.168.234.113   192.168.234.113   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]#

</code></pre>
<p>uncordon之后一样的可以调度了</p>
<pre><code class="language-SHELL">[root@master-etcd1-234-101 yaml]# kubectl uncordon 192.168.234.113
node/192.168.234.113 uncordoned
[root@master-etcd1-234-101 yaml]# kubectl get pod -A -o wide
NAMESPACE     NAME                                         READY   STATUS    RESTARTS      AGE     IP                NODE              NOMINATED NODE   READINESS GATES
n60           n60-nginx-deployment-6b85b6d695-98jg5        1/1     Running   0             26s     10.200.199.132    192.168.234.113   &lt;none&gt;           &lt;none&gt;
n60           n60-nginx-deployment-6b85b6d695-m72bb        1/1     Running   0             26s     10.200.199.134    192.168.234.113   &lt;none&gt;           &lt;none&gt;
n60           n60-nginx-deployment-6b85b6d695-s2gxn        1/1     Running   0             26s     10.200.199.133    192.168.234.113   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]# kubectl apply -f nginx.yaml
deployment.apps/n60-nginx-deployment configured
service/n60-nginx-service unchanged
[root@master-etcd1-234-101 yaml]# kubectl get pod -A -o wide
NAMESPACE     NAME                                         READY   STATUS    RESTARTS      AGE     IP                NODE              NOMINATED NODE   READINESS GATES
n60           n60-nginx-deployment-6b85b6d695-s2gxn        1/1     Running   0             34s     10.200.199.133    192.168.234.113   &lt;none&gt;           &lt;none&gt;
[root@master-etcd1-234-101 yaml]#
</code></pre>
<p><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20220916181027.png" alt="" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20220916184549.png" alt="" loading="lazy"></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#1cordon%E7%9A%84%E4%BD%BF%E7%94%A8">1.cordon的使用</a></li>
<li><a href="#2drain%E9%A9%B1%E9%80%90%E5%91%BD%E4%BB%A4%E7%9A%84%E4%BD%BF%E7%94%A8">2.drain驱逐命令的使用</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://orochw.github.io/post/cadvisor/">
              <h3 class="post-title">
                cadvisor
              </h3>
            </a>
          </div>
        

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'e16544ded74efb82c5cb',
    clientSecret: 'cc815ef9f98d5cf6d79ae6f9ca0f9fc2b521645a',
    repo: 'Gitalk',
    owner: 'OrochW',
    admin: ['OrochW'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://orochw.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
