<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://orochw.github.io</id>
    <title>OrochW&apos;s Blog</title>
    <updated>2025-09-07T14:59:46.407Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://orochw.github.io"/>
    <link rel="self" href="https://orochw.github.io/atom.xml"/>
    <logo>https://orochw.github.io/images/avatar.png</logo>
    <icon>https://orochw.github.io/favicon.ico</icon>
    <rights>All rights reserved 2025, OrochW&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kafka KRaft 模式安装与使用指南（无需 Zookeeper）]]></title>
        <id>https://orochw.github.io/post/kafka-kraft-mo-shi-an-zhuang-yu-shi-yong-zhi-nan-wu-xu-zookeeper/</id>
        <link href="https://orochw.github.io/post/kafka-kraft-mo-shi-an-zhuang-yu-shi-yong-zhi-nan-wu-xu-zookeeper/">
        </link>
        <updated>2025-09-02T10:37:30.000Z</updated>
        <content type="html"><![CDATA[<p>以下是 <strong>完整、按流程顺序重写的 Kafka KRaft 集群搭建文档</strong>，包含从零开始的完整初始化流程。</p>
<hr>
<h1 id="kafka-kraft-模式集群搭建完整流程无需-zookeeper"><strong>Kafka KRaft 模式集群搭建完整流程（无需 Zookeeper）</strong></h1>
<blockquote>
<p><strong>适用版本：Kafka 2.13-4.0.0（KRaft 模式）</strong><br>
本文档详细介绍如何在 <strong>不依赖 Zookeeper</strong> 的情况下，使用 <strong>KRaft（Kafka Raft Metadata）模式</strong> 搭建一个 3 节点高可用 Kafka 集群。<br>
包含 <strong>元数据初始化、格式化、启动、验证</strong> 全流程。</p>
</blockquote>
<hr>
<h2 id="一-集群规划3-节点示例">🌐 一、集群规划（3 节点示例）</h2>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP 地址</th>
<th><code>node.id</code></th>
<th>角色</th>
</tr>
</thead>
<tbody>
<tr>
<td>kafka1</td>
<td>172.31.7.107</td>
<td>1</td>
<td>broker + controller</td>
</tr>
<tr>
<td>kafka2</td>
<td>172.31.7.108</td>
<td>2</td>
<td>broker + controller</td>
</tr>
<tr>
<td>kafka3</td>
<td>172.31.7.109</td>
<td>3</td>
<td>broker + controller</td>
</tr>
</tbody>
</table>
<blockquote>
<p>✅ 所有节点均使用 <strong>混合角色模式</strong>（broker + controller）</p>
</blockquote>
<hr>
<h2 id="二-环境准备">🔧 二、环境准备</h2>
<h3 id="1-安装-openjdk-17推荐">1. 安装 OpenJDK 17（推荐）</h3>
<pre><code class="language-bash"># Ubuntu/Debian
sudo apt update &amp;&amp; sudo apt install openjdk-17-jdk -y

# CentOS/RHEL
sudo yum install java-17-openjdk-devel -y
</code></pre>
<p>验证：</p>
<pre><code class="language-bash">java -version
</code></pre>
<blockquote>
<p>⚠️ <strong>注意</strong>：OpenJDK 11 在 Kafka 4.0.0 中可能出现兼容性问题，<strong>强烈推荐使用 OpenJDK 17</strong>。</p>
</blockquote>
<hr>
<h2 id="三-下载与解压-kafka">📦 三、下载与解压 Kafka</h2>
<p>在所有节点执行：</p>
<pre><code class="language-bash">cd /apps
wget https://downloads.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
tar -xzf kafka_2.13-4.0.0.tgz
ln -s kafka_2.13-4.0.0 kafka
</code></pre>
<p>设置环境变量（可选）：</p>
<pre><code class="language-bash">export KAFKA_HOME=/apps/kafka_2.13-4.0.0
export PATH=$KAFKA_HOME/bin:$PATH
</code></pre>
<hr>
<h2 id="️-四-配置文件configserverproperties">⚙️ 四、配置文件：<code>config/server.properties</code></h2>
<p>在所有节点创建或编辑 <code>$KAFKA_HOME/config/server.properties</code>：</p>
<pre><code class="language-properties">############################# Server Basics #############################
process.roles=broker,controller
node.id=__NODE_ID__

############################# Socket Server Settings #############################
listeners=PLAINTEXT://:9092,CONTROLLER://:9093
advertised.listeners=PLAINTEXT://__IP__:9092,CONTROLLER://__IP__:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
inter.broker.listener.name=PLAINTEXT

############################# Log Basics #############################
log.dirs=/apps/kafka_2.13-4.0.0/logs

############################# Controller Quorum #############################
# 三节点控制器投票配置
controller.quorum.voters=1@172.31.7.107:9093,2@172.31.7.108:9093,3@172.31.7.109:9093

############################# Internal Topics #############################
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
</code></pre>
<blockquote>
<p>🔁 <code>__NODE_ID__</code> 和 <code>__IP__</code> 将由脚本自动替换。</p>
</blockquote>
<hr>
<h2 id="五-初始化集群元数据关键步骤">🧰 五、初始化集群元数据（关键步骤！）</h2>
<blockquote>
<p>⚠️ <strong>此步骤必须在首次启动前执行，且只需执行一次</strong></p>
</blockquote>
<h3 id="step-1生成唯一的-clusterid">Step 1：生成唯一的 <code>cluster.id</code></h3>
<p>在任意一台机器上生成集群 ID：</p>
<pre><code class="language-bash">CLUSTER_ID=$(bin/kafka-storage.sh random-uuid)
echo $CLUSTER_ID
</code></pre>
<p>输出示例：</p>
<pre><code>3hzu8188TC68nQcL5A-Y0g
</code></pre>
<blockquote>
<p>✅ 记下这个 <code>cluster.id</code>，所有节点将使用相同的 ID。</p>
</blockquote>
<hr>
<h3 id="step-2格式化存储目录所有节点执行">Step 2：格式化存储目录（所有节点执行）</h3>
<p>在 <strong>每台 Kafka 节点</strong> 上执行格式化命令，使用上一步生成的 <code>cluster.id</code>：</p>
<pre><code class="language-bash">bin/kafka-storage.sh format \
  -t 3hzu8188TC68nQcL5A-Y0g \
  -c /apps/kafka_2.13-4.0.0/config/server.properties \
  --cluster-id 3hzu8188TC68nQcL5A-Y0g
</code></pre>
<blockquote>
<p>🔍 参数说明：</p>
<ul>
<li><code>-t</code>：指定 <code>node.id</code>（每台机器不同）</li>
<li><code>-c</code>：配置文件路径</li>
<li><code>--cluster-id</code>：上一步生成的全局唯一 ID</li>
</ul>
</blockquote>
<h4 id="预期输出">✅ 预期输出：</h4>
<pre><code>Formatting /apps/kafka_2.13-4.0.0/logs with cluster id 3hzu8188TC68nQcL5A-Y0g
</code></pre>
<blockquote>
<p>✅ 此操作会在 <code>log.dirs</code> 目录下创建元数据日志，<strong>每个节点必须单独执行</strong>。</p>
</blockquote>
<hr>
<h2 id="六-启动脚本支持动态-ip-替换">🚀 六、启动脚本（支持动态 IP 替换）</h2>
<h3 id="创建管理脚本binkafka-managersh">创建管理脚本：<code>bin/kafka-manager.sh</code></h3>
<pre><code class="language-bash">#!/bin/bash

# 自动检测 Java
if command -v java &gt;/dev/null 2&gt;&amp;1; then
    JAVA_BIN=$(command -v java)
    JAVA_HOME=$(dirname &quot;$(dirname &quot;$JAVA_BIN&quot;)&quot;)
    export JAVA_HOME
    export PATH=$JAVA_HOME/bin:$PATH
else
    echo &quot;❌ Java not found, please install OpenJDK 17+&quot;
    exit 1
fi

KAFKA_HOME=&quot;/apps/kafka_2.13-4.0.0&quot;
LOG_FILE=&quot;$KAFKA_HOME/logs/kafka.log&quot;
PID_FILE=&quot;$KAFKA_HOME/logs/kafka.pid&quot;
BOOTSTRAP_SERVER=&quot;$(hostname -I | awk '{print $1}'):9092&quot;

get_pid() {
    if [[ -f &quot;$PID_FILE&quot; ]]; then
        pid=$(cat &quot;$PID_FILE&quot;)
        if [[ -n &quot;$pid&quot; &amp;&amp; -d &quot;/proc/$pid&quot; ]]; then echo &quot;$pid&quot;; else rm -f &quot;$PID_FILE&quot;; fi
    else
        pgrep -f &quot;kafka\.Kafka&quot; || echo &quot;&quot;
    fi
}

prepare_config() {
    IP=$(hostname -I | awk '{print $1}')
    sed -i &quot;s|__IP__|$IP|g&quot; config/server.properties
}

start() {
    pid=$(get_pid)
    [[ -n &quot;$pid&quot; ]] &amp;&amp; { echo &quot;❌ Kafka is already running (PID: $pid)&quot;; return 1; }

    prepare_config
    echo &quot;🚀 Starting Kafka server...&quot;
    nohup bin/kafka-server-start.sh config/server.properties &gt; &quot;$LOG_FILE&quot; 2&gt;&amp;1 &amp;
    echo $! &gt; &quot;$PID_FILE&quot;
    disown
    echo &quot;✅ Kafka started, PID: $!&quot;
}

stop() {
    pid=$(get_pid)
    [[ -z &quot;$pid&quot; ]] &amp;&amp; { echo &quot;❌ Kafka is not running.&quot;; return 1; }

    echo &quot;🛑 Stopping Kafka (PID: $pid)...&quot;
    kill -15 &quot;$pid&quot; &amp;&amp; rm -f &quot;$PID_FILE&quot;
    for i in {1..30}; do
        ! kill -0 &quot;$pid&quot; 2&gt;/dev/null &amp;&amp; { echo &quot;✅ Kafka stopped.&quot;; return 0; }
        sleep 1
    done
    echo &quot;⚠️  Force killing...&quot;
    kill -9 &quot;$pid&quot; &amp;&amp; rm -f &quot;$PID_FILE&quot;
    echo &quot;✅ Kafka killed.&quot;
}

status() {
    pid=$(get_pid)
    [[ -n &quot;$pid&quot; ]] &amp;&amp; echo &quot;🟢 Kafka is running (PID: $pid)&quot; || echo &quot;🔴 Kafka is not running.&quot;
}

controller_info() {
    echo &quot;🧠 Controller Quorum Info:&quot;
    bin/kafka-metadata-quorum.sh --bootstrap-server &quot;$BOOTSTRAP_SERVER&quot; describe --status
}

case &quot;$1&quot; in
    start) start ;;
    stop) stop ;;
    restart) stop; sleep 3; start ;;
    status) status ;;
    controller-info) controller_info ;;
    *)
        echo &quot;📌 Usage: $0 {start|stop|restart|status|controller-info}&quot;
        ;;
esac
</code></pre>
<p>赋予执行权限：</p>
<pre><code class="language-bash">chmod +x bin/kafka-manager.sh
</code></pre>
<hr>
<h2 id="️-七-启动集群所有节点">▶️ 七、启动集群（所有节点）</h2>
<p>在每台机器上执行：</p>
<h3 id="1-设置-nodeid">1. 设置 <code>node.id</code></h3>
<pre><code class="language-bash"># 节点1
sed -i 's/__NODE_ID__/1/g' config/server.properties

# 节点2
sed -i 's/__NODE_ID__/2/g' config/server.properties

# 节点3
sed -i 's/__NODE_ID__/3/g' config/server.properties
</code></pre>
<h3 id="2-启动-kafka">2. 启动 Kafka</h3>
<pre><code class="language-bash">bin/kafka-manager.sh start
</code></pre>
<hr>
<h2 id="八-验证集群状态">✅ 八、验证集群状态</h2>
<h3 id="1-检查运行状态">1. 检查运行状态</h3>
<pre><code class="language-bash">bin/kafka-manager.sh status
</code></pre>
<h3 id="2-查看控制器信息任一节点执行">2. 查看控制器信息（任一节点执行）</h3>
<pre><code class="language-bash">bin/kafka-manager.sh controller-info
</code></pre>
<h4 id="实际输出示例来自你的测试">✅ 实际输出示例（来自你的测试）：</h4>
<pre><code>🧠 Controller Quorum Info:
ClusterId:              3hzu8188TC68nQcL5A-Y0g
LeaderId:               1
LeaderEpoch:            1
HighWatermark:          235
MaxFollowerLag:         0
MaxFollowerLagTimeMs:   0
CurrentVoters:          [{&quot;id&quot;: 1, &quot;directoryId&quot;: null, &quot;endpoints&quot;: [&quot;CONTROLLER://172.31.7.107:9093&quot;]}, {&quot;id&quot;: 2, &quot;directoryId&quot;: null, &quot;endpoints&quot;: [&quot;CONTROLLER://172.31.7.108:9093&quot;]}, {&quot;id&quot;: 3, &quot;directoryId&quot;: null, &quot;endpoints&quot;: [&quot;CONTROLLER://172.31.7.109:9093&quot;]}]
CurrentObservers:       []
</code></pre>
<blockquote>
<p>✅ <strong>健康标志</strong>：</p>
<ul>
<li><code>LeaderId</code> 存在</li>
<li><code>MaxFollowerLag: 0</code></li>
<li><code>CurrentVoters</code> 包含所有 3 个节点</li>
</ul>
</blockquote>
<hr>
<h2 id="九-常见问题">📌 九、常见问题</h2>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>kafka-storage.sh: command not found</code></td>
<td>路径错误</td>
<td>使用 <code>bin/kafka-storage.sh</code></td>
</tr>
<tr>
<td>格式化失败</td>
<td><code>node.id</code> 或 <code>cluster.id</code> 错误</td>
<td>检查参数</td>
</tr>
<tr>
<td>节点无法加入</td>
<td>网络不通或端口被占用</td>
<td>检查 <code>9092/9093</code> 端口</td>
</tr>
<tr>
<td>启动崩溃</td>
<td>OpenJDK 版本问题</td>
<td>升级到 <strong>OpenJDK 17</strong></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="十-总结">🎉 十、总结</h2>
<p>✅ <strong>KRaft 模式初始化完整流程</strong>：</p>
<ol>
<li>安装 OpenJDK 17</li>
<li>下载 Kafka 并配置 <code>server.properties</code></li>
<li><strong>生成 <code>cluster.id</code>：<code>bin/kafka-storage.sh random-uuid</code></strong></li>
<li><strong>格式化存储：<code>bin/kafka-storage.sh format -t &lt;node.id&gt; --cluster-id &lt;id&gt;</code></strong></li>
<li>设置 <code>node.id</code> 并启动</li>
<li>使用 <code>kafka-metadata-quorum.sh describe --status</code> 验证</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Redis 4.0.14 集群部署与排错手册]]></title>
        <id>https://orochw.github.io/post/kubernetes-redis-4014-ji-qun-bu-shu-yu-pai-cuo-shou-ce/</id>
        <link href="https://orochw.github.io/post/kubernetes-redis-4014-ji-qun-bu-shu-yu-pai-cuo-shou-ce/">
        </link>
        <updated>2025-08-12T21:15:07.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubernetes-redis-4014-集群部署与排错手册真实验证版">📚 <strong>Kubernetes Redis 4.0.14 集群部署与排错手册（真实验证版）</strong></h1>
<blockquote>
<p><strong>命名空间：cka</strong> | 版本：v9.0（含真实命令验证）</p>
</blockquote>
<hr>
<h2 id="1️⃣-第一部分引言">1️⃣ 第一部分：引言</h2>
<p>本文档记录了在 Kubernetes 集群中，<strong>使用原始配置文件</strong>（<code>pv</code>、<code>redis.conf</code>、<code>redis-cluster.yaml</code>）部署 Redis 4.0.14 集群的全过程。</p>
<p>重点包括：</p>
<ul>
<li>使用 <code>redis.conf</code> 创建 ConfigMap</li>
<li>挂载配置到 Redis Pod</li>
<li>集群初始化失败：<code>redis:4.0.14</code> 官方镜像不支持 <code>redis-cli --cluster</code></li>
<li>排错验证与最终解决方案</li>
<li>一键初始化脚本</li>
</ul>
<p>所有步骤均基于真实命令验证，确保可复现。</p>
<hr>
<h2 id="2️⃣-第二部分开始部署">2️⃣ 第二部分：开始部署</h2>
<h3 id="21-准备-nfs-存储在-nfs-服务器上执行">2.1 准备 NFS 存储（在 NFS 服务器上执行）</h3>
<pre><code class="language-bash">mkdir -p /data/k8sdata/cka/redis{0..5}
chmod 777 /data/k8sdata/cka/redis{0..5}
</code></pre>
<hr>
<h3 id="22-创建-persistentvolumepv">2.2 创建 PersistentVolume（PV）</h3>
<blockquote>
<p>✅ 使原始 <code>pv/redis-cluster-pv.yaml</code></p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv0
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis0

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv1
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis1

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv2
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis2

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv3
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis3

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv4
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis4

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-cluster-pv5
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: 172.31.7.110
    path: /data/k8sdata/cka/redis5
</code></pre>
<h4 id="应用-pv">应用 PV：</h4>
<pre><code class="language-bash">kubectl apply -f pv/redis-cluster-pv.yaml
kubectl get pv
</code></pre>
<blockquote>
<p>✅ 确认 6 个 PV 状态为 <code>Available</code>。</p>
</blockquote>
<hr>
<h3 id="23-创建-redis-配置文件-redisconf">2.3 创建 Redis 配置文件 <code>redis.conf</code></h3>
<blockquote>
<p>✅ 使用原始的 <code>redis.conf</code></p>
</blockquote>
<pre><code class="language-conf"># redis.conf
port 6379
bind 0.0.0.0
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
save &quot;&quot;
</code></pre>
<blockquote>
<p>💡 说明：</p>
<ul>
<li><code>cluster-enabled yes</code>：启用集群模式</li>
<li><code>cluster-config-file nodes.conf</code>：集群节点信息文件</li>
<li><code>appendonly yes</code>：开启 AOF 持久化</li>
<li><code>save &quot;&quot;</code>：关闭 RDB 持久化，避免与 AOF 冲突</li>
</ul>
</blockquote>
<hr>
<h3 id="24-从-redisconf-创建-configmap">2.4 从 <code>redis.conf</code> 创建 ConfigMap</h3>
<pre><code class="language-bash">kubectl create configmap redis-config \
  --from-file=redis.conf=./redis.conf \
  -n cka
</code></pre>
<blockquote>
<p>✅ 验证 ConfigMap：</p>
</blockquote>
<pre><code class="language-bash">kubectl get configmap redis-config -n cka -o yaml
</code></pre>
<hr>
<h3 id="25-部署-redis-statefulset-与-service含-configmap-挂载">2.5 部署 Redis StatefulSet 与 Service（含 ConfigMap 挂载）</h3>
<blockquote>
<p>✅ 更新后的 <code>redis-cluster.yaml</code>，<strong>包含 ConfigMap 挂载</strong></p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: cka
  labels:
    app: redis
spec:
  selector:
    app: redis
    appCluster: redis-cluster
  ports:
    - name: redis
      port: 6379
      targetPort: 6379
    - name: cluster
      port: 16379
      targetPort: 16379
  clusterIP: None

---
apiVersion: v1
kind: Service
metadata:
  name: redis-access
  namespace: cka
  labels:
    app: redis
spec:
  selector:
    app: redis
    appCluster: redis-cluster
  ports:
    - name: redis-access
      protocol: TCP
      port: 6379
      targetPort: 6379
  type: ClusterIP

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
  namespace: cka
spec:
  serviceName: redis
  replicas: 6
  selector:
    matchLabels:
      app: redis
      appCluster: redis-cluster
  template:
    metadata:
      labels:
        app: redis
        appCluster: redis-cluster
    spec:
      terminationGracePeriodSeconds: 20
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - redis
                topologyKey: kubernetes.io/hostname
      containers:
        - name: redis
          image: redis:4.0.14
          command:
            - &quot;redis-server&quot;
          args:
            - &quot;/etc/redis/redis.conf&quot;
          resources:
            requests:
              cpu: &quot;500m&quot;
              memory: &quot;500Mi&quot;
          ports:
            - containerPort: 6379
              name: redis
              protocol: TCP
            - containerPort: 16379
              name: cluster
              protocol: TCP
          volumeMounts:
            - name: conf
              mountPath: /etc/redis
            - name: data
              mountPath: /var/lib/redis
      volumes:
        - name: conf
          configMap:
            name: redis-config
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: [ &quot;ReadWriteOnce&quot; ]
        resources:
          requests:
            storage: 5Gi
</code></pre>
<h4 id="应用配置">应用配置：</h4>
<pre><code class="language-bash">kubectl apply -f redis-cluster.yaml
</code></pre>
<hr>
<h3 id="26-等待-pod-就绪">2.6 等待 Pod 就绪</h3>
<pre><code class="language-bash">kubectl get pods -n cka -o wide -w
</code></pre>
<blockquote>
<p>✅ 等待所有 <code>redis-0</code> 到 <code>redis-5</code> 状态变为 <code>Running</code>。</p>
</blockquote>
<hr>
<h2 id="3️⃣-第三部分初始化报错首次尝试">3️⃣ 第三部分：初始化报错（首次尝试）</h2>
<h3 id="尝试使用-redis4014-镜像中的-redis-cli-初始化集群">尝试使用 <code>redis:4.0.14</code> 镜像中的 <code>redis-cli</code> 初始化集群</h3>
<pre><code class="language-bash">kubectl run -it --rm --restart=Never \
  --namespace cka \
  redis-admin \
  --image=redis:4.0.14 \
  -- redis-cli --cluster create \
    redis-access.cka.svc.cluster.local:6379 \
    redis-access.cka.svc.cluster.local:6379 \
    redis-access.cka.svc.cluster.local:6379 \
    redis-access.cka.svc.cluster.local:6379 \
    redis-access.cka.svc.cluster.local:6379 \
    redis-access.cka.svc.cluster.local:6379 \
  --cluster-replicas 1 \
  --cluster-yes
</code></pre>
<h3 id="报错信息">❌ 报错信息：</h3>
<pre><code>redis-cli: unknown option '--cluster'
</code></pre>
<blockquote>
<p>🔴 <strong>错误原因</strong>：<code>redis:4.0.14</code> 镜像中的 <code>redis-cli</code> <strong>不支持 <code>--cluster</code> 子命令</strong>。</p>
</blockquote>
<hr>
<h2 id="4️⃣-第四部分排错与验证">4️⃣ 第四部分：排错与验证</h2>
<h3 id="排错-1验证-redis4014-是否支持-cluster">🔎 排错 1：验证 <code>redis:4.0.14</code> 是否支持 <code>--cluster</code></h3>
<pre><code class="language-bash">docker run -it --rm redis:4.0.14 redis-cli --help | grep cluster
</code></pre>
<blockquote>
<p>🔍 <strong>输出为空</strong></p>
</blockquote>
<pre><code class="language-bash">docker run -it --rm redis:4.0.14 redis-cli --cluster help
</code></pre>
<blockquote>
<p>❌ 输出：</p>
</blockquote>
<pre><code>redis-cli: unknown option '--cluster'
</code></pre>
<blockquote>
<p>✅ <strong>结论</strong>：<code>redis:4.0.14</code> 官方镜像确实不支持 <code>--cluster</code>。</p>
</blockquote>
<hr>
<h3 id="排错-2尝试使用-redis-tribrb">🔎 排错 2：尝试使用 <code>redis-trib.rb</code></h3>
<pre><code class="language-bash">docker run -it --rm redis:4.0.14 sh -c &quot;ls /usr/local/bin | grep trib&quot;
</code></pre>
<blockquote>
<p>❌ 输出为空，<code>redis-trib.rb</code> 不存在。</p>
</blockquote>
<blockquote>
<p>✅ 结论：无法使用旧工具。</p>
</blockquote>
<hr>
<h3 id="解决方案使用高版本-redis-cliredis626-pod-ip">✅ 解决方案：使用高版本 <code>redis-cli</code>（<code>redis:6.2.6</code>） + Pod IP</h3>
<h4 id="获取-pod-ip">获取 Pod IP：</h4>
<pre><code>root@101-master1:/opt/k8s-data/yaml/cka/redis-cluster# kubectl get pod -cka -o wide
NAMESPACE              NAME                                             READY   STATUS    RESTARTS      AGE     IP              NODE           NOMINATED NODE   READINESS GATES
cka                 deploy-devops-redis-6d9fd4dbcb-hbs6q             1/1     Running   0             176m    10.200.45.203   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 cka-nginx-deployment-69db98d5ff-qc7mr         1/1     Running   0             9h      10.200.11.135   172.31.7.106   &lt;none&gt;           &lt;none&gt;
cka                 cka-tomcat-app1-deployment-78c495f67d-6db72   1/1     Running   0             24h     10.200.45.199   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 cka-tomcat-app1-deployment-78c495f67d-xbm5f   1/1     Running   0             24h     10.200.11.134   172.31.7.106   &lt;none&gt;           &lt;none&gt;
cka                 redis-0                                          1/1     Running   0             30m     10.200.210.76   172.31.7.104   &lt;none&gt;           &lt;none&gt;
cka                 redis-1                                          1/1     Running   0             29m     10.200.45.204   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 redis-2                                          1/1     Running   0             24m     10.200.11.136   172.31.7.106   &lt;none&gt;           &lt;none&gt;
cka                 redis-3                                          1/1     Running   0             24m     10.200.210.77   172.31.7.104   &lt;none&gt;           &lt;none&gt;
cka                 redis-4                                          1/1     Running   0             24m     10.200.45.205   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 redis-5                                          1/1     Running   0             24m     10.200.11.137   172.31.7.106   &lt;none&gt;           &lt;none&gt;
cka                 zookeeper1-5666cd8f6f-b7flr                      1/1     Running   0             4h28m   10.200.45.201   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 zookeeper2-c4964cd66-vsrt6                       1/1     Running   0             4h28m   10.200.45.202   172.31.7.105   &lt;none&gt;           &lt;none&gt;
cka                 zookeeper3-55fc5c6847-xfw9g                      1/1     Running   0             4h28m   10.200.210.75   172.31.7.104   &lt;none&gt;           &lt;none&gt;
</code></pre>
<pre><code class="language-bash">kubectl get pods -n cka -l app=redis -o jsonpath='{range .items[*]}{.status.podIP}{&quot;\n&quot;}{end}'
</code></pre>
<h4 id="使用高版本-redis-cli-初始化">使用高版本 <code>redis-cli</code> 初始化：</h4>
<pre><code class="language-bash">kubectl run -it --rm --restart=Never \
  --namespace cka \
  redis-admin \
  --image=redis:6.2.6 \
  -- redis-cli --cluster create \
    10.200.210.76:6379 \
    10.200.45.204:6379 \
    10.200.11.136:6379 \
    10.200.210.77:6379 \
    10.200.45.205:6379 \
    10.200.11.137:6379 \
  --cluster-replicas 1 \
  --cluster-yes
</code></pre>
<blockquote>
<p>✅ 成功输出：</p>
</blockquote>
<pre><code class="language-bash">root@101-master1:/opt/k8s-data/yaml/cka/redis-cluster# kubectl run -it --rm --restart=Never \
&gt;   --namespace cka \
&gt;   redis-admin \
&gt;   --image=redis:6.2.6 \
&gt;   -- redis-cli --cluster create \
&gt;     10.200.210.76:6379 \
&gt;     10.200.45.204:6379 \
&gt;     10.200.11.136:6379 \
&gt;     10.200.210.77:6379 \
&gt;     10.200.45.205:6379 \
&gt;     10.200.11.137:6379 \
&gt;   --cluster-replicas 1 \
&gt;   --cluster-yes
If you don't see a command prompt, try pressing enter.
..
&gt;&gt;&gt; Performing Cluster Check (using node 10.200.210.76:6379)
M: 0d3db136c17df8b0681fe8e2436a4f99203f0507 10.200.210.76:6379
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: a6f78d2a82b2c56fdd8f61794a96e30f2398777f 10.200.11.137:6379
   slots: (0 slots) slave
   replicates 931716e199e90bc313c3480661af530d1f32bf08
S: 4a4baa12df1ada0d81e27b9a1ecc7aa2f9cec0ff 10.200.45.205:6379
   slots: (0 slots) slave
   replicates 0d3db136c17df8b0681fe8e2436a4f99203f0507
S: 6f33cd693707f3109704ca23151a4f1b7716e380 10.200.210.77:6379
   slots: (0 slots) slave
   replicates 7a8f9abcbc779faebb10e2b5de9f00be0d7ad482
M: 7a8f9abcbc779faebb10e2b5de9f00be0d7ad482 10.200.11.136:6379
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
M: 931716e199e90bc313c3480661af530d1f32bf08 10.200.45.204:6379
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
&gt;&gt;&gt; Check for open slots...
&gt;&gt;&gt; Check slots coverage...
[OK] All 16384 slots covered.
pod &quot;redis-admin&quot; deleted
root@101-master1:/opt/k8s-data/yaml/cka/redis-cluster# kubectl exec -n cka redis-0 -- redis-cli -c cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:55
cluster_stats_messages_pong_sent:58
cluster_stats_messages_sent:113
cluster_stats_messages_ping_received:53
cluster_stats_messages_pong_received:55
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:113
root@101-master1:/opt/k8s-data/yaml/cka/redis-cluster# kubectl exec -n cka redis-0 -- redis-cli -c cluster nodes
a6f78d2a82b2c56fdd8f61794a96e30f2398777f 10.200.11.137:6379@16379 slave 931716e199e90bc313c3480661af530d1f32bf08 0 1755030011982 6 connected
0d3db136c17df8b0681fe8e2436a4f99203f0507 10.200.210.76:6379@16379 myself,master - 0 1755030010000 1 connected 0-5460
4a4baa12df1ada0d81e27b9a1ecc7aa2f9cec0ff 10.200.45.205:6379@16379 slave 0d3db136c17df8b0681fe8e2436a4f99203f0507 0 1755030011000 5 connected
6f33cd693707f3109704ca23151a4f1b7716e380 10.200.210.77:6379@16379 slave 7a8f9abcbc779faebb10e2b5de9f00be0d7ad482 0 1755030011577 4 connected
7a8f9abcbc779faebb10e2b5de9f00be0d7ad482 10.200.11.136:6379@16379 master - 0 1755030010000 3 connected 10923-16383
931716e199e90bc313c3480661af530d1f32bf08 10.200.45.204:6379@16379 master - 0 1755030010971 2 connected 5461-10922

[OK] All 16384 slots covered.
</code></pre>
<hr>
<h3 id="排错总结">📌 排错总结</h3>
<table>
<thead>
<tr>
<th>问题</th>
<th>原因</th>
<th>解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>redis-cli: unknown option '--cluster'</code></td>
<td><code>redis:4.0.14</code> 镜像不支持</td>
<td>使用 <code>redis:6.2.6</code> 的 <code>redis-cli</code></td>
</tr>
<tr>
<td><code>redis-trib.rb</code> 不存在</td>
<td>已被移除</td>
<td>放弃使用</td>
</tr>
<tr>
<td>“same host” 错误</td>
<td>使用了 ClusterIP 服务</td>
<td>改用 Pod IP 初始化</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="5️⃣-第五部分验证集群状态">5️⃣ 第五部分：验证集群状态</h2>
<h3 id="51-检查集群整体状态">5.1 检查集群整体状态</h3>
<pre><code class="language-bash">kubectl exec -n cka redis-0 -- redis-cli -c cluster info
</code></pre>
<blockquote>
<p>✅ 输出：</p>
</blockquote>
<pre><code>cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_known_nodes:6
cluster_size:3
</code></pre>
<hr>
<h3 id="52-查看节点拓扑">5.2 查看节点拓扑</h3>
<pre><code class="language-bash">kubectl exec -n cka redis-0 -- redis-cli -c cluster nodes
</code></pre>
<blockquote>
<p>✅ 输出（节选）：</p>
</blockquote>
<pre><code>0d3db136c17df8b0681fe8e2436a4f99203f0507 10.200.210.76:6379@16379 master - 0 1755030010000 1 connected 0-5460
931716e199e90bc313c3480661af530d1f32bf08 10.200.45.204:6379@16379 master - 0 1755030010971 2 connected 5461-10922
7a8f9abcbc779faebb10e2b5de9f00be0d7ad482 10.200.11.136:6379@16379 master - 0 1755030010000 3 connected 10923-16383
</code></pre>
<blockquote>
<p>✅ 结论：3 主 3 从，集群健康。</p>
</blockquote>
<hr>
<h2 id="总结">✅ 总结</h2>
<ul>
<li>** <code>redis.conf</code> 已通过 ConfigMap 正确挂载**</li>
<li><strong><code>redis:4.0.14</code> 官方镜像确实不支持 <code>--cluster</code></strong></li>
<li><strong>必须使用高版本 <code>redis-cli</code>（如 <code>6.2.6</code>）进行集群管理</strong></li>
<li><strong>初始化必须使用 Pod IP，避免服务名解析问题</strong></li>
</ul>
<blockquote>
<p>🎉 此文档为完整、真实、可执行的部署手册，可用于团队交付。</p>
</blockquote>
<hr>
<h2 id="6️⃣-第六部分一键初始化脚本">6️⃣ 第六部分：一键初始化脚本</h2>
<h3 id="init-redis-clustersh"><code>init-redis-cluster.sh</code></h3>
<pre><code class="language-bash">#!/bin/bash
# init-redis-cluster.sh
# 一键初始化 Redis 集群（使用 Pod IP + 高版本 redis-cli）
# 命名空间：cka

set -e

NAMESPACE=&quot;cka&quot;
APP_LABEL=&quot;app=redis&quot;
REPLICA_COUNT=1

echo &quot;🔍 获取 Redis Pod 及其 IP 地址...&quot;

POD_IPS=($(kubectl get pods -n $NAMESPACE -l $APP_LABEL -o jsonpath='{.items[*].status.podIP}'))
POD_NAMES=($(kubectl get pods -n $NAMESPACE -l $APP_LABEL -o jsonpath='{.items[*].metadata.name}'))

if [ ${#POD_IPS[@]} -ne 6 ]; then
  echo &quot;❌ 错误：期望 6 个 Pod，但只找到 ${#POD_IPS[@]} 个&quot;
  kubectl get pods -n $NAMESPACE -l $APP_LABEL
  exit 1
fi

echo &quot;_Pods: ${POD_NAMES[*]}&quot;
echo &quot;_Pods IPs: ${POD_IPS[*]}&quot;

echo &quot;🚀 使用 redis:6.2.6 初始化 Redis 集群...&quot;

kubectl run -it --rm --restart=Never \
  --namespace $NAMESPACE \
  redis-admin \
  --image=redis:6.2.6 \
  -- sh -c &quot;
    redis-cli --cluster create \
      ${POD_IPS[0]}:6379 \
      ${POD_IPS[1]}:6379 \
      ${POD_IPS[2]}:6379 \
      ${POD_IPS[3]}:6379 \
      ${POD_IPS[4]}:6379 \
      ${POD_IPS[5]}:6379 \
    --cluster-replicas $REPLICA_COUNT \
    --cluster-yes
  &quot;

echo &quot;✅ Redis 集群初始化完成！&quot;
</code></pre>
<blockquote>
<p>✅ 使用方法：</p>
</blockquote>
<pre><code class="language-bash">chmod +x init-redis-cluster.sh
./init-redis-cluster.sh
</code></pre>
<hr>
<h2 id="总结-2">✅ 总结</h2>
<ul>
<li><strong><code>redis:4.0.14</code> 官方镜像中的 <code>redis-cli</code> 确实不支持 <code>--cluster</code> 命令</strong>，这是部署中的关键障碍。</li>
<li><strong>必须使用高版本 <code>redis-cli</code>（如 <code>redis:6.2.6</code>）</strong> 来管理 Redis 4.0.14 集群。</li>
<li><strong>初始化时必须使用 Pod IP</strong>，避免 ClusterIP 导致的“同一主机”错误。</li>
</ul>
<blockquote>
<p>🎉 此文档真实还原了排错过程，可作为标准操作手册。</p>
</blockquote>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes 1.25 集群部署指南  ]]></title>
        <id>https://orochw.github.io/post/kubernetes-125-ji-qun-bu-shu-zhi-nan/</id>
        <link href="https://orochw.github.io/post/kubernetes-125-ji-qun-bu-shu-zhi-nan/">
        </link>
        <updated>2025-03-25T11:12:37.000Z</updated>
        <content type="html"><![CDATA[<p>本指南将指导您如何从头开始设置一个使用 Docker 作为容器运行时的 Kubernetes 1.25 集群。我们将涵盖从系统准备到集群初始化的所有步骤，并提供一些常见问题的解决方法。</p>
<h3 id="架构概述">架构概述</h3>
<table>
<thead>
<tr>
<th>节点类型</th>
<th>主机名</th>
<th>IP 地址</th>
<th>角色描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Master 节点</td>
<td>k8s-master</td>
<td>172.31.7.100</td>
<td>集群管理节点，负责集群的初始化、调度等工作</td>
</tr>
<tr>
<td>Worker 节点 1</td>
<td>k8s-node1</td>
<td>172.31.7.101</td>
<td>执行容器工作负载</td>
</tr>
<tr>
<td>Worker 节点 2</td>
<td>k8s-node2</td>
<td>172.31.7.102</td>
<td>执行容器工作负载</td>
</tr>
<tr>
<td>Worker 节点 3</td>
<td>k8s-node3</td>
<td>172.31.7.103</td>
<td>执行容器工作负载</td>
</tr>
</tbody>
</table>
<h3 id="安装与配置概览">安装与配置概览</h3>
<table>
<thead>
<tr>
<th>序号</th>
<th>步骤</th>
<th>主要操作</th>
<th>目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>前置配置</td>
<td>关闭 swap、设置主机名解析、加载内核模块等</td>
<td>准备系统环境，确保 Kubernetes 能够正常运行</td>
</tr>
<tr>
<td>2</td>
<td>安装 Docker</td>
<td>添加 Docker APT 源并安装指定版本的 Docker</td>
<td>提供容器运行时支持</td>
</tr>
<tr>
<td>3</td>
<td>安装 cri-dockerd</td>
<td>下载并安装 <code>cri-dockerd</code>，使其兼容 Kubernetes CRI 接口</td>
<td>允许 Docker 作为 Kubernetes 的容器运行时</td>
</tr>
<tr>
<td>4</td>
<td>安装 Kubernetes 组件</td>
<td>安装 <code>kubeadm</code>, <code>kubelet</code>, 和 <code>kubectl</code></td>
<td>初始化和管理 Kubernetes 集群</td>
</tr>
<tr>
<td>5</td>
<td>初始化 Master 节点</td>
<td>使用 <code>kubeadm init</code> 命令初始化集群</td>
<td>创建集群，并生成加入命令供其他节点使用</td>
</tr>
<tr>
<td>6</td>
<td>Worker 节点加入集群</td>
<td>在每个 Worker 节点上执行生成的 join 命令</td>
<td>将 Worker 节点加入到集群中</td>
</tr>
<tr>
<td>7</td>
<td>安装网络插件</td>
<td>使用 Calico 网络插件</td>
<td>提供 Pod 网络支持</td>
</tr>
<tr>
<td>8</td>
<td>验证集群状态</td>
<td>使用 <code>kubectl get nodes</code> 和 <code>kubectl get pods -n kube-system</code> 查看状态</td>
<td>确认所有节点和系统组件正常运行</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="一-所有节点前置配置所有节点执行">一、所有节点前置配置（所有节点执行）</h3>
<pre><code class="language-bash"># 关闭 swap
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

# 设置主机名解析（所有节点）
# 在 100 节点执行：
sudo hostnamectl set-hostname k8s-master
# 在 101-103 节点分别执行：
sudo hostnamectl set-hostname k8s-node1
sudo hostnamectl set-hostname k8s-node2
sudo hostnamectl set-hostname k8s-node3

# 所有节点添加 hosts 解析（可选）
cat &lt;&lt;EOF | sudo tee -a /etc/hosts
172.31.7.100 k8s-master
172.31.7.101 k8s-node1
172.31.7.102 k8s-node2
172.31.7.103 k8s-node3
EOF

# 加载内核模块
cat &lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

# 设置内核参数
cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system

# 禁用防火墙（如果使用）
sudo ufw disable
</code></pre>
<hr>
<h3 id="二-安装-docker所有节点">二、安装 Docker（所有节点）</h3>
<pre><code class="language-bash"># 安装 Docker
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
sudo apt-get update
sudo apt-get install -y docker-ce=5:20.10.23~3-0~ubuntu-focal docker-ce-cli=5:20.10.23~3-0~ubuntu-focal containerd.io

# 配置 Docker 使用 systemd
cat &lt;&lt;EOF | sudo tee /etc/docker/daemon.json
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

sudo systemctl restart docker
sudo systemctl enable docker
</code></pre>
<hr>
<h3 id="三-安装-cri-dockerd所有节点">三、安装 cri-dockerd（所有节点）</h3>
<pre><code class="language-bash"># 下载预编译二进制包
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.4/cri-dockerd_0.3.4.3-0.ubuntu-focal_amd64.deb
sudo dpkg -i cri-dockerd_0.3.4.3-0.ubuntu-focal_amd64.deb

# 修改服务配置（适配 Kubernetes 1.25）
sudo sed -i 's|--network-plugin=cni --cni-bin-dir=/opt/cni/bin|--network-plugin=cni --cni-conf-dir=/etc/cni/net.d|g' /lib/systemd/system/cri-docker.service
sudo systemctl daemon-reload
sudo systemctl enable cri-docker &amp;&amp; sudo systemctl start cri-docker
</code></pre>
<hr>
<h3 id="四-安装-kubeadmkubeletkubectl所有节点">四、安装 kubeadm/kubelet/kubectl（所有节点）</h3>
<pre><code class="language-bash"># 添加阿里云镜像源
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

# 安装指定版本
sudo apt-get update
sudo apt-get install -y kubelet=1.25.7-00 kubeadm=1.25.7-00 kubectl=1.25.7-00
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre>
<hr>
<h3 id="五-初始化-master-节点仅在-master-执行">五、初始化 Master 节点（仅在 master 执行）</h3>
<pre><code class="language-bash"># 生成初始化命令（替换实际IP）
sudo kubeadm init \
  --apiserver-advertise-address=172.31.7.100 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.25.7 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=192.168.0.0/16 \
  --cri-socket unix:///var/run/cri-dockerd.sock

# 完成后按提示操作
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# 保存 join 命令（稍后用于节点加入）
kubeadm token create --print-join-command &gt; join-command.txt
</code></pre>
<hr>
<h3 id="六-worker-节点加入集群在-101-103-执行">六、Worker 节点加入集群（在 101-103 执行）</h3>
<pre><code class="language-bash"># 使用上一步生成的 join 命令（需添加 --cri-socket）
sudo $(cat join-command.txt) --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<hr>
<h3 id="七-安装网络插件在-master-执行">七、安装网络插件（在 master 执行）</h3>
<pre><code class="language-bash"># 安装 Calico（匹配 --pod-network-cidr=192.168.0.0/16）
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
</code></pre>
<hr>
<h3 id="八-验证集群状态">八、验证集群状态</h3>
<pre><code class="language-bash">kubectl get nodes -o wide
# 等待所有节点状态变为 Ready
kubectl get pods -n kube-system
</code></pre>
<h2 id=""><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250325192110.png" alt="" loading="lazy"></h2>
<h3 id="常见问题处理">常见问题处理</h3>
<ol>
<li>
<p><strong>镜像拉取失败</strong>：</p>
<pre><code class="language-bash"># 手动拉取镜像（例如）
docker pull registry.aliyuncs.com/google_containers/pause:3.8
</code></pre>
</li>
<li>
<p><strong>cri-dockerd 未启动</strong>：</p>
<pre><code class="language-bash">journalctl -u cri-docker -l
</code></pre>
</li>
<li>
<p><strong>节点 NotReady</strong>：</p>
<pre><code class="language-bash">journalctl -u kubelet -f
</code></pre>
</li>
</ol>
<hr>
<p>完成以上步骤后，您将获得一个使用 Docker 作为容器运行时的 Kubernetes 1.25 集群。所有节点均可通过 <code>kubectl</code> 命令在 Master 节点管理。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Network Policy 详解与实践]]></title>
        <id>https://orochw.github.io/post/kubernetes-network-policy-xiang-jie-yu-shi-jian/</id>
        <link href="https://orochw.github.io/post/kubernetes-network-policy-xiang-jie-yu-shi-jian/">
        </link>
        <updated>2025-02-18T14:12:43.000Z</updated>
        <content type="html"><![CDATA[<hr>
<h1 id="kubernetes-network-policy-详解与实践">Kubernetes Network Policy 详解与实践</h1>
<p>Kubernetes 的 <code>NetworkPolicy</code> 是一种强大的工具，用于定义 Pod 之间的网络通信规则。本文将通过详细的解析和示例，帮助您理解如何使用 <code>NetworkPolicy</code> 控制 Pod 的入站（Ingress）和出站（Egress）流量。</p>
<hr>
<h2 id="一-引言">一、引言</h2>
<p>在 Kubernetes 集群中，<code>NetworkPolicy</code> 提供了一种声明式的方式来指定 Pod 间的通信规则。通过定义 <code>NetworkPolicy</code>，我们可以精细地控制 Pod 的入站和出站流量，从而增强集群的安全性。</p>
<p>本文将分为以下几个部分：</p>
<ol>
<li><strong>基础概念</strong>：介绍 <code>NetworkPolicy</code> 的基本结构和关键字段。</li>
<li><strong>Ingress 和 Egress 概述</strong>：讲解入站和出站流量的概念及其管理方式。</li>
<li><strong>示例详解</strong>：通过 9 个具体示例，详细解析 <code>NetworkPolicy</code> 的配置和使用方法。</li>
<li><strong>总结</strong>：回顾关键点并提供进一步学习的建议。</li>
</ol>
<hr>
<h2 id="二-network-policy-基础">二、Network Policy 基础</h2>
<p><code>NetworkPolicy</code> 资源对象定义在 <code>networking.k8s.io/v1</code> API 版本中，主要包含以下关键字段：</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 资源类型为 NetworkPolicy
metadata:
  name: example-networkpolicy # 网络策略名称
  namespace: default # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: web # 匹配标签为 app: web 的 Pod
  ingress: # 入站规则
  - from:
    - podSelector:
        matchLabels:
          app: db # 允许来源：标签为 app: db 的 Pod
</code></pre>
<h3 id="关键字段说明">关键字段说明：</h3>
<ul>
<li><strong>apiVersion</strong>：指定使用的 API 版本。</li>
<li><strong>kind</strong>：资源类型为 <code>NetworkPolicy</code>。</li>
<li><strong>metadata</strong>：包含网络策略的名称和所在的命名空间。</li>
<li><strong>spec</strong>：网络策略的具体规则，包括 <code>policyTypes</code>、<code>podSelector</code>、<code>ingress</code> 和 <code>egress</code> 等。</li>
</ul>
<hr>
<h2 id="三-ingress-和-egress-概述">三、Ingress 和 Egress 概述</h2>
<h3 id="1-ingress入站流量">1. Ingress（入站流量）</h3>
<p><code>Ingress</code> 是 Kubernetes 中用于管理外部访问集群内部服务的 HTTP 和 HTTPS 路由的资源对象。它定义了路由规则，控制流量的路由，但本身不处理流量，而是依赖于 <code>Ingress Controller</code> 来实现流量处理。</p>
<h4 id="ingress-架构">Ingress 架构</h4>
<p>Ingress 架构通常包括两个主要组件：</p>
<ul>
<li><strong>Ingress Controller</strong>：负责监听 Ingress 规则变化并据此更新负载均衡器配置。</li>
<li><strong>Ingress 规则</strong>：定义了如何将外部流量路由到集群内部的服务。</li>
</ul>
<h4 id="示例">示例</h4>
<p>以下是一个简单的 Ingress 示例，定义了两个规则：</p>
<ul>
<li>将所有 <code>/demo</code> 请求发送到 <code>demo-service</code> 服务。</li>
<li>将所有其他请求发送到 <code>main-service</code> 服务。</li>
</ul>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: basic-ingress
spec:
  rules:
  - http:
      paths:
      - path: /demo
        pathType: Prefix
        backend:
          service:
            name: demo-service
            port:
              number: 80
  defaultBackend:
    service:
      name: main-service
      port:
        number: 80
</code></pre>
<h3 id="2-egress出站流量">2. Egress（出站流量）</h3>
<p>相对于 <code>Ingress</code>，<code>Egress</code> 代表离开 Kubernetes 集群的流量。在 Kubernetes 中，<code>Egress</code> 流量通常通过 Node 的网络接口直接发送到外部网络。虽然 <code>Ingress</code> 有专门的控制器和规则来管理，但 <code>Egress</code> 流量则相对简单，通常不需要特别的控制器或规则来管理（除非有特定的安全或策略需求）。</p>
<hr>
<h2 id="四-示例详解">四、示例详解</h2>
<p>以下是 9 个具体的 <code>NetworkPolicy</code> 示例，详细解析其配置和使用方法。</p>
<h3 id="示例-1基于-pod-标签的入站规则">示例 1：基于 Pod 标签的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则 也就是允许哪个 Pod 访问这个 Pod
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  ingress: # 入站规则
  - from:
    - podSelector:
        matchLabels:
          project: &quot;python&quot; # 允许来源：标签为 project: python 的 Pod
</code></pre>
<h3 id="示例-1基于-pod-标签的入站规则-2">示例 1：基于 Pod 标签的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  ingress: # 入站规则
  - from:
    - podSelector:
        matchLabels:
          project: &quot;python&quot; # 允许来源：标签为 project: python 的 Pod
</code></pre>
<hr>
<h3 id="示例-2基于-pod-标签和端口的入站规则">示例 2：基于 Pod 标签和端口的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  ingress: # 入站规则
  - from:
    - podSelector:
        matchLabels:
          project: &quot;python&quot; # 允许来源：标签为 project: python 的 Pod
    ports:
    - protocol: TCP # 指定协议：TCP
      port: 8080 # 指定端口：8080
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>ports</code> 字段限制了允许访问的协议和端口。</li>
<li>在此示例中，仅允许来自标签为 <code>project: python</code> 的 Pod 通过 TCP 协议访问目标 Pod 的 8080 端口。</li>
</ul>
<hr>
<h3 id="示例-3多端口入站规则">示例 3：多端口入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  ingress: # 入站规则
  - from:
    - podSelector: {} # 不限制来源 Pod
    ports:
    - protocol: TCP # 指定协议：TCP
      port: 8080 # 指定端口：8080
    - protocol: TCP
      port: 3306 # 指定端口：3306
    - protocol: TCP
      port: 6379 # 指定端口：6379
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>from</code> 中的 <code>podSelector: {}</code> 表示不限制来源 Pod。</li>
<li><code>ports</code> 列出了多个端口和协议，允许访问的目标 Pod 的这些端口。</li>
</ul>
<hr>
<h3 id="示例-4不限制源-pod-的入站规则">示例 4：不限制源 Pod 的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels: {} # 匹配所有目标 Pod
  ingress: # 入站规则
  - from:
    - podSelector: {} # 不限制来源 Pod
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>podSelector: {}</code> 表示该规则适用于命名空间中的所有 Pod。</li>
<li><code>from</code> 中的 <code>podSelector: {}</code> 表示不限制来源 Pod。</li>
</ul>
<hr>
<h3 id="示例-5基于-ip-段的入站规则">示例 5：基于 IP 段的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  ingress: # 入站规则
  - from:
    - ipBlock:
        cidr: 10.200.0.0/16 # 允许来自 10.200.0.0/16 网段的 IP 地址
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>ipBlock</code> 定义了允许访问的 IP 地址范围。</li>
<li>在此示例中，仅允许来自 <code>10.200.0.0/16</code> 网段的 IP 地址访问目标 Pod。</li>
</ul>
<hr>
<h3 id="示例-6基于-namespace-标签的入站规则">示例 6：基于 Namespace 标签的入站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: tomcat-access--networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Ingress # 定义入站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels: {} # 匹配命名空间中的所有 Pod
  ingress: # 入站规则
  - from:
    - namespaceSelector:
        matchLabels:
          nsname: linux # 允许来源：标签为 nsname: linux 的 Namespace
    ports:
    - protocol: TCP # 指定协议：TCP
      port: 8080 # 指定端口：8080
    - protocol: TCP
      port: 3306 # 指定端口：3306
    - protocol: TCP
      port: 6379 # 指定端口：6379
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>namespaceSelector</code> 定义了允许访问的 Namespace。</li>
<li>在此示例中，仅允许来自标签为 <code>nsname: linux</code> 的 Namespace 中的 Pod 访问目标 Pod 的指定端口。</li>
</ul>
<hr>
<h3 id="示例-7基于-ip-段的出站规则">示例 7：基于 IP 段的出站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: egress-access-networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Egress # 定义出站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-tomcat-app1-selector # 匹配标签为 app: python-tomcat-app1-selector 的 Pod
  egress: # 出站规则
  - to:
    - ipBlock:
        cidr: 10.200.0.0/16 # 允许访问 10.200.0.0/16 网段的 IP 地址
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>egress</code> 定义了出站流量规则。</li>
<li>在此示例中，仅允许目标 Pod 访问 <code>10.200.0.0/16</code> 网段的 IP 地址。</li>
</ul>
<hr>
<h3 id="示例-8基于-pod-标签的出站规则">示例 8：基于 Pod 标签的出站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: egress-access-networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Egress # 定义出站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-nginx-selector # 匹配标签为 app: python-nginx-selector 的 Pod
  egress: # 出站规则
  - to:
    - podSelector:
        matchLabels:
          app: python-tomcat-app1-selector # 允许访问：标签为 app: python-tomcat-app1-selector 的 Pod
    ports:
    - protocol: TCP # 指定协议：TCP
      port: 8080 # 指定端口：8080
    - protocol: TCP
      port: 53 # 指定端口：53
    - protocol: UDP
      port: 53 # 指定端口：53
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>to</code> 定义了允许访问的目标 Pod。</li>
<li>在此示例中，仅允许目标 Pod 访问标签为 <code>app: python-tomcat-app1-selector</code> 的 Pod 的指定端口。</li>
</ul>
<hr>
<h3 id="示例-9基于-namespace-标签的出站规则">示例 9：基于 Namespace 标签的出站规则</h3>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1 # API 版本
kind: NetworkPolicy # 网络策略资源类型
metadata:
  name: egress-access-networkpolicy # 网络策略名称
  namespace: python # 所属命名空间
spec:
  policyTypes:
  - Egress # 定义出站流量规则
  podSelector: # 目标 Pod 的选择器
    matchLabels:
      app: python-nginx-selector # 匹配标签为 app: python-nginx-selector 的 Pod
  egress: # 出站规则
  - to:
    - namespaceSelector:
        matchLabels:
          nsname: python # 允许访问：标签为 nsname: python 的 Namespace
    - namespaceSelector:
        matchLabels:
          nsname: linux # 允许访问：标签为 nsname: linux 的 Namespace
    ports:
    - protocol: TCP # 指定协议：TCP
      port: 8080 # 指定端口：8080
    - protocol: TCP
      port: 53 # 指定端口：53
    - protocol: UDP
      port: 53 # 指定端口：53
</code></pre>
<p><strong>注释说明</strong>：</p>
<ul>
<li><code>namespaceSelector</code> 定义了允许访问的 Namespace。</li>
<li>在此示例中，仅允许目标 Pod 访问标签为 <code>nsname: python</code> 和 <code>nsname: linux</code> 的 Namespace 中的 Pod 的指定端口。</li>
</ul>
<hr>
<hr>
<h2 id="五-总结">五、总结</h2>
<h3 id="1-核心概念回顾">1. <strong>核心概念回顾</strong></h3>
<ul>
<li><strong>NetworkPolicy</strong>：用于定义 Pod 间通信规则的核心资源对象。</li>
<li><strong>Ingress</strong>：管理进入集群的流量，通常通过 <code>Ingress Controller</code> 实现。</li>
<li><strong>Egress</strong>：管理离开集群的流量，通常需要额外的安全策略。</li>
</ul>
<h3 id="2-关键字段">2. <strong>关键字段</strong></h3>
<ul>
<li><strong>apiVersion</strong> 和 <strong>kind</strong>：定义资源类型和 API 版本。</li>
<li><strong>metadata</strong>：指定资源名称和命名空间。</li>
<li><strong>spec</strong>：定义具体的规则，包括 <code>policyTypes</code>、<code>podSelector</code>、<code>ingress</code> 和 <code>egress</code>。</li>
</ul>
<h3 id="3-应用场景">3. <strong>应用场景</strong></h3>
<ul>
<li><strong>安全性</strong>：通过限制 Pod 间的通信，防止未经授权的访问。</li>
<li><strong>流量管理</strong>：精确控制流量的流向，优化资源利用率。</li>
<li><strong>多租户环境</strong>：隔离不同租户的网络通信，确保数据安全。</li>
</ul>
<h3 id="4-下一步学习建议">4. <strong>下一步学习建议</strong></h3>
<ul>
<li>学习更多关于 <code>CNI</code> 插件的知识，了解它们如何支持 <code>NetworkPolicy</code>。</li>
<li>探索 <code>Ingress Controller</code> 的高级功能，例如 SSL/TLS 支持和负载均衡。</li>
<li>实践复杂的网络策略，结合实际业务场景进行优化。</li>
</ul>
<p>希望本文能帮助您更好地理解和应用 Kubernetes 的 <code>NetworkPolicy</code>！如果您有任何疑问或需要进一步的帮助，请随时留言交流！</p>
<hr>
<p>如果还有其他需求，请告诉我！</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[一个简单的jenkins+harbor+k8s的cicd]]></title>
        <id>https://orochw.github.io/post/yi-ge-jian-dan-de-jenkinsharbork8s-de-cicd/</id>
        <link href="https://orochw.github.io/post/yi-ge-jian-dan-de-jenkinsharbork8s-de-cicd/">
        </link>
        <updated>2025-01-16T17:34:19.000Z</updated>
        <content type="html"><![CDATA[<pre><code class="language-markdown"># Kubernetes CI/CD 环境搭建指南

由于学习K8S有一段时间了，之前使用Jenkins Pipeline实现的CI/CD也已经很久没有更新。现在想尝试一些新的东西。

## 网络配置概览

| IP地址      | 主机名         | 备注       |
|-------------|----------------|------------|
| 172.31.7.11 | master1-7-11   | 主Master   |
| 172.31.7.21 | jenkins-7-21   | Jenkins    |
| 172.31.7.22 | gitlab-7-22    | GitLab     |
| 172.31.7.10 | harbor-7-10    | Harbor     |

备注：系统均为Ubuntu

---

# 一、准备Jenkins环境（特别是JDK）

### 安装JDK

```bash
apt install openjdk-17-jdk 
</code></pre>
<h3 id="下载最新稳定版jenkins安装包">下载最新稳定版Jenkins安装包</h3>
<p><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117021236.png" alt="下载页面" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117021419.png" alt="选择版本" loading="lazy"></p>
<h3 id="安装长期支持版本jenkins">安装长期支持版本Jenkins</h3>
<pre><code class="language-bash">sudo wget -O /usr/share/keyrings/jenkins-keyring.asc \
  https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key
echo &quot;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc]&quot; \
  https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
  /etc/apt/sources.list.d/jenkins.list &gt; /dev/null
sudo apt-get update
sudo apt-get install jenkins
</code></pre>
<h4 id="修改jenkins配置文件-etcdefaultjenkins">修改Jenkins配置文件 <code>/etc/default/jenkins</code></h4>
<pre><code class="language-bash">vim /etc/default/jenkins
NAME=root
JAVA_ARGS=&quot;-Djava.awt.headless=true&quot;
JENKINS_USER=root
JENKINS_GROUP=root
JENKINS_HOME=/var/lib/$NAME
</code></pre>
<h4 id="修改jenkins-systemd服务文件-usrlibsystemdsystemjenkinsservice">修改Jenkins systemd服务文件 <code>/usr/lib/systemd/system/jenkins.service</code></h4>
<pre><code class="language-bash">vim /usr/lib/systemd/system/jenkins.service
User=root # 将服务启动的用户改为root
Group=root # 服务启动的组改为root
</code></pre>
<p>重启Jenkins服务：</p>
<pre><code class="language-bash">systemctl restart jenkins
systemctl daemon-reload
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117022346.png" alt="Jenkins Web界面" loading="lazy"></figure>
<hr>
<h1 id="二-安装gitlab">二、安装GitLab</h1>
<h3 id="下载gitlab安装包">下载GitLab安装包</h3>
<p>去<a href="https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/apt/packages.gitlab.com/gitlab/gitlab-ce/ubuntu/pool/focal/main/g/gitlab-ce/">清华大学镜像站</a>下载喜欢的版本。</p>
<h3 id="配置并启动gitlab">配置并启动GitLab</h3>
<p>参考<a href="https://blog.csdn.net/Hemameba/article/details/133854744">这篇博客</a>进行安装与配置。</p>
<h3 id="创建用户并修改密码创建测试项目">创建用户并修改密码，创建测试项目</h3>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117023112.png" alt="GitLab用户管理" loading="lazy"></figure>
<pre><code>
### 1. **Kubernetes Deployment YAML 文件**

这个YAML文件定义了一个Kubernetes部署（Deployment）和服务（Service），用于在Kubernetes集群中运行一个Tomcat应用。

```yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: item1-tomcat-app1-deployment-label
  name: item1-tomcat-app1-deployment
  namespace: item1
spec:
  replicas: 2
  selector:
    matchLabels:
      app: item1-tomcat-app1-selector
  template:
    metadata:
      labels:
        app: item1-tomcat-app1-selector
    spec:
      imagePullSecrets:
      - name: harbor-secret
      containers:
      - name: item1-tomcat-app1-container
        image: harbor.xtec.com/item1/tomcat:8.5.43_20250117_013112
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          protocol: TCP
          name: http
        env:
        - name: &quot;password&quot;
          value: &quot;123456&quot;
        - name: &quot;age&quot;
          value: &quot;18&quot;
        resources:
          limits:
            cpu: 1
            memory: &quot;512Mi&quot;
          requests:
            cpu: 500m
            memory: &quot;512Mi&quot;
        volumeMounts:
        - name: item1-images
          mountPath: /usr/local/nginx/html/webapp/images
          readOnly: false
        - name: item1-static
          mountPath: /usr/local/nginx/html/webapp/static
          readOnly: false
      volumes:
      - name: item1-images
        nfs:
          server: 172.31.7.20
          path: /data/k8sdata/item1/images
      - name: item1-static
        nfs:
          server: 172.31.7.20
          path: /data/k8sdata/item1/static

---
kind: Service
apiVersion: v1
metadata:
  labels:
    app: item1-tomcat-app1-service-label
  name: item1-tomcat-app1-service
  namespace: item1
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
    nodePort: 30092
  selector:
    app: item1-tomcat-app1-selector
</code></pre>
<p><strong>功能说明</strong>：</p>
<ul>
<li>定义了一个名为 <code>item1-tomcat-app1-deployment</code> 的部署，使用两个副本（replicas: 2）。</li>
<li>每个容器使用 <code>harbor.xtec.com/item1/tomcat:8.5.43_20250117_013112</code> 镜像，并暴露8080端口。</li>
<li>使用NFS挂载卷来存储静态资源。</li>
<li>定义了一个名为 <code>item1-tomcat-app1-service</code> 的服务，类型为 <code>NodePort</code>，将集群内部的8080端口映射到节点上的30092端口。</li>
</ul>
<h3 id="2-dockerfile">2. <strong>Dockerfile</strong></h3>
<p>这个Dockerfile基于Harbor镜像仓库中的基础Tomcat镜像构建一个新的Tomcat应用镜像。</p>
<pre><code class="language-dockerfile">FROM harbor.xtec.com/pub-images/tomcat:8.5.43_20250107_235050

# 创建并设置 Tomcat 应用目录
RUN mkdir -p /data/tomcat/webapps/myapp &amp;&amp; \
    chown -R tomcat:tomcat /data/tomcat/webapps/myapp

# 清空默认应用目录并复制文件
COPY catalina.sh /apps/tomcat/bin/catalina.sh
COPY server.xml /apps/tomcat/conf/server.xml
COPY app1.tar.gz /data/tomcat/webapps/myapp/app1.tar.gz
COPY run_tomcat.sh /apps/tomcat/bin/run_tomcat.sh

# 解压应用包并设置权限
RUN tar -xzf /data/tomcat/webapps/myapp/app1.tar.gz -C /data/tomcat/webapps/myapp/ &amp;&amp; \
    rm /data/tomcat/webapps/myapp/app1.tar.gz &amp;&amp; \
    chown -R tomcat:tomcat /data/ /apps/ &amp;&amp; \
    chmod +x /apps/tomcat/bin/catalina.sh /apps/tomcat/bin/run_tomcat.sh

# 清理 Tomcat 工作目录
RUN rm -rf /apps/tomcat/work/Catalina/localhost/*

# 健康检查
HEALTHCHECK CMD curl -f http://localhost:8080 || exit 1

# 暴露端口
EXPOSE 8080 8443

# 启动命令
CMD [&quot;/apps/tomcat/bin/run_tomcat.sh&quot;]
</code></pre>
<p><strong>功能说明</strong>：</p>
<ul>
<li>基于基础Tomcat镜像创建新的镜像。</li>
<li>设置应用目录并复制必要的配置文件和应用程序包。</li>
<li>解压缩应用程序包并设置正确的权限。</li>
<li>清理Tomcat的工作目录以确保干净启动。</li>
<li>添加健康检查以确保Tomcat正常运行。</li>
<li>暴露8080和8443端口，并使用自定义脚本启动Tomcat。</li>
</ul>
<h3 id="3-镜像构建脚本-build-commandsh">3. <strong>镜像构建脚本 (<code>build-command.sh</code>)</strong></h3>
<p>这个脚本用于构建和推送Docker镜像到Harbor镜像仓库。</p>
<pre><code class="language-bash">#!/bin/bash
# build-command.sh - 构建和推送 Docker 镜像脚本
# Author: orochw
# Updated: 2025-01-16

HARBOR=&quot;harbor.xtec.com&quot;
REPO=&quot;item1&quot;
IMAGE=&quot;tomcat&quot;
FIXED_TAG=&quot;8.5.43&quot;
DOCKERFILE_DIR=&quot;/opt/k8s-data/dockerfile/web/item1/tomcat-app1&quot;

# 确保接收时间戳参数
if [ -z &quot;$1&quot; ]; then
  echo &quot;ERROR: Date timestamp is missing!&quot;
  exit 1
fi
DATE=$1
FULL_IMAGE=&quot;$HARBOR/$REPO/$IMAGE:${FIXED_TAG}_$DATE&quot;

log() {
  echo &quot;[$(date +'%Y-%m-%d %H:%M:%S')] $1&quot;
}

ERROR_exit() {
  echo &quot;[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1&quot;
  exit 1
}

# 检查 Docker 是否可用
docker -v &gt;/dev/null 2&gt;&amp;1 || ERROR_exit &quot;Docker 未安装或不可用，请检查环境！&quot;

# 构建 Docker 镜像
log &quot;开始构建 Docker 镜像: $FULL_IMAGE&quot;
docker build -t $FULL_IMAGE $DOCKERFILE_DIR || ERROR_exit &quot;Docker 构建失败！&quot;

# 推送 Docker 镜像
log &quot;开始推送 Docker 镜像: $FULL_IMAGE&quot;
docker push $FULL_IMAGE || ERROR_exit &quot;Docker 推送失败！&quot;

log &quot;Docker 镜像构建和推送完成: $FULL_IMAGE&quot;
</code></pre>
<p><strong>功能说明</strong>：</p>
<ul>
<li>确保传递了时间戳参数，并生成完整的镜像名称。</li>
<li>检查Docker是否可用。</li>
<li>构建并推送镜像到指定的Harbor仓库。</li>
</ul>
<h3 id="4-cicd-脚本">4. <strong>CI/CD 脚本</strong></h3>
<p>这个脚本实现了从代码克隆、打包、传输、镜像构建、更新Kubernetes配置到应用更改的整个CI/CD流程。</p>
<pre><code class="language-bash">#!/bin/bash

# 配置部分
HARBOR=&quot;harbor.xtec.com&quot;
REPO=&quot;item1&quot;
IMAGE=&quot;tomcat&quot;
FIXED_TAG=&quot;8.5.43&quot;
K8S_CONTROLLER=&quot;172.31.7.11&quot;
GIT_URL=&quot;git@172.31.7.22:item1/app1.git&quot;
SRC_DIR=&quot;/data/gitdata/item1/app1&quot;
DEST_DIR=&quot;/opt/k8s-data/dockerfile/web/item1/tomcat-app1&quot;
K8S_YAML=&quot;/opt/k8s-data/yaml/item1/tomcat-app1/tomcat-app1.yaml&quot;

# 记录开始时间
starttime=$(date +'%Y-%m-%d %H:%M:%S')

# 函数定义
log() {
  echo &quot;[$(date +'%Y-%m-%d %H:%M:%S')] $1&quot;
}

ERROR_exit() {
  echo &quot;[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1&quot;
  exit 1
}

code_clone() {
  log &quot;开始拉取代码分支: $BRANCH&quot;
  mkdir -p $(dirname $SRC_DIR)
  rm -rf $SRC_DIR
  git clone -b $BRANCH $GIT_URL $SRC_DIR || ERROR_exit &quot;Git 克隆失败！&quot;
  log &quot;代码拉取完成&quot;
}

package_file() {
  log &quot;开始打包代码文件&quot;
  mkdir -p $DEST_DIR
  tar -czf $DEST_DIR/app1.tar.gz -C $SRC_DIR . || ERROR_exit &quot;文件打包失败！&quot;
  log &quot;文件打包完成: $DEST_DIR/app1.tar.gz&quot;
}

copy_file() {
  log &quot;开始复制文件到 K8S 控制器: $K8S_CONTROLLER&quot;
  ssh root@$K8S_CONTROLLER &quot;mkdir -p $DEST_DIR&quot;
  scp -r $DEST_DIR/* root@$K8S_CONTROLLER:$DEST_DIR || ERROR_exit &quot;文件复制失败！&quot;
  log &quot;文件成功复制到 $K8S_CONTROLLER:$DEST_DIR&quot;
}

build_image() {
  log &quot;开始构建和推送 Docker 镜像: $FULL_IMAGE&quot;
  ssh root@$K8S_CONTROLLER &lt;&lt;EOF
    cd $DEST_DIR
    chmod +x build-command.sh
    ./build-command.sh $DATE || exit 1
EOF
  [ $? -ne 0 ] &amp;&amp; ERROR_exit &quot;Docker 镜像构建或推送失败！&quot;
  log &quot;Docker 镜像构建并推送完成: $FULL_IMAGE&quot;
}

update_k8s_yaml() {
  log &quot;开始更新 Kubernetes YAML 文件&quot;
  ssh root@$K8S_CONTROLLER &lt;&lt;EOF
    if [ -f $K8S_YAML ]; then
      sed -i &quot;s|image: $HARBOR/$REPO/$IMAGE:.*|image: $FULL_IMAGE|g&quot; $K8S_YAML
      if ! grep -q 'imagePullSecrets' $K8S_YAML; then
        sed -i '/containers:/i \      imagePullSecrets:\n      - name: harbor-secret' $K8S_YAML
      fi
    else
      echo &quot;ERROR: Kubernetes YAML 文件不存在: $K8S_YAML&quot;
      exit 1
    fi
EOF
  [ $? -ne 0 ] &amp;&amp; ERROR_exit &quot;Kubernetes YAML 文件更新失败！&quot;
  log &quot;Kubernetes YAML 文件更新完成&quot;
}

apply_k8s_changes() {
  log &quot;应用 Kubernetes 部署更新&quot;
  ssh root@$K8S_CONTROLLER &quot;kubectl apply -f $K8S_YAML&quot; || ERROR_exit &quot;Kubernetes 部署更新失败！&quot;
  log &quot;Kubernetes 部署更新完成&quot;
}

rollback_last_version() {
  log &quot;开始回滚到上一个版本&quot;
  ssh root@$K8S_CONTROLLER &quot;kubectl rollout undo deployment/item1-tomcat-app1-deployment -n item1&quot; || ERROR_exit &quot;回滚失败！&quot;
  log &quot;回滚成功&quot;
}

# 显示用法
usage() {
  echo &quot;Usage: $0 {deploy|rollback_last_version} [branch]&quot;
}

# 参数处理
OPERATION=${1:-deploy}
BRANCH=${2:-main} # 默认分支为 main
DATE=$(date +%Y%m%d_%H%M%S)
FULL_IMAGE=&quot;$HARBOR/$REPO/$IMAGE:${FIXED_TAG}_$DATE&quot;

# 主逻辑
case $OPERATION in
  deploy)
    code_clone
    package_file
    copy_file
    build_image
    update_k8s_yaml
    apply_k8s_changes
    ;;
  rollback_last_version)
    rollback_last_version
    ;;
  *)
    usage
    exit 1
    ;;
esac

# 显示总耗时
endtime=$(date +'%Y-%m-%d %H:%M:%S')
start_seconds=$(date --date=&quot;$starttime&quot; +%s)
end_seconds=$(date --date=&quot;$endtime&quot; +%s)
log &quot;总执行时间: $((end_seconds - start_seconds)) 秒&quot;
</code></pre>
<p><strong>功能说明</strong>：</p>
<ul>
<li><strong>code_clone</strong>: 从指定Git仓库拉取特定分支的代码。</li>
<li><strong>package_file</strong>: 将代码打包成<code>.tar.gz</code>格式。</li>
<li><strong>copy_file</strong>: 将打包好的文件复制到Kubernetes控制器节点。</li>
<li><strong>build_image</strong>: 在Kubernetes控制器上构建并推送Docker镜像。</li>
<li><strong>update_k8s_yaml</strong>: 更新Kubernetes YAML文件中的镜像地址，并添加<code>imagePullSecrets</code>部分。</li>
<li><strong>apply_k8s_changes</strong>: 应用更新后的YAML文件到Kubernetes集群。</li>
<li><strong>rollback_last_version</strong>: 回滚到上一个版本。</li>
<li><strong>主逻辑</strong>: 根据传入的操作类型（<code>deploy</code>或<code>rollback_last_version</code>）执行相应的操作。</li>
</ul>
<h3 id="脚本中的函数及其作用">脚本中的函数及其作用</h3>
<p>以下是脚本中定义的主要函数及其作用：</p>
<ol>
<li>
<p><strong><code>log()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 打印带时间戳的日志信息，便于调试和追踪脚本执行过程。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">log &quot;开始构建 Docker 镜像: $FULL_IMAGE&quot;
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>ERROR_exit()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 当发生错误时打印错误信息并退出脚本执行，确保问题不会被忽略。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">ERROR_exit &quot;Docker 构建失败！&quot;
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>code_clone()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 从指定Git仓库克隆代码到本地目录，以便后续步骤可以访问这些代码进行打包和构建。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">code_clone
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>package_file()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 将克隆下来的代码打包成压缩文件（如<code>.tar.gz</code>），以便传输到Kubernetes控制器节点。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">package_file
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>copy_file()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 使用<code>scp</code>命令将打包好的文件复制到Kubernetes控制器节点上的指定目录。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">copy_file
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>build_image()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 在Kubernetes控制器节点上运行<code>build-command.sh</code>脚本，实际执行镜像的构建和推送操作。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">build_image
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>update_k8s_yaml()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 更新Kubernetes YAML配置文件，确保其引用的是最新构建的镜像。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">update_k8s_yaml
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>apply_k8s_changes()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 应用更新后的Kubernetes YAML配置文件，触发集群内的部署更新。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">apply_k8s_changes
</code></pre>
</li>
</ul>
</li>
<li>
<p><strong><code>rollback_last_version()</code></strong>:</p>
<ul>
<li><strong>作用</strong>: 回滚到前一个稳定版本，使用Kubernetes的<code>rollout undo</code>命令实现。</li>
<li><strong>示例调用</strong>:<pre><code class="language-bash">rollback_last_version
</code></pre>
</li>
</ul>
</li>
</ol>
<h2 id="这些函数共同构成了一个完整的自动化流程使得从代码提交到生产环境部署变得更加高效且可管理">这些函数共同构成了一个完整的自动化流程，使得从代码提交到生产环境部署变得更加高效且可管理。</h2>
<h1 id="四-创建jenkins任务">四、创建Jenkins任务</h1>
<h2 id="1-安装ssh插件及配置凭据">1. 安装SSH插件及配置凭据</h2>
<h3 id="插件安装">插件安装</h3>
<ul>
<li><strong>步骤</strong>：在Jenkins中安装SSH插件。</li>
<li><strong>截图</strong>：<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117024646.png" alt="插件安装" loading="lazy"></li>
</ul>
<h3 id="远程主机设置">远程主机设置</h3>
<ul>
<li><strong>创建新的凭据</strong>：
<ul>
<li>登录Jenkins -&gt; 系统管理 -&gt; 凭据 -&gt; 全局 -&gt; 新增凭据</li>
<li>结果如图：<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117024756.png" alt="远程主机设置" loading="lazy"></li>
</ul>
</li>
<li><strong>系统管理</strong> -&gt; <strong>系统</strong> -&gt; <strong>SSH Remote Hosts</strong>
<ul>
<li>设置如图：<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117024934.png" alt="SSH远程主机" loading="lazy"></li>
</ul>
</li>
</ul>
<h2 id="2-创建任务">2. 创建任务</h2>
<ul>
<li><strong>新建任务</strong>：选择“自由风格项目”类型。
<ul>
<li>截图：<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117024436.png" alt="创建任务" loading="lazy"></li>
<li>配置Git仓库地址和分支。</li>
<li>构建触发器：可以设置定时构建、轮询SCM等触发条件。</li>
<li>构建环境：如果需要，可以设置环境变量等。</li>
<li>构建：添加执行Shell脚本的步骤，调用前面准备好的CI/CD脚本进行构建、打包、推送镜像以及更新Kubernetes YAML文件等操作。</li>
</ul>
</li>
</ul>
<h1 id="五-测试cicd流程">五、测试CI/CD流程</h1>
<h2 id="更新测试">更新测试</h2>
<h3 id="修改indexhtml">修改index.html</h3>
<ul>
<li>对应用的<code>index.html</code>文件进行了修改，作为版本更新的一部分。</li>
<li>触发Jenkins任务执行更新流程。</li>
<li>控制台输出显示了从克隆代码、打包、传输文件、构建和推送Docker镜像、更新Kubernetes YAML文件到最后应用更改的全过程，并且最终结果显示为SUCCESS。</li>
</ul>
<pre><code class="language-bash">Started by user root
Running as SYSTEM
Building in workspace /var/lib/jenkins/workspace/bash-update-item1-tomcat-app1-deploy
...
Successfully built a524b7dbb530
Successfully tagged harbor.xtec.com/item1/tomcat:8.5.43_20250117_025158
...
deployment.apps/item1-tomcat-app1-deployment configured
service/item1-tomcat-app1-service unchanged
[2025-01-17 02:52:09] Kubernetes ������������������
[2025-01-17 02:52:09] ���������������: 11 ���
Finished: SUCCESS
</code></pre>
<h3 id="查看更新前后的结果">查看更新前后的结果</h3>
<ul>
<li>截图：<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117025413.png" alt="更新前后对比" loading="lazy"></li>
<li><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20250117025433.png" alt="更新前后对比" loading="lazy"></li>
</ul>
<h2 id="测试回滚">测试回滚</h2>
<pre><code class="language-bash">Started by user root
Running as SYSTEM
Building in workspace /var/lib/jenkins/workspace/bash-update-item1-tomcat-app1-deploy
...
deployment.apps/item1-tomcat-app1-deployment rolled back
[2025-01-17 01:32:03] ������������
[2025-01-17 01:32:03] ���������������: 0 ���
Finished: SUCCESS
</code></pre>
<hr>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ssh终端tabby语法高亮]]></title>
        <id>https://orochw.github.io/post/ssh-zhong-duan-tabby-yu-fa-gao-liang/</id>
        <link href="https://orochw.github.io/post/ssh-zhong-duan-tabby-yu-fa-gao-liang/">
        </link>
        <updated>2025-01-06T18:10:10.000Z</updated>
        <content type="html"><![CDATA[<pre><code>{
    &quot;id&quot;: &quot;60606be0-c0ff-42bc-bf77-de8a2435447f&quot;,
    &quot;name&quot;: &quot;Default&quot;,
    &quot;keywords&quot;: [
      {
        &quot;text&quot;: &quot;([_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;14&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(?=(\\b|\\D))(((\\d{1,2})|(1\\d{1,2})|(2[0-4]\\d)|(25[0-5]))\\.){3}((\\d{1,2})|(1\\d{1,2})|(2[0-4]\\d)|(25[0-5]))(?=(\\b|\\D))\\s*&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;4&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(http(s)?://[a-zA-Z0-9_.&amp;?=%~#{}()@:+/-]+)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;4&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(\\berror\\b)|(\\bfail(ed)?\\b)|(\\bfalse\\b)|(\\bdown\\b)|(\\blocked\\b)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;1&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(\\bactive(d)?\\b)|(\\bsuccess(ful(ly)?)?\\b)|(\\btrue\\b)|(\\bok\\b)|(\\bup\\b)|(\\brunning\\b)|(\\bdeployed\\b)|(\\bunlocked\\b)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;10&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(\\bstart(ed|ing)?\\b)|(\\bbegin(ning)?\\b)|(\\benable(d)?\\b)|(\\bcreate(d)?\\b)|(\\bopen\\b)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;10&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(\\bstop(ped)?\\b)|(\\bend\\b)|(\\bfinish(ed)?\\b)|(\\bdisable(d)?\\b)|(\\bdelete(d)?\\b)|(\\bclose(d)?\\b)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;13&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;(\\bwarn(ing)?\\b)|(\\binactive\\b)|(\\bunknown\\b)&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;11&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;\\bDEBUG\\b&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;13&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;\\binfo\\b&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;6&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      },
      {
        &quot;text&quot;: &quot;\\/\\b(\\d|([1-9]\\d)|(1[01]\\d)|(12[0-8]))\\b&quot;,
        &quot;enabled&quot;: true,
        &quot;background&quot;: false,
        &quot;backgroundColor&quot;: &quot;0&quot;,
        &quot;foregroundColor&quot;: &quot;4&quot;,
        &quot;isRegExp&quot;: true,
        &quot;foreground&quot;: true,
        &quot;isCaseSensitive&quot;: false
      }
    ]
  }
  
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubernetes v1.29.0 支持docker的安装配置]]></title>
        <id>https://orochw.github.io/post/kubernetes-v1290-zhi-chi-docker-de-an-zhuang-pei-zhi/</id>
        <link href="https://orochw.github.io/post/kubernetes-v1290-zhi-chi-docker-de-an-zhuang-pei-zhi/">
        </link>
        <updated>2024-11-29T21:36:18.000Z</updated>
        <content type="html"><![CDATA[<h1 id="kubernetes-安装和配置指南">Kubernetes 安装和配置指南</h1>
<p>Kubernetes 本身并不直接支持 Docker 作为容器运行时，而是通过 Container Runtime Interface (CRI) 来与不同的容器运行时进行交互。CRI 是 Kubernetes 定义的一种标准接口，允许 Kubernetes 与多种容器运行时（如 containerd、CRI-O 等）进行通信。</p>
<p>Docker 公司开发了一个适配器 <code>cri-dockerd</code>，使得 Kubernetes 可以通过 CRI 接口与 Docker 进行通信。然而，这种方法并不是官方推荐的方式，并且存在一些潜在的问题和限制：</p>
<ol>
<li><strong>维护和支持</strong>：<code>cri-dockerd</code> 并不是由 Kubernetes 社区官方维护的项目，因此在遇到问题时可能得不到及时的支持。</li>
<li><strong>性能和稳定性</strong>：由于额外的适配层，使用 <code>cri-dockerd</code> 可能会导致一定的性能开销，并且可能存在稳定性问题。</li>
<li><strong>功能兼容性</strong>：某些高级功能和优化可能无法完全通过 <code>cri-dockerd</code> 实现，导致无法充分利用 Kubernetes 的全部潜力。</li>
<li><strong>社区趋势</strong>：大多数 Kubernetes 用户和社区都在转向使用 containerd 或 CRI-O，这些是 Kubernetes 官方推荐的容器运行时。</li>
</ol>
<p>因此，虽然可以通过 <code>cri-dockerd</code> 在 Kubernetes 中使用 Docker，但为了更好的性能、稳定性和社区支持，建议使用官方推荐的容器运行时，如 containerd 或 CRI-O。</p>
<p>如果你仍然希望继续使用 Docker，可以参考以下步骤安装和配置 <code>cri-dockerd</code>：</p>
<h3 id="安装-containerd">安装 containerd</h3>
<p>containerd 是一个轻量级的容器运行时，也是 Kubernetes 官方推荐的默认容器运行时之一。</p>
<pre><code class="language-bash"># 配置 containerd 的 yum 源
cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/containerd.repo
[containerd]
name=containerd NEAR Repo
baseurl=https://download.opensuse.org/repositories/home:/kubic:/project:/containers:/stable:/cri-o-1.29/CentOS_8/
enabled=1
gpgcheck=1
gpgkey=https://download.opensuse.org/repositories/home:/kubic:/project:/containers:/stable:/cri-o-1.29/CentOS_8/repodata/repomd.xml.key
EOF

# 安装 containerd
sudo yum install -y containerd

# 配置 containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

# 启动并启用 containerd 服务
sudo systemctl restart containerd
sudo systemctl enable containerd
</code></pre>
<h3 id="配置-kubernetes-使用-containerd">配置 Kubernetes 使用 containerd</h3>
<p>编辑 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 文件，确保 <code>--container-runtime</code> 设置为 <code>remote</code>，并且 <code>--container-runtime-endpoint</code> 设置为 <code>unix:///run/containerd/containerd.sock</code>。</p>
<pre><code class="language-bash"># 编辑 kubelet 配置文件
sudo sed -i 's/--container-runtime=docker/--container-runtime=remote/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo sed -i 's/--container-runtime-endpoint=unix:\/\/\/var\/run\/docker.sock/--container-runtime-endpoint=unix:\/\/\/run\/containerd\/containerd.sock/' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

# 重启 kubelet 服务
sudo systemctl daemon-reload
sudo systemctl restart kubelet
</code></pre>
<p>通过以上步骤，你可以将 Kubernetes 配置为使用 containerd 作为容器运行时，从而获得更好的性能和稳定性。</p>
<h2 id="1-修改主机名">1. 修改主机名</h2>
<pre><code class="language-bash">hostnamectl set-hostname &lt;your-hostname&gt;
</code></pre>
<h2 id="2-安装-ipvs">2. 安装 IPVS</h2>
<pre><code class="language-bash">yum install -y ipvsadm
</code></pre>
<h2 id="3-开启路由转发">3. 开启路由转发</h2>
<pre><code class="language-bash">echo &quot;net.ipv4.ip_forward=1&quot; &gt;&gt; /etc/sysctl.conf
yum install -y epel-release
yum install -y bridge-utils
modprobe br_netfilter
echo 'br_netfilter' &gt;&gt; /etc/modules-load.d/bridge.conf
echo 'net.bridge.bridge-nf-call-iptables=1' &gt;&gt; /etc/sysctl.conf
echo 'net.bridge.bridge-nf-call-ip6tables=1' &gt;&gt; /etc/sysctl.conf
sysctl -p
</code></pre>
<h2 id="4-安装-docker">4. 安装 Docker</h2>
<pre><code class="language-bash">sudo dnf config-manager --add-repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo
cd /etc/yum.repos.d
sed -i 's|download.docker.com|mirrors.ustc.edu.cn/docker-ce|g' docker-ce.repo
yum -y install docker-ce

cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;data-root&quot;: &quot;/data/docker&quot;,
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;,
    &quot;max-file&quot;: &quot;100&quot;
  },
  &quot;insecure-registries&quot;: [&quot;harbor.xinxainghf.com&quot;],
  &quot;registry-mirrors&quot;: [&quot;https://kfp63jaj.mirror.aliyuncs.com&quot;]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d
systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl enable docker
</code></pre>
<h2 id="5-安装-cri-dockerd-cri-翻译器">5. 安装 cri-dockerd -&gt; CRI 翻译器</h2>
<pre><code class="language-bash">wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.9/cri-dockerd-0.3.9.amd64.tgz
tar -xf cri-dockerd-0.3.9.amd64.tgz
cp cri-dockerd/cri-dockerd /usr/bin/
chmod +x /usr/bin/cri-dockerd
</code></pre>
<h2 id="6-配置套接字和编写启动配置">6. 配置套接字和编写启动配置</h2>
<pre><code class="language-bash">cat &gt; /usr/lib/systemd/system/cri-docker.service &lt;&lt;EOF
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify
ExecStart=/usr/bin/cri-dockerd --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.8
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF

cat &gt; /usr/lib/systemd/system/cri-docker.socket &lt;&lt;EOF
[Unit]
Description=CRI Docker socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker

[Install]
WantedBy=sockets.target
EOF
</code></pre>
<h2 id="7-检查服务">7. 检查服务</h2>
<pre><code class="language-bash">systemctl daemon-reload
systemctl enable cri-docker
systemctl start cri-docker
systemctl is-active cri-docker
</code></pre>
<h2 id="8-安装-kubernetes">8. 安装 Kubernetes</h2>
<pre><code class="language-bash">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
EOF

yum install -y kubelet-1.29.0 kubectl-1.29.0 kubeadm-1.29.0
systemctl enable kubelet.service
</code></pre>
<h2 id="9-初始化-master-节点">9. 初始化 Master 节点</h2>
<p>在 Master 节点上执行以下命令：</p>
<pre><code class="language-bash">kubeadm init --apiserver-advertise-address=192.168.234.11 --image-repository registry.aliyuncs.com/google_containers --kubernetes-version 1.29.1 --service-cidr=10.10.0.0/12 --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<h2 id="10-加入-worker-节点">10. 加入 Worker 节点</h2>
<p>在 Worker 节点上执行以下命令：</p>
<pre><code class="language-bash">kubeadm join 192.168.234.11:6443 --token vglwnn.qy5s0raiun1yufdi --discovery-token-ca-cert-hash sha256:951d73488711366cbc3028d93c4cdc52c240d78ff49b32fbef942e1eee9c7454 --cri-socket unix:///var/run/cri-dockerd.sock
</code></pre>
<h2 id="11-安装网络插件-calico">11. 安装网络插件 Calico</h2>
<p>编辑 Calico 安装 YAML 文件并应用：</p>
<pre><code class="language-bash">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
</code></pre>
<p>根据需要修改以下配置：</p>
<pre><code class="language-yaml">- name: CALICO_IPV4POOL_CIDR
  value: &quot;10.244.0.0/16&quot;

- name: CALICO_IPV4POOL_VXLAN
  value: &quot;Always&quot;
</code></pre>
<h2 id="12-安装-kubernetes-命令增强补全">12. 安装 Kubernetes 命令增强补全</h2>
<pre><code class="language-bash">yum install bash-completion -y
echo &quot;source /usr/share/bash-completion/bash_completion&quot; &gt;&gt; ~/.bashrc
echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[rsync同步]]></title>
        <id>https://orochw.github.io/post/rsync-tong-bu/</id>
        <link href="https://orochw.github.io/post/rsync-tong-bu/">
        </link>
        <updated>2024-10-19T17:21:10.000Z</updated>
        <content type="html"><![CDATA[<h1 id="rsync">rsync</h1>
<h2 id="一-介绍">一、介绍</h2>
<p><code>rsync</code> 是一个功能强大的、高效的用于数据镜像（备份）、文件传输和同步的工具。它可以在本地主机和远程主机之间进行快速同步，并且只传输两个机器之间不同的部分。</p>
<h3 id="基本用法">基本用法</h3>
<pre><code class="language-bash">rsync [选项] 源目录 目标目录
</code></pre>
<ul>
<li><strong>源目录</strong>：要同步的源文件或目录。</li>
<li><strong>目标目录</strong>：同步的目标文件或目录。</li>
</ul>
<h3 id="常用选项">常用选项</h3>
<ul>
<li><code>-a</code>：归档模式，保留所有文件属性，包括权限、时间、组、所有者等。等同于 <code>-rlptgoD</code>。</li>
<li><code>-r</code>：递归处理目录。</li>
<li><code>-l</code>：保留软链接。</li>
<li><code>-p</code>：保留权限。</li>
<li><code>-t</code>：保留修改时间。</li>
<li><code>-g</code>：保留组。</li>
<li><code>-o</code>：保留所有者。</li>
<li><code>-D</code>：保留设备文件和特殊文件。</li>
<li><code>-v</code>：详细模式，显示详细的传输过程。</li>
<li><code>-z</code>：压缩传输数据，可以加快传输速度，尤其是对于文本文件。</li>
<li><code>-e</code>：指定远程 shell，通常为 <code>ssh</code>。</li>
<li><code>--delete</code>：删除目标目录中不存在于源目录的文件。</li>
<li><code>--exclude=PATTERN</code>：排除符合 PATTERN 的文件或目录。</li>
<li><code>--include=PATTERN</code>：仅包含符合 PATTERN 的文件或目录。</li>
</ul>
<h2 id="二-安装配置">二、安装配置</h2>
<h3 id="1-检查是否安装">1. 检查是否安装</h3>
<pre><code class="language-bash">[root@234-12 ~]# rpm -qa | grep rsync
rsync-3.1.2-12.el7_9.x86_64
</code></pre>
<p>如果命令结果为空，可以使用以下命令安装：</p>
<pre><code class="language-bash">yum install -y rsync
</code></pre>
<h3 id="2-配置文件解析">2. 配置文件解析</h3>
<h4 id="查看-rsync-安装文件情况">查看 rsync 安装文件情况</h4>
<pre><code class="language-bash">[root@234-12 ~]# rpm -ql rsync
/etc/rsyncd.conf          # 配置文件
/etc/sysconfig/rsyncd     # 守护进程配置
/usr/bin/rsync            # 命令所在目录
/usr/lib/systemd/system/rsyncd.service  # systemctl 服务管理
/usr/lib/systemd/system/rsyncd.socket    # 进程生成的套接字文件
/usr/lib/systemd/system/rsyncd@.service
/usr/share/doc/rsync-3.1.2
/usr/share/doc/rsync-3.1.2/COPYING
/usr/share/doc/rsync-3.1.2/NEWS
/usr/share/doc/rsync-3.1.2/OLDNEWS
/usr/share/doc/rsync-3.1.2/README
/usr/share/doc/rsync-3.1.2/support
/usr/share/doc/rsync-3.1.2/support/Makefile
/usr/share/doc/rsync-3.1.2/support/atomic-rsync
/usr/share/doc/rsync-3.1.2/support/cvs2includes
/usr/share/doc/rsync-3.1.2/support/deny-rsync
/usr/share/doc/rsync-3.1.2/support/file-attr-restore
/usr/share/doc/rsync-3.1.2/support/files-to-excludes
/usr/share/doc/rsync-3.1.2/support/git-set-file-times
/usr/share/doc/rsync-3.1.2/support/instant-rsyncd
/usr/share/doc/rsync-3.1.2/support/logfilter
/usr/share/doc/rsync-3.1.2/support/lsh
/usr/share/doc/rsync-3.1.2/support/lsh.sh
/usr/share/doc/rsync-3.1.2/support/mapfrom
/usr/share/doc/rsync-3.1.2/support/mapto
/usr/share/doc/rsync-3.1.2/support/mnt-excl
/usr/share/doc/rsync-3.1.2/support/munge-symlinks
/usr/share/doc/rsync-3.1.2/support/rrsync
/usr/share/doc/rsync-3.1.2/support/rsync-no-vanished
/usr/share/doc/rsync-3.1.2/support/rsync-slash-strip
/usr/share/doc/rsync-3.1.2/support/rsyncstats
/usr/share/doc/rsync-3.1.2/support/savetransfer.c
/usr/share/doc/rsync-3.1.2/tech_report.tex
/usr/share/man/man1/rsync.1.gz
/usr/share/man/man5/rsyncd.conf.5.gz
</code></pre>
<h4 id="rsync-配置文件详解">rsync 配置文件详解</h4>
<pre><code class="language-bash"># rsync 配置文件详解

# uid = nobody  # 指定 rsyncd 运行时的用户
# gid = nobody  # 指定 rsyncd 运行时的组
# use chroot = yes  # 启用 chroot 功能，提高安全性
# max connections = 4  # 限制同时可以连接到 rsyncd 的最大连接数
# pid file = /var/run/rsyncd.pid  # 指定 rsyncd 进程的 PID 文件
# exclude = lost+found/  # 排除 lost+found 目录
# transfer logging = yes  # 启用传输日志记录
# timeout = 900  # 设置连接超时时间
# ignore nonreadable = yes  # 忽略无法读取的文件
# dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2  # 不压缩某些类型的文件

# [ftp]
#       path = /home/ftp  # 指定 &quot;ftp&quot; 模块的同步根目录
#       comment = ftp export area  # 注释，说明 &quot;ftp&quot; 模块的作用
</code></pre>
<h2 id="三-本地同步">三、本地同步</h2>
<p>以下是实现文件监控和自动同步的方案：</p>
<h3 id="1-安装-inotify-tools">1. 安装 inotify-tools</h3>
<p><code>inotify-tools</code> 是一个 Linux 实时文件系统监控工具，可以监控目录的文件变动。首先，你需要在 CentOS 上安装它。</p>
<pre><code class="language-bash">sudo yum install inotify-tools -y
</code></pre>
<h3 id="2-创建监控和同步脚本">2. 创建监控和同步脚本</h3>
<p>编写一个脚本，当 <code>/home/zhangsan</code>、<code>/home/lisi</code>、<code>/home/wangwu</code> 目录发生变动时自动触发 <code>rsync</code> 同步到 <code>/backup</code> 目录。</p>
<h4 id="脚本示例rsync_inotify_syncsh">脚本示例：<code>rsync_inotify_sync.sh</code></h4>
<pre><code class="language-bash">#!/bin/bash

# 定义要监控的目录
SOURCE_DIRS=(&quot;/home/zhangsan&quot; &quot;/home/lisi&quot; &quot;/home/wangwu&quot;)
BACKUP_DIR=&quot;/backup&quot;

# 创建备份目录
mkdir -p &quot;$BACKUP_DIR&quot;

# 监控目录并实时同步
for dir in &quot;${SOURCE_DIRS[@]}&quot;; do
    # 获取当前目录的名称
    BASENAME=$(basename &quot;$dir&quot;)

    echo &quot;正在监控目录：$dir&quot;

    # 使用 inotifywait 监控创建、修改和删除事件
    inotifywait -mrq -e modify,create,delete &quot;$dir&quot; | while read path action file; do
        echo &quot;检测到 $dir 中的变化，正在同步到 $BACKUP_DIR/$BASENAME&quot;
        # 使用 rsync 同步变化的目录
        rsync -av --delete &quot;$dir/&quot; &quot;$BACKUP_DIR/$BASENAME/&quot;
        echo &quot;[$(date)] $dir 同步到 $BACKUP_DIR/$BASENAME 完成&quot;
    done &amp;
done
</code></pre>
<h4 id="赋予脚本执行权限">赋予脚本执行权限</h4>
<p>将脚本保存为 <code>/usr/local/bin/rsync_inotify_sync.sh</code>，并赋予执行权限：</p>
<pre><code class="language-bash">sudo chmod +x /usr/local/bin/rsync_inotify_sync.sh
</code></pre>
<h3 id="3-后台运行脚本">3. 后台运行脚本</h3>
<p>为了让监控和同步脚本长期运行，你可以将其放在后台执行。例如，使用 <code>nohup</code> 或 <code>&amp;</code> 让脚本在后台运行：</p>
<pre><code class="language-bash">nohup /usr/local/bin/rsync_inotify_sync.sh &gt; /var/log/rsync_inotify_sync.log 2&gt;&amp;1 &amp;
</code></pre>
<p>这里使用 <code>nohup</code> 确保脚本在你退出终端后仍然运行。日志将记录在 <code>/var/log/rsync_inotify_sync.log</code> 中。</p>
<h3 id="4-使用-systemd-实现自动启动">4. 使用 systemd 实现自动启动</h3>
<p>如果你希望系统启动时自动开始文件监控，可以创建一个 systemd 服务。</p>
<h4 id="创建-systemd-服务文件">创建 systemd 服务文件</h4>
<p>创建一个服务文件 <code>/etc/systemd/system/rsync_inotify.service</code>：</p>
<pre><code class="language-bash">[Unit]
Description=Rsync Inotify File Sync
After=network.target

[Service]
ExecStart=/usr/local/bin/rsync_inotify_sync.sh
Restart=always

[Install]
WantedBy=multi-user.target
</code></pre>
<h4 id="启用并启动服务">启用并启动服务</h4>
<p>执行以下命令以启用并启动该服务：</p>
<pre><code class="language-bash">sudo systemctl daemon-reload
sudo systemctl enable rsync_inotify.service
sudo systemctl start rsync_inotify.service
</code></pre>
<h3 id="5-日志管理">5. 日志管理</h3>
<p>脚本运行时会产生日志。你可以使用 <code>logrotate</code> 来管理日志文件，以防日志过大。创建 <code>logrotate</code> 配置文件 <code>/etc/logrotate.d/rsync_inotify_sync</code>：</p>
<pre><code class="language-bash">/var/log/rsync_inotify_sync.log {
    weekly
    rotate 4
    compress
    missingok
    notifempty
    create 640 root root
}
</code></pre>
<h2 id="四-远端同步-服务器-a-和服务器-b-配置文档">四、远端同步 服务器 A 和服务器 B 配置文档</h2>
<h3 id="服务器信息">服务器信息</h3>
<table>
<thead>
<tr>
<th>服务器</th>
<th>IP 地址</th>
<th>角色</th>
<th>安装的软件</th>
</tr>
</thead>
<tbody>
<tr>
<td>服务器 A</td>
<td>192.168.234.11</td>
<td>源服务器：负责监控 <code>/data/nfs_share</code> 目录的变化，并通过 <code>rsync</code> 将变化同步到服务器 B</td>
<td><code>nfs-utils</code>, <code>rsync</code>, <code>sersync</code></td>
</tr>
<tr>
<td>服务器 B</td>
<td>192.168.234.12</td>
<td>目标服务器：接收来自服务器 A 的文件同步，并存储在 <code>/data/backup</code> 目录中</td>
<td><code>nfs-utils</code>, <code>rsync</code></td>
</tr>
</tbody>
</table>
<h3 id="服务器-a-19216823411">服务器 A (192.168.234.11)</h3>
<h4 id="角色">角色</h4>
<ul>
<li><strong>源服务器</strong>：负责监控 <code>/data/nfs_share</code> 目录的变化，并通过 <code>rsync</code> 将变化同步到服务器 B。</li>
</ul>
<h4 id="流程">流程</h4>
<ol>
<li>
<p><strong>安装必要的软件</strong></p>
<ul>
<li><code>nfs-utils</code>：用于设置 NFS 共享。</li>
<li><code>rsync</code>：用于文件同步。</li>
<li><code>sersync</code>：用于实时监控目录变化并触发 <code>rsync</code> 同步。</li>
</ul>
</li>
<li>
<p><strong>设置系统参数</strong></p>
<ul>
<li>设置 <code>inotify</code> 参数以支持更多的文件系统事件监听。
<ul>
<li>设置 <code>inotify</code> 参数：
<ul>
<li><code>fs.inotify.max_user_watches=50000000</code></li>
<li><code>fs.inotify.max_queued_events=327679</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>配置 NFS 共享</strong></p>
<ul>
<li>创建共享目录 <code>/data/nfs_share</code>。</li>
<li>编辑 <code>/etc/exports</code> 文件，允许服务器 B 访问共享目录。</li>
<li>启动 NFS 服务并导出共享目录。
<ul>
<li>创建共享目录 <code>/data/nfs_share</code> 并设置权限。</li>
<li>编辑 <code>/etc/exports</code> 文件，添加共享目录配置。</li>
<li>启动 NFS 服务并导出共享目录。</li>
<li>查看导出的目录。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>配置 <code>rsync</code></strong></p>
<ul>
<li>创建 <code>rsync</code> 密码文件 <code>/etc/rsync.password</code>。
<ul>
<li>创建密码文件并设置权限。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>配置 <code>sersync</code></strong></p>
<ul>
<li>解压 <code>sersync</code> 软件包并将其移动到 <code>/app/sersync2</code>。</li>
<li>编辑 <code>sersync</code> 配置文件 <code>/app/sersync2/confxml.xml</code>，配置监控目录和远程服务器信息。</li>
<li>启动 <code>sersync</code> 服务并设置开机自启动。
<ul>
<li>编辑 <code>sersync</code> 配置文件。</li>
<li>启动 <code>sersync</code> 服务。</li>
<li>设置 <code>sersync</code> 开机自启动。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="安装的软件">安装的软件</h4>
<ul>
<li><code>nfs-utils</code></li>
<li><code>rsync</code></li>
<li><code>sersync</code></li>
</ul>
<h4 id="脚本a并执行">脚本A并执行</h4>
<pre><code class="language-bash">#!/bin/bash

# 设置系统参数
echo 50000000 &gt; /proc/sys/fs/inotify/max_user_watches
echo 327679 &gt; /proc/sys/fs/inotify/max_queued_events

# 永久设置系统参数
echo 'fs.inotify.max_user_watches=50000000' | sudo tee -a /etc/sysctl.conf
echo 'fs.inotify.max_queued_events=327679' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# 解压 sersync 软件包并设置路径
cd /app
# 确保解压到正确的目录，并命名为 sersync2
tar -xzvf sersync2.5.4_64bit_binary_stable_final.tar.gz
# 检查解压后的文件夹
if [ -d &quot;GNU-Linux-x86&quot; ]; then
    mv GNU-Linux-x86 sersync2
else
    echo &quot;sersync 解压失败，检查软件包路径或压缩文件。&quot;
    exit 1
fi

# 添加 sersync2 到 PATH
echo 'export PATH=$PATH:/app/sersync2' | sudo tee -a /etc/profile
source /etc/profile

# 安装和配置 NFS 服务
sudo yum install -y nfs-utils

# 创建共享目录
sudo mkdir -p /data/nfs_share
sudo chown -R nobody:nobody /data/nfs_share
sudo chmod -R 777 /data/nfs_share

# 编辑 NFS 配置文件
echo '/data/nfs_share 192.168.234.12(rw,sync,no_subtree_check)' | sudo tee -a /etc/exports

# 启动和配置 NFS 服务
sudo systemctl start rpcbind
sudo systemctl start nfs-server
sudo systemctl enable rpcbind
sudo systemctl enable nfs-server

# 导出共享目录
sudo exportfs -a

# 查看导出的目录
sudo showmount -e

# 安装 rsync
sudo yum install -y rsync

# 编辑 sersync 配置文件
sudo tee /app/sersync2/confxml.xml &lt;&lt;EOF
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;head version=&quot;2.5&quot;&gt;
    &lt;!-- 主机配置，监听本地的 8008 端口 --&gt;
    &lt;host hostip=&quot;localhost&quot; port=&quot;8008&quot;&gt;&lt;/host&gt;

    &lt;!-- 调试模式，关闭调试 --&gt;
    &lt;debug start=&quot;false&quot;/&gt;

    &lt;!-- 文件系统配置，关闭对 XFS 文件系统的支持 --&gt;
    &lt;fileSystem xfs=&quot;false&quot;/&gt;

    &lt;!-- 文件过滤规则，当前关闭文件过滤功能 --&gt;
    &lt;filter start=&quot;false&quot;&gt;
        &lt;exclude expression=&quot;(.*)\.svn&quot;&gt;&lt;/exclude&gt; &lt;!-- 排除 .svn 目录 --&gt;
        &lt;exclude expression=&quot;(.*)\.gz&quot;&gt;&lt;/exclude&gt; &lt;!-- 排除 .gz 文件 --&gt;
        &lt;exclude expression=&quot;^info/*&quot;&gt;&lt;/exclude&gt; &lt;!-- 排除 info 目录下的文件 --&gt;
        &lt;exclude expression=&quot;^static/*&quot;&gt;&lt;/exclude&gt; &lt;!-- 排除 static 目录下的文件 --&gt;
    &lt;/filter&gt;

    &lt;!-- inotify 事件监听配置，启用关键文件事件监听 --&gt;
    &lt;inotify&gt;
        &lt;delete start=&quot;true&quot;/&gt; &lt;!-- 监听文件删除事件 --&gt;
        &lt;createFolder start=&quot;true&quot;/&gt; &lt;!-- 监听目录创建事件 --&gt;
        &lt;createFile start=&quot;true&quot;/&gt; &lt;!-- 监听文件创建事件 --&gt;
        &lt;closeWrite start=&quot;true&quot;/&gt; &lt;!-- 监听文件关闭写入事件 --&gt;
        &lt;moveFrom start=&quot;true&quot;/&gt; &lt;!-- 监听文件移动前事件 --&gt;
        &lt;moveTo start=&quot;true&quot;/&gt; &lt;!-- 监听文件移动后事件 --&gt;
        &lt;attrib start=&quot;false&quot;/&gt; &lt;!-- 关闭文件属性变更事件的监听 --&gt;
        &lt;modify start=&quot;false&quot;/&gt; &lt;!-- 关闭文件修改事件的监听 --&gt;
    &lt;/inotify&gt;

    &lt;!-- sersync 配置 --&gt;
    &lt;sersync&gt;
        &lt;!-- 本地路径监听 --&gt;
        &lt;localpath watch=&quot;/data/nfs_share&quot;&gt;
            &lt;!-- 远程服务器配置，备份到远程服务器 --&gt;
            &lt;remote ip=&quot;192.168.234.12&quot; name=&quot;backup&quot;/&gt;
        &lt;/localpath&gt;

        &lt;!-- rsync 配置 --&gt;
        &lt;rsync&gt;
            &lt;commonParams params=&quot;-avz&quot;/&gt; &lt;!-- rsync 常用参数，压缩并详细输出 --&gt;
            &lt;auth start=&quot;true&quot; users=&quot;rsync_user&quot; passwordfile=&quot;/etc/rsync.password&quot;/&gt; &lt;!-- 启用认证，使用密码文件 --&gt;
            &lt;userDefinedPort start=&quot;false&quot; port=&quot;874&quot;/&gt; &lt;!-- 自定义端口，未启用 --&gt;
            &lt;timeout start=&quot;false&quot; time=&quot;100&quot;/&gt; &lt;!-- 超时时间配置，未启用 --&gt;
            &lt;ssh start=&quot;false&quot;/&gt; &lt;!-- 不使用 SSH 进行同步 --&gt;
        &lt;/rsync&gt;

        &lt;!-- 失败日志配置 --&gt;
        &lt;failLog path=&quot;/tmp/rsync_fail_log.sh&quot; timeToExecute=&quot;60&quot;/&gt; &lt;!-- 定期执行失败日志处理，每 60 秒执行一次 --&gt;

        &lt;!-- 定时任务配置，当前未启用 --&gt;
        &lt;crontab start=&quot;false&quot; schedule=&quot;600&quot;&gt;
            &lt;crontabfilter start=&quot;false&quot;&gt;
                &lt;exclude expression=&quot;*.php&quot;/&gt; &lt;!-- 排除 .php 文件 --&gt;
                &lt;exclude expression=&quot;info/*&quot;/&gt; &lt;!-- 排除 info 目录下的文件 --&gt;
            &lt;/crontabfilter&gt;
        &lt;/crontab&gt;

        &lt;!-- 插件配置，当前未启用 --&gt;
        &lt;plugin start=&quot;false&quot; name=&quot;command&quot;/&gt;
    &lt;/sersync&gt;
&lt;/head&gt;

EOF

# 创建密码文件
echo &quot;123456&quot; | sudo tee /etc/rsync.password
sudo chmod 600 /etc/rsync.password

# 启动 sersync 服务
if [ -f &quot;/app/sersync2/sersync2&quot; ]; then
    /app/sersync2/sersync2 -r -d -o /app/sersync2/confxml.xml
else
    echo &quot;sersync2 程序文件不存在，请检查软件包解压路径。&quot;
    exit 1
fi

# 设置 sersync 开机自启动
sudo tee -a /etc/rc.local &lt;&lt;EOF
#!/bin/bash
/app/sersync2/sersync2 -r -d -o /app/sersync2/confxml.xml
EOF
sudo chmod +x /etc/rc.local

</code></pre>
<h3 id="服务器-b-19216823412">服务器 B (192.168.234.12)</h3>
<h4 id="角色-2">角色</h4>
<ul>
<li><strong>目标服务器</strong>：接收来自服务器 A 的文件同步，并存储在 <code>/data/backup</code> 目录中。</li>
</ul>
<h4 id="流程-2">流程</h4>
<ol>
<li>
<p><strong>安装必要的软件</strong></p>
<ul>
<li><code>nfs-utils</code>：用于挂载 NFS 共享。</li>
<li><code>rsync</code>：用于接收文件同步。</li>
</ul>
</li>
<li>
<p><strong>挂载 NFS 共享</strong></p>
<ul>
<li>创建挂载点 <code>/data/backup</code>。</li>
<li>编辑 <code>/etc/fstab</code> 文件，自动挂载 NFS 共享。</li>
<li>挂载 NFS 共享。</li>
</ul>
</li>
<li>
<p><strong>配置 <code>rsync</code></strong></p>
<ul>
<li>创建 <code>rsync</code> 密码文件 <code>/etc/rsync.password</code>。
<ul>
<li>创建密码文件并设置权限。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>测试同步</strong></p>
<ul>
<li>手动运行 <code>rsync</code> 命令测试同步。</li>
<li>确认同步成功后，可以通过 <code>sersync</code> 自动同步。</li>
</ul>
</li>
</ol>
<h4 id="安装的软件-2">安装的软件</h4>
<ul>
<li><code>nfs-utils</code></li>
<li><code>rsync</code></li>
</ul>
<h4 id="脚本b并执行">脚本B并执行</h4>
<pre><code class="language-bash">#!/bin/bash

# 设置系统参数
echo 50000000 &gt; /proc/sys/fs/inotify/max_user_watches
echo 327679 &gt; /proc/sys/fs/inotify/max_queued_events

# 永久设置系统参数
echo 'fs.inotify.max_user_watches=50000000' | sudo tee -a /etc/sysctl.conf
echo 'fs.inotify.max_queued_events=327679' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# 安装和配置 NFS 客户端
sudo yum install -y nfs-utils

# 创建挂载点
sudo mkdir -p /mnt/nfs_share

# 挂载 NFS 共享目录
sudo mount 192.168.234.11:/data/nfs_share /mnt/nfs_share

# 验证挂载是否成功
df -h

# 安装 rsync
sudo yum install -y rsync

# 编辑 rsync 配置文件
sudo tee /etc/rsyncd.conf &lt;&lt;EOF
uid = root
gid = root
use chroot = no
max connections = 4
timeout = 600
pid file = /var/run/rsyncd.pid
lock file = /var/run/rsync.lock
log file = /var/log/rsync.log

[backup]
path = /data/backup
comment = backup data
read only = no
auth users = rsync_user
secrets file = /etc/rsyncd.secrets
hosts allow = 192.168.234.11
EOF

# 创建密码文件
echo &quot;rsync:123456&quot; | sudo tee /etc/rsyncd.secrets
sudo chmod 600 /etc/rsyncd.secrets

# 创建备份目录
sudo mkdir -p /data/backup
sudo chown -R nobody:nobody /data/backup
sudo chmod -R 777 /data/backup

# 创建 rsync 的 systemd 服务文件
sudo tee /etc/systemd/system/rsync.service &lt;&lt;EOF
[Unit]
Description=rsync daemon
After=network.target

[Service]
Type=forking
ExecStart=/usr/bin/rsync --daemon --config=/etc/rsyncd.conf
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# 重新加载 systemd 配置
sudo systemctl daemon-reload

# 启动并启用 rsync 服务
sudo systemctl start rsync
sudo systemctl enable rsync

# 设置 rsync 开机自启动
sudo tee -a /etc/rc.local &lt;&lt;EOF
#!/bin/bash
systemctl start rsync
EOF
sudo chmod +x /etc/rc.local

</code></pre>
<h4 id="在服务器-b-上验证同步结果">在服务器 B 上验证同步结果</h4>
<pre><code class="language-bash"># 检查 `/data/backup` 目录中的文件是否与服务器 A 的 `/data/nfs_share` 目录一致
ls -l /data/backup
</code></pre>
<h2 id="总结">总结</h2>
<ol>
<li><strong>安装 rsync 和 inotify-tools</strong>：用 <code>yum</code> 安装。</li>
<li><strong>编写脚本</strong>：用 <code>inotifywait</code> 监控文件变化，并用 <code>rsync</code> 实时同步。</li>
<li><strong>后台运行</strong>：使用 <code>nohup</code> 或 <code>systemd</code> 将脚本作为后台服务运行。</li>
<li><strong>日志管理</strong>：通过 <code>logrotate</code> 管理日志文件，避免日志过大。</li>
<li><strong>远端同步</strong>：配置服务器 A 和服务器 B，实现文件的实时同步。</li>
<li><strong>测试同步</strong>：手动触发同步并验证结果，确保配置正确。</li>
</ol>
<p>这样，你就可以实现基于文件变化的实时同步了。每当 <code>zhangsan</code>、<code>lisi</code> 或 <code>wangwu</code> 目录有文件更改时，会自动同步到备份目录。同时，服务器 A 和服务器 B 之间的文件也可以实现远端同步。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[自动化运维 Ansible]]></title>
        <id>https://orochw.github.io/post/zi-dong-hua-yun-wei-ansible/</id>
        <link href="https://orochw.github.io/post/zi-dong-hua-yun-wei-ansible/">
        </link>
        <updated>2023-03-01T17:14:28.000Z</updated>
        <content type="html"><![CDATA[<h1 id="运维自动化之ansible">运维自动化之ANSIBLE</h1>
<h3 id="本章内容">本章内容</h3>
<ul>
<li>运维自动化发展历程及技术应用</li>
<li>Ansible命令使用</li>
<li>Ansible常用模块详解</li>
<li>YAML语法简介</li>
<li>Ansible playbook基础</li>
<li>Playbook变量、tags、handlers使用</li>
<li>Playbook模板templates</li>
<li>Playbook条件判断 when</li>
<li>Playbook字典 with_items</li>
<li>Ansible Roles</li>
</ul>
<h3 id="运维自动化发展历程及技术应用">运维自动化发展历程及技术应用</h3>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011711.png" alt="" loading="lazy"></figure>
<h3 id="企业实际应用场景分析">企业实际应用场景分析</h3>
<pre><code>Dev开发环境
    使用者：程序员
    功能：程序员开发软件，测试BUG的环境
    管理者：程序员

测试环境    
    使用者：QA测试工程师
    功能：测试经过Dev环境测试通过的软件的功能
    管理者：运维

说明：测试环境往往有多套,测试环境满足测试功能即可，不宜过多
1、测试人员希望测试环境有多套,公司的产品多产品线并发，即多个版本，意味着多个版本同步测试
2、通常测试环境有多少套和产品线数量保持一样

发布环境：代码发布机，有些公司为堡垒机（安全屏障）
    使用者：运维
    功能：发布代码至生产环境
    管理者：运维（有经验）
    发布机：往往需要有2台（主备）

生产环境
    使用者：运维，少数情况开放权限给核心开发人员，极少数公司将权限完全
    开放给开发人员并其维护
    功能：对用户提供公司产品的服务

管理者：只能是运维
    生产环境服务器数量：一般比较多，且应用非常重要。往往需要自动工具协助部署配置应用

灰度环境（生产环境的一部分）
    使用者：运维
    功能：在全量发布代码前将代码的功能面向少量精准用户发布的环境,可基
    于主机或用户执行灰度发布
    案例：共100台生产服务器，先发布其中的10台服务器，这10台服务器就是灰度服务器
    管理者：运维
    灰度环境：往往该版本功能变更较大，为保险起见特意先让一部分用户优化体验该功能，
              待这部分用户使用没有重大问题的时候，再全量发布至所有服务器
</code></pre>
<h3 id="程序发布">程序发布</h3>
<pre><code>程序发布要求：
    不能导致系统故障或造成系统完全不可用
    不能影响用户体验
预发布验证：
    新版本的代码先发布到服务器（跟线上环境配置完全相同，只是未接入到调度器）
灰度发布：
    基于主机，用户，业务
发布路径：
    /webapp/tuangou
    /webapp/tuangou-1.1
    /webapp/tuangou-1.2
发布过程：在调度器上下线一批主机(标记为maintanance状态) --&gt; 关闭服务 --&gt;
          部署新版本的应用程序 --&gt; 启动服务 --&gt; 在调度器上启用这一批服务器
自动化灰度发布：脚本、发布平台
</code></pre>
<h3 id="运维自动化发展历程及技术应用-2">运维自动化发展历程及技术应用</h3>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011813.png" alt="" loading="lazy"></figure>
<h3 id="自动化运维应用场景">自动化运维应用场景</h3>
<pre><code>文件传输
应用部署
配置管理
任务流编排
</code></pre>
<h3 id="常用自动化运维工具">常用自动化运维工具</h3>
<pre><code>Ansible：python，Agentless，中小型应用环境
Saltstack：python，一般需部署agent，执行效率更高
Puppet：ruby, 功能强大，配置复杂，重型,适合大型环境
Fabric：python，agentless
Chef：ruby，国内应用少
Cfengine
func
</code></pre>
<h3 id="企业级自动化运维工具应用实战ansible">企业级自动化运维工具应用实战ansible</h3>
<pre><code>公司计划在年底做一次大型市场促销活动，全面冲刺下交易额，为明年的上市做准备。
公司要求各业务组对年底大促做准备，运维部要求所有业务容量进行三倍的扩容，
并搭建出多套环境可以共开发和测试人员做测试，运维老大为了在年底有所表现，
要求运维部门同学尽快实现，当你接到这个任务时，有没有更快的解决方案？
</code></pre>
<h3 id="ansible发展史">Ansible发展史</h3>
<pre><code>Ansible
Michael DeHaan（ Cobbler 与 Func 作者）
名称来自《安德的游戏》中跨越时空的即时通信工具
2012-03-09，发布0.0.1版，2015-10-17，Red Hat宣布收购
官网：https://www.ansible.com/
官方文档：https://docs.ansible.com/
同类自动化工具GitHub关注程度（2016-07-10）
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011838.png" alt="" loading="lazy"></figure>
<h3 id="特性">特性</h3>
<pre><code>1&gt; 模块化：调用特定的模块，完成特定任务
2&gt; Paramiko（python对ssh的实现），PyYAML，Jinja2（模板语言）三个关键模块
3&gt; 支持自定义模块
4&gt; 基于Python语言实现
5&gt; 部署简单，基于python和SSH(默认已安装)，agentless
6&gt; 安全，基于OpenSSH
7&gt; 支持playbook编排任务
8&gt; 幂等性：一个任务执行1遍和执行n遍效果一样，不因重复执行带来意外情况
9&gt; 无需代理不依赖PKI（无需ssl）
10&gt; 可使用任何编程语言写模块
11&gt; YAML格式，编排任务，支持丰富的数据结构
12&gt; 较强大的多层解决方案
</code></pre>
<h3 id="ansible架构">Ansible架构</h3>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011849.png" alt="" loading="lazy"></figure>
<pre><code>ansible的作用以及工作结构
1、ansible简介：
ansible是新出现的自动化运维工具，基于Python开发，
集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，
实现了批量系统配置、批量程序部署、批量运行命令等功能。
ansible是基于模块工作的，本身没有批量部署的能力。
真正具有批量部署的是ansible所运行的模块，ansible只是提供一种框架。
主要包括：
    (1)、连接插件connection plugins：负责和被监控端实现通信；
    (2)、host inventory：指定操作的主机，是一个配置文件里面定义监控的主机；
    (3)、各种模块核心模块、command模块、自定义模块；
    (4)、借助于插件完成记录日志邮件等功能；
    (5)、playbook：剧本执行多个任务时，非必需可以让节点一次性运行多个任务。

2、ansible的架构：连接其他主机默认使用ssh协议	
</code></pre>
<h3 id="ansible工作原理">Ansible工作原理</h3>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011905.png" alt="" loading="lazy"></figure>
<h3 id="ansible主要组成部分">Ansible主要组成部分</h3>
<pre><code>ANSIBLE PLAYBOOKS：任务剧本（任务集），编排定义Ansible任务集的配置文件，
                   由Ansible顺序依次执行，通常是JSON格式的YML文件
INVENTORY：Ansible管理主机的清单  /etc/anaible/hosts
MODULES：  Ansible执行命令的功能模块，多数为内置核心模块，也可自定义
PLUGINS：  模块功能的补充，如连接类型插件、循环插件、变量插件、过滤插件等，该功能不常用
API：      供第三方程序调用的应用程序编程接口 
ANSIBLE：  组合INVENTORY、API、MODULES、PLUGINS的绿框，可以理解为是ansible命令工具，其为核心执行工具
</code></pre>
<pre><code>Ansible命令执行来源：
    1&gt; USER，普通用户，即SYSTEM ADMINISTRATOR
    2&gt; CMDB（配置管理数据库） API 调用
    3&gt; PUBLIC/PRIVATE CLOUD API调用  (公有私有云的API接口调用)
    4&gt; USER-&gt; Ansible Playbook -&gt; Ansibile

利用ansible实现管理的方式：
    1&gt; Ad-Hoc 即ansible单条命令，主要用于临时命令使用场景
    2&gt; Ansible-playbook 主要用于长期规划好的，大型项目的场景，需要有前期的规划过程
</code></pre>
<pre><code>Ansible-playbook（剧本）执行过程
    将已有编排好的任务集写入Ansible-Playbook
    通过ansible-playbook命令分拆任务集至逐条ansible命令，按预定规则逐条执行

Ansible主要操作对象
   HOSTS主机
   NETWORKING网络设备

注意事项:
   执行ansible的主机一般称为主控端，中控，master或堡垒机
   主控端Python版本需要2.6或以上
   被控端Python版本小于2.4需要安装python-simplejson
   被控端如开启SELinux需要安装libselinux-python
   windows不能做为主控端
   ansible不是服务,不会一直启动,只是需要的时候启动
</code></pre>
<h3 id="安装">安装</h3>
<pre><code>rpm包安装: EPEL源
    yum install ansible

编译安装:
    yum -y install python-jinja2 PyYAML python-paramiko python-babel
    python-crypto
    tar xf ansible-1.5.4.tar.gz
    cd ansible-1.5.4
    python setup.py build
    python setup.py install
    mkdir /etc/ansible
    cp -r examples/* /etc/ansible


Git方式:
    git clone git://github.com/ansible/ansible.git --recursive
    cd ./ansible
    source ./hacking/env-setup

pip安装： pip是安装Python包的管理器，类似yum
    yum install python-pip python-devel
    yum install gcc glibc-devel zibl-devel rpm-bulid openssl-devel
    pip install --upgrade pip
    pip install ansible --upgrade

确认安装：
    ansible --version
</code></pre>
<h3 id="相关文件">相关文件</h3>
<pre><code>配置文件
    /etc/ansible/ansible.cfg  主配置文件,配置ansible工作特性(一般无需修改)
    /etc/ansible/hosts        主机清单(将被管理的主机放到此文件)
    /etc/ansible/roles/       存放角色的目录

程序
    /usr/bin/ansible          主程序，临时命令执行工具
    /usr/bin/ansible-doc      查看配置文档，模块功能查看工具
    /usr/bin/ansible-galaxy   下载/上传优秀代码或Roles模块的官网平台
    /usr/bin/ansible-playbook 定制自动化任务，编排剧本工具
    /usr/bin/ansible-pull     远程执行命令的工具
    /usr/bin/ansible-vault    文件加密工具
    /usr/bin/ansible-console  基于Console界面与用户交互的执行工具
</code></pre>
<h3 id="主机清单inventory">主机清单inventory</h3>
<pre><code>Inventory 主机清单
1&gt; ansible的主要功用在于批量主机操作，为了便捷地使用其中的部分主机，可以在inventory file中将其分组命名 
2&gt; 默认的inventory file为/etc/ansible/hosts
3&gt; inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成

/etc/ansible/hosts文件格式
inventory文件遵循INI文件风格，中括号中的字符为组名。
可以将同一个主机同时归并到多个不同的组中；
此外，当如若目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号来标明
    ntp.ansible.com   不分组,直接加
    
    [webservers]     webservers组
    www1.ansible.com:2222  可以指定端口
    www2.ansible.com
    
    [dbservers]
    db1.ansible.com
    db2.ansible.com
    db3.ansible.com

如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机
示例：
    [websrvs]
    www[1:100].example.com   ip: 1-100
    
    [dbsrvs]
    db-[a:f].example.com     dba-dbff
</code></pre>
<h3 id="ansible-配置文件">ansible 配置文件</h3>
<pre><code>Ansible 配置文件/etc/ansible/ansible.cfg （一般保持默认）

vim /etc/ansible/ansible.cfg

[defaults]
#inventory     = /etc/ansible/hosts      # 主机列表配置文件
#library       = /usr/share/my_modules/  # 库文件存放目录
#remote_tmp    = $HOME/.ansible/tmp      # 临时py命令文件存放在远程主机目录
#local_tmp     = $HOME/.ansible/tmp      # 本机的临时命令执行目录  
#forks         = 5                       # 默认并发数,同时可以执行5次
#sudo_user     = root                    # 默认sudo 用户
#ask_sudo_pass = True                    # 每次执行ansible命令是否询问ssh密码
#ask_pass      = True                    # 每次执行ansible命令是否询问ssh口令
#remote_port   = 22                      # 远程主机的端口号(默认22)

建议优化项： 
host_key_checking = False               # 检查对应服务器的host_key，建议取消注释
log_path=/var/log/ansible.log           # 日志文件,建议取消注释
module_name   = command                 # 默认模块
</code></pre>
<h3 id="ansible系列命令">ansible系列命令</h3>
<pre><code>Ansible系列命令
    ansible ansible-doc ansible-playbook ansible-vault ansible-console
    ansible-galaxy ansible-pull

ansible-doc: 显示模块帮助
    ansible-doc [options] [module...]
        -a            显示所有模块的文档
        -l, --list    列出可用模块
        -s, --snippet 显示指定模块的playbook片段(简化版,便于查找语法)

示例：
    ansible-doc -l      列出所有模块
    ansible-doc ping    查看指定模块帮助用法
    ansible-doc -s ping 查看指定模块帮助用法
</code></pre>
<h3 id="ansible">ansible</h3>
<pre><code>ansible通过ssh实现配置管理、应用部署、任务执行等功能，
建议配置ansible端能基于密钥认证的方式联系各被管理节点

ansible &lt;host-pattern&gt; [-m module_name] [-a args]
ansible +被管理的主机(ALL) +模块  +参数
    --version              显示版本
    -m module              指定模块，默认为command
    -v                     详细过程 –vv -vvv更详细
    --list-hosts           显示主机列表，可简写 --list
    -k, --ask-pass         提示输入ssh连接密码,默认Key验证
    -C, --check            检查，并不执行
    -T, --timeout=TIMEOUT  执行命令的超时时间,默认10s
    -u, --user=REMOTE_USER 执行远程执行的用户
    -b, --become           代替旧版的sudo切换
        --become-user=USERNAME 指定sudo的runas用户,默认为root
    -K, --ask-become-pass  提示输入sudo时的口令
</code></pre>
<pre><code>ansible all --list  列出所有主机
ping模块: 探测网络中被管理主机是否能够正常使用  走ssh协议
          如果对方主机网络正常,返回pong
ansible-doc -s ping   查看ping模块的语法 

检测所有主机的网络状态
1&gt;  默认情况下连接被管理的主机是ssh基于key验证,如果没有配置key,权限将会被拒绝
    因此需要指定以谁的身份连接,输入用户密码,必须保证被管理主机用户密码一致
    ansible all -m ping -k

2&gt; 或者实现基于key验证 将公钥ssh-copy-id到被管理的主机上 , 实现免密登录
   ansible all -m ping
</code></pre>
<h3 id="ansible的host-pattern">ansible的Host-pattern</h3>
<pre><code>ansible的Host-pattern
匹配主机的列表
    All ：表示所有Inventory中的所有主机
        ansible all –m ping
    * :通配符
        ansible &quot;*&quot; -m ping  (*表示所有主机)
        ansible 192.168.1.* -m ping
        ansible &quot;*srvs&quot; -m ping
    或关系 &quot;:&quot;
        ansible &quot;websrvs:appsrvs&quot; -m ping
        ansible “192.168.1.10:192.168.1.20” -m ping
    逻辑与 &quot;:&amp;&quot;
        ansible &quot;websrvs:&amp;dbsrvs&quot; –m ping
        在websrvs组并且在dbsrvs组中的主机
    逻辑非 &quot;:!&quot;
        ansible 'websrvs:!dbsrvs' –m ping
        在websrvs组，但不在dbsrvs组中的主机
        注意：此处为单引号
    综合逻辑
        ansible 'websrvs:dbsrvs:&amp;appsrvs:!ftpsrvs' –m ping
    正则表达式
        ansible &quot;websrvs:&amp;dbsrvs&quot; –m ping
        ansible &quot;~(web|db).*\.ansible\.com&quot; –m ping
</code></pre>
<h3 id="ansible命令执行过程">ansible命令执行过程</h3>
<pre><code>ansible命令执行过程
    1. 加载自己的配置文件 默认/etc/ansible/ansible.cfg
    2. 加载自己对应的模块文件，如command
    3. 通过ansible将模块或命令生成对应的临时py文件，
       并将该文件传输至远程服务器的对应执行用户$HOME/.ansible/tmp/ansible-tmp-数字/XXX.PY文件
    4. 给文件+x执行
    5. 执行并返回结果
    6. 删除临时py文件，sleep 0退出

执行状态：
    绿色：执行成功并且不需要做改变的操作
    黄色：执行成功并且对目标主机做变更
    红色：执行失败
</code></pre>
<h3 id="ansible使用示例">ansible使用示例</h3>
<pre><code>示例
    以wang用户执行ping存活检测
        ansible all -m ping -u wang -k
    以wang sudo至root执行ping存活检测
        ansible all -m ping -u wang -k -b
    以wang sudo至ansible用户执行ping存活检测
        ansible all -m ping -u wang -k -b --become-user=ansible
    以wang sudo至root用户执行ls
        ansible all -m command -u wang -a 'ls /root' -b --become-user=root -k -K

ansible ping模块测试连接
    ansible 192.168.38.126,192.168.38.127 -m ping -k 
</code></pre>
<h3 id="ansible常用模块">ansible常用模块</h3>
<pre><code>模块文档：https://docs.ansible.com/ansible/latest/modules/modules_by_category.html

Command：在远程主机执行命令，默认模块，可忽略-m选项
    &gt; ansible srvs -m command -a 'service vsftpd start'
    &gt; ansible srvs -m command -a 'echo adong |passwd --stdin 123456'
此命令不支持 $VARNAME &lt; &gt; | ; &amp; 等,用shell模块实现

    chdir:   进入到被管理主机目录
    creates: 如果有一个目录是存在的,步骤将不会运行Command命令
    ansible websrvs -a 'chdir=/data/ ls'

Shell：和command相似，用shell执行命令
    &gt; ansible all -m shell  -a 'getenforce'  查看SELINUX状态
    &gt;  ansible all -m shell  -a &quot;sed -i 's/SELINUX=.*/SELINUX=disabled' /etc/selinux/config&quot;
    &gt; ansible srv -m shell -a 'echo ansible |passwd –stdin wang'
      
    调用bash执行命令 类似 cat /tmp/stanley.md | awk -F'|' '{print $1,$2}' &amp;&gt; /tmp/example.txt     
    这些复杂命令，即使使用shell也可能会失败，
    解决办法：写到脚本时，copy到远程执行，再把需要的结果拉回执行命令的机器

    修改配置文件,使shell作为默认模块    
        vim /etc/ansible/ansible.cfg
        module_name = shell

Script：在远程主机上运行ansible服务器上的脚本
    &gt; -a &quot;/PATH/TO/SCRIPT_FILE&quot;
    &gt; ansible websrvs -m script -a /data/test.sh

Copy：从主控端复制文件到远程主机
      src : 源文件  指定拷贝文件的本地路径  (如果有/ 则拷贝目录内容,比拷贝目录本身)
      dest: 指定目标路径
      mode: 设置权限
      backup: 备份源文件
      content: 代替src  指定本机文件内容,生成目标主机文件
      
      &gt; ansible websrvs -m copy -a &quot;src=/root/test1.sh dest=/tmp/test2.showner=wang mode=600 backup=yes&quot;
        如果目标存在，默认覆盖，此处指定先备份
      &gt; ansible websrvs -m copy -a &quot;content='test content\nxxx' dest=/tmp/test.txt&quot;
        指定内容，直接生成目标文件

Fetch：从远程主机提取文件至主控端，copy相反，目前不支持目录,可以先打包,再提取文件
     &gt; ansible websrvs -m fetch -a 'src=/root/test.sh dest=/data/scripts'
     会生成每个被管理主机不同编号的目录,不会发生文件名冲突
     
     &gt; ansible all -m shell -a 'tar jxvf test.tar.gz /root/test.sh'
     &gt; ansible all -m fetch -a 'src=/root/test.tar.gz dest=/data/'

File：设置文件属性
    path: 要管理的文件路径 (强制添加)
    recurse: 递归,文件夹要用递归
    src:  创建硬链接,软链接时,指定源目标,配合'state=link' 'state=hard' 设置软链接,硬链接
    state: 状态
          absent 缺席,删除
          
    &gt; ansible websrvs -m file -a 'path=/app/test.txt state=touch'       创建文件
    &gt; ansible websrvs -m file -a &quot;path=/data/testdir state=directory&quot;   创建目录    
    &gt; ansible websrvs -m file -a &quot;path=/root/test.sh owner=wang mode=755&quot;  设置权限755
    &gt; ansible websrvs -m file -a 'src=/data/testfile dest=/data/testfile-link state=link' 创建软链接
    
    
unarchive：解包解压缩，有两种用法：
    1、将ansible主机上的压缩包传到远程主机后解压缩至特定目录，设置copy=yes.
    2、将远程主机上的某个压缩包解压缩到指定路径下，设置copy=no

    常见参数：
        copy：默认为yes，当copy=yes，拷贝的文件是从ansible主机复制到远程主机上，
              如果设置为copy=no，会在远程主机上寻找src源文件
        src： 源路径，可以是ansible主机上的路径，也可以是远程主机上的路径，
              如果是远程主机上的路径，则需要设置copy=no
        dest：远程主机上的目标路径
        mode：设置解压缩后的文件权限
    
    示例：
        ansible websrvs -m unarchive -a 'src=foo.tgz dest=/var/lib/foo'  
          #默认copy为yes ,将本机目录文件解压到目标主机对应目录下
        ansible websrvs -m unarchive -a 'src=/tmp/foo.zip dest=/data copy=no mode=0777'
          # 解压被管理主机的foo.zip到data目录下, 并设置权限777
        ansible websrvs -m unarchive -a 'src=https://example.com/example.zip dest=/data copy=no'

Archive：打包压缩
    &gt; ansible all -m archive -a 'path=/etc/sysconfig dest=/data/sysconfig.tar.bz2 format=bz2 owner=wang mode=0777'
    将远程主机目录打包 
        path:   指定路径
        dest:   指定目标文件
        format: 指定打包格式
        owner:  指定所属者
        mode:   设置权限

Hostname：管理主机名
    ansible appsrvs -m hostname -a &quot;name=app.adong.com&quot;  更改一组的主机名
    ansible 192.168.38.103 -m hostname -a &quot;name=app2.adong.com&quot; 更改单个主机名

Cron：计划任务
    支持时间：minute,hour,day,month,weekday
    &gt; ansible websrvs -m cron -a &quot;minute=*/5 job='/usr/sbin/ntpdate 172.16.0.1 &amp;&gt;/dev/null' name=Synctime&quot; 
    创建任务
    &gt; ansible websrvs -m cron -a 'state=absent name=Synctime' 
    删除任务
    &gt; ansible websrvs -m cron -a 'minute=*/10 job='/usr/sbin/ntpdate 172.30.0.100&quot; name=synctime disabled=yes'
    注释任务,不在生效

Yum：管理包
    ansible websrvs -m yum -a 'list=httpd'  查看程序列表
    
    ansible websrvs -m yum -a 'name=httpd state=present' 安装
    ansible websrvs -m yum -a 'name=httpd state=absent'  删除
    可以同时安装多个程序包
    
Service：管理服务
    ansible srv -m service -a 'name=httpd state=stopped'  停止服务
    ansible srv -m service -a 'name=httpd state=started enabled=yes' 启动服务,并设为开机自启
    ansible srv -m service -a 'name=httpd state=reloaded'  重新加载
    ansible srv -m service -a 'name=httpd state=restarted' 重启服务

User：管理用户
    home   指定家目录路径
    system 指定系统账号
    group  指定组
    remove 清除账户
    shell  指定shell类型
    
    ansible websrvs -m user -a 'name=user1 comment=&quot;test user&quot; uid=2048 home=/app/user1 group=root'
    ansible websrvs -m user -a 'name=sysuser1 system=yes home=/app/sysuser1'
    ansible websrvs -m user -a 'name=user1 state=absent remove=yes'  清空用户所有数据
    ansible websrvs -m user -a 'name=app uid=88 system=yes home=/app groups=root shell=/sbin/nologin password=&quot;$1$zfVojmPy$ZILcvxnXljvTI2PhP2Iqv1&quot;'  创建用户
    ansible websrvs -m user -a 'name=app state=absent'  不会删除家目录
    
    安装mkpasswd 
    yum insatll expect 
    mkpasswd 生成口令
    openssl passwd -1  生成加密口令
    

删除用户及家目录等数据
    Group：管理组
        ansible srv -m group -a &quot;name=testgroup system=yes&quot;   创建组
        ansible srv -m group -a &quot;name=testgroup state=absent&quot; 删除组
</code></pre>
<h3 id="ansible系列命令-2">ansible系列命令</h3>
<pre><code>可以通过网上写好的
ansible-galaxy
    &gt; 连接 https://galaxy.ansible.com 
      下载相应的roles(角色)
    
    &gt; 列出所有已安装的galaxy
        ansible-galaxy list
    
    &gt; 安装galaxy
        ansible-galaxy install geerlingguy.redis
    
    &gt; 删除galaxy
        ansible-galaxy remove geerlingguy.redis
        
ansible-pull
    推送命令至远程，效率无限提升，对运维要求较高
    

ansible-playbook  可以引用按照标准的yml语言写的脚本
    执行playbook
    示例：ansible-playbook hello.yml
        cat hello.yml
        #hello world yml file
        - hosts: websrvs
          remote_user: root
          tasks:
            - name: hello world
              command: /usr/bin/wall hello world

ansible-vault  (了解)
功能：管理加密解密yml文件
    ansible-vault [create|decrypt|edit|encrypt|rekey|view]
        ansible-vault encrypt hello.yml 加密
        ansible-vault decrypt hello.yml 解密
        ansible-vault view hello.yml    查看
        ansible-vault edit hello.yml    编辑加密文件
        ansible-vault rekey hello.yml   修改口令
        ansible-vault create new.yml    创建新文件


Ansible-console：2.0+新增，可交互执行命令，支持tab  (了解)

    root@test (2)[f:10] $
    执行用户@当前操作的主机组 (当前组的主机数量)[f:并发数]$

    设置并发数：         forks n   例如： forks 10
    切换组：             cd 主机组 例如： cd web
    列出当前组主机列表： list
    列出所有的内置命令： ?或help
    示例：
        root@all (2)[f:5]$ list
        root@all (2)[f:5]$ cd appsrvs
        root@appsrvs (2)[f:5]$ list
        root@appsrvs (2)[f:5]$ yum name=httpd state=present
        root@appsrvs (2)[f:5]$ service name=httpd state=started
</code></pre>
<h3 id="playbook">playbook</h3>
<pre><code>&gt; playbook是由一个或多个&quot;play&quot;组成的列表
&gt; play的主要功能在于将预定义的一组主机，装扮成事先通过ansible中的task定义好的角色。
  Task实际是调用ansible的一个module，将多个play组织在一个playbook中，
  即可以让它们联合起来，按事先编排的机制执行预定义的动作
&gt; Playbook采用YAML语言编写
</code></pre>
<h3 id="playbook图解">playbook图解</h3>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011931.png" alt="" loading="lazy"></figure>
<pre><code>用户通过ansible命令直接调用yml语言写好的playbook,playbook由多条play组成
每条play都有一个任务(task)相对应的操作,然后调用模块modules，应用在主机清单上,通过ssh远程连接
从而控制远程主机或者网络设备
</code></pre>
<h3 id="yaml介绍">YAML介绍</h3>
<pre><code>YAML是一个可读性高的用来表达资料序列的格式。
    YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822等。
    Clark Evans在2001年在首次发表了这种语言，另外Ingy döt Net与Oren Ben-Kiki也是这语言的共同设计者

YAML Ain't Markup Language，即YAML不是XML。
不过，在开发的这种语言时，YAML的意思其实是：&quot;Yet Another Markup Language&quot;（仍是一种标记语言）

特性
    YAML的可读性好
    YAML和脚本语言的交互性好
    YAML使用实现语言的数据类型
    YAML有一个一致的信息模型
    YAML易于实现
    YAML可以基于流来处理
    YAML表达能力强，扩展性好

更多的内容及规范参见：http://www.yaml.org
</code></pre>
<h3 id="yaml语法简介">YAML语法简介</h3>
<pre><code>&gt; 在单一档案中，可用连续三个连字号(——)区分多个档案。
  另外，还有选择性的连续三个点号( ... )用来表示档案结尾
&gt; 次行开始正常写Playbook的内容，一般建议写明该Playbook的功能
&gt; 使用#号注释代码
&gt; 缩进必须是统一的，不能空格和tab混用
&gt; 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行来实现的
&gt; YAML文件内容是区别大小写的，k/v的值均需大小写敏感
&gt; 多个k/v可同行写也可换行写，同行使用:分隔
&gt; v可是个字符串，也可是另一个列表[]
&gt; 一个完整的代码块功能需最少元素需包括 name 和 task
&gt; 一个name只能包括一个task
&gt; YAML文件扩展名通常为yml或yaml
</code></pre>
<h3 id="yaml语法简介-2">YAML语法简介</h3>
<pre><code>List：列表，其所有元素均使用“-”打头
      列表代表同一类型的元素
示例：
# A list of tasty fruits
- Apple
- Orange
- Strawberry
- Mango

Dictionary：字典，通常由多个key与value构成 键值对
示例：
---
# An employee record
name: Example Developer
job: Developer
skill: Elite

也可以将key:value放置于{}中进行表示，用,分隔多个key:value
示例：
---
# An employee record
{name: Example Developer, job: Developer, skill: Elite}  有空格
</code></pre>
<h3 id="yaml语法">YAML语法</h3>
<pre><code>YAML的语法和其他高阶语言类似，并且可以简单表达清单、散列表、标量等数据结构。
其结构（Structure）通过空格来展示，序列（Sequence）里的项用&quot;-&quot;来代表，Map里的键值对用&quot;:&quot;分隔
示例
    name: John Smith
    age: 41
    gender: Male
    spouse:
      name: Jane Smith
      age: 37
      gender: Female
    children:
      - name: Jimmy Smith
        age: 17
        gender: Male
      - name: Jenny Smith
        age 13
        gender: Female
</code></pre>
<h3 id="三种常见的数据交换格式">三种常见的数据交换格式</h3>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302011946.png" alt="" loading="lazy"></figure>
<h3 id="playbook核心元素">Playbook核心元素</h3>
<pre><code>Hosts          执行的远程主机列表(应用在哪些主机上)

Tasks          任务集

Variables      内置变量或自定义变量在playbook中调用

Templates模板  可替换模板文件中的变量并实现一些简单逻辑的文件

Handlers和notify结合使用，由特定条件触发的操作，满足条件方才执行，否则不执行

tags标签       指定某条任务执行，用于选择运行playbook中的部分代码。
                ansible具有幂等性，因此会自动跳过没有变化的部分，
                即便如此，有些代码为测试其确实没有发生变化的时间依然会非常地长。
                此时，如果确信其没有变化，就可以通过tags跳过此些代码片断
                ansible-playbook -t tagsname useradd.yml
</code></pre>
<h3 id="playbook基础组件">playbook基础组件</h3>
<pre><code>Hosts：
    &gt; playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务。
      hosts用于指定要执行指定任务的主机，须事先定义在主机清单中

    &gt; 可以是如下形式：
        one.example.com
        one.example.com:two.example.com
        192.168.1.50
        192.168.1.*
    &gt; Websrvs:dbsrvs       或者，两个组的并集
    &gt; Websrvs:&amp;dbsrvs      与，两个组的交集
    &gt; webservers:!phoenix  在websrvs组，但不在dbsrvs组
    示例: - hosts: websrvs：dbsrvs

remote_user: 
    可用于Host和task中。
    也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；
    此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户
    - hosts: websrvs
        remote_user: root   (可省略,默认为root)  以root身份连接
      tasks:    指定任务
    - name: test connection
        ping:
        remote_user: ansible
        sudo: yes           默认sudo为root
        sudo_user:wang      sudo为wang
    
task列表和action
    任务列表task:由多个动作,多个任务组合起来的,每个任务都调用的模块,一个模块一个模块执行
    1&gt; play的主体部分是task list，task list中的各任务按次序逐个在hosts中指定的所有主机上执行，
       即在所有主机上完成第一个任务后，再开始第二个任务

    2&gt; task的目的是使用指定的参数执行模块，而在模块参数中可以使用变量。
       模块执行是幂等的，这意味着多次执行是安全的，因为其结果均一致

    3&gt; 每个task都应该有其name，用于playbook的执行结果输出，建议其内容能清晰地描述任务执行步骤。
       如果未提供name，则action的结果将用于输出
</code></pre>
<pre><code>tasks：任务列表
两种格式：
    (1) action: module arguments
    (2) module: arguments 建议使用  模块: 参数
    注意：shell和command模块后面跟命令，而非key=value

某任务的状态在运行后为changed时，可通过&quot;notify&quot;通知给相应的handlers

任务可以通过&quot;tags&quot;打标签，可在ansible-playbook命令上使用-t指定进行调用
示例：
tasks:
  - name: disable selinux   描述
    command: /sbin/setenforce 0   模块名: 模块对应的参数

</code></pre>
<pre><code>如果命令或脚本的退出码不为零，可以使用如下方式替代
tasks:
  - name: run this command and ignore the result
    shell: /usr/bin/somecommand || /bin/true  
    转错为正  如果命令失败则执行 true

或者使用ignore_errors来忽略错误信息
tasks:
  - name: run this command and ignore the result
    shell: /usr/bin/somecommand
    ignore_errors: True  忽略错误
</code></pre>
<h3 id="运行playbook">运行playbook</h3>
<pre><code>运行playbook的方式
    ansible-playbook &lt;filename.yml&gt; ... [options]

常见选项
    --check -C       只检测可能会发生的改变，但不真正执行操作 
                     (只检查语法,如果执行过程中出现问题,-C无法检测出来)
                     (执行playbook生成的文件不存在,后面的程序如果依赖这些文件,也会导致检测失败)
    --list-hosts     列出运行任务的主机
    --list-tags      列出tag  (列出标签)
    --list-tasks     列出task (列出任务)
    --limit 主机列表 只针对主机列表中的主机执行
    -v -vv -vvv      显示过程

示例
    ansible-playbook hello.yml --check 只检测
    ansible-playbook hello.yml --list-hosts  显示运行任务的主机
    ansible-playbook hello.yml --limit websrvs  限制主机
</code></pre>
<h3 id="playbook-vs-shellscripts">Playbook VS ShellScripts</h3>
<p>安装httpd</p>
<pre><code>SHELL脚本
#!/bin/bash
# 安装Apache
yum install --quiet -y httpd
# 复制配置文件
cp /tmp/httpd.conf /etc/httpd/conf/httpd.conf
cp/tmp/vhosts.conf /etc/httpd/conf.d/
# 启动Apache，并设置开机启动
service httpd start
chkconfig httpd on
</code></pre>
<pre><code>Playbook定义
---
- hosts: all
  remote_user: root
  
  tasks:
    - name: &quot;安装Apache&quot;
      yum: name=httpd       yum模块:安装httpd
    - name: &quot;复制配置文件&quot;
      copy: src=/tmp/httpd.conf dest=/etc/httpd/conf/  copy模块: 拷贝文件
    - name: &quot;复制配置文件&quot;
      copy: src=/tmp/vhosts.conf dest=/etc/httpd/conf.d/  
    - name: &quot;启动Apache，并设置开机启动&quot;
      service: name=httpd state=started enabled=yes   service模块: 启动服务 
</code></pre>
<h3 id="示例playbook-创建用户">示例:Playbook 创建用户</h3>
<pre><code>示例：sysuser.yml
---
- hosts: all
  remote_user: root

  tasks:
    - name: create mysql user
      user: name=mysql system=yes uid=36
    - name: create a group
      group: name=httpd system=yes
</code></pre>
<h3 id="playbook示例-安装httpd服务">Playbook示例  安装httpd服务</h3>
<pre><code>示例：httpd.yml
- hosts: websrvs
  remote_user: root

  tasks:
    - name: Install httpd
      yum: name=httpd state=present
    - name: Install configure file
      copy: src=files/httpd.conf dest=/etc/httpd/conf/
    - name: start service
      service: name=httpd state=started enabled=yes
</code></pre>
<h3 id="playbook示例-安装nginx服务">Playbook示例  安装nginx服务</h3>
<pre><code>示例 nginx.yml
- hosts: all
  remote_user: root

  tasks:
    - name: add group nginx
      user: name=nginx state=present
    - name: add user nginx
      user: name=nginx state=present group=nginx
    - name: Install Nginx
      yum: name=nginx state=present
    - name: Start Nginx
      service: name=nginx state=started enabled=yes
</code></pre>
<h3 id="handlers和notify结合使用触发条件">handlers和notify结合使用触发条件</h3>
<pre><code>Handlers 实际上就是一个触发器
是task列表，这些task与前述的task并没有本质上的不同,用于当关注的资源发生变化时，才会采取一定的操作

Notify此action可用于在每个play的最后被触发，
这样可避免多次有改变发生时每次都执行指定的操作，仅在所有的变化发生完成后一次性地执行指定操作。
在notify中列出的操作称为handler，也即notify中调用handler中定义的操作
</code></pre>
<h3 id="playbook中handlers使用">Playbook中handlers使用</h3>
<pre><code>- hosts: websrvs
  remote_user: root

  tasks:
    - name: Install httpd
      yum: name=httpd state=present
    - name: Install configure file
      copy: src=files/httpd.conf dest=/etc/httpd/conf/
      notify: restart httpd
    - name: ensure apache is running
      service: name=httpd state=started enabled=yes
  
  handlers:
    - name: restart httpd
      service: name=httpd state=restarted
</code></pre>
<h3 id="示例">示例</h3>
<pre><code>- hosts: webnodes
  vars:
    http_port: 80
    max_clients: 256
  remote_user: root
  
  tasks:
    - name: ensure apache is at the latest version
      yum: name=httpd state=latest
    - name: ensure apache is running
      service: name=httpd state=started
    - name: Install configure file
      copy: src=files/httpd.conf dest=/etc/httpd/conf/
      notify: restart httpd
  
  handlers:
      - name: restart httpd 
        service: name=httpd state=restarted
</code></pre>
<h3 id="示例-2">示例</h3>
<pre><code>- hosts: websrvs
  remote_user: root
  
  tasks:
    - name: add group nginx
      tags: user
      user: name=nginx state=present
    - name: add user nginx
      user: name=nginx state=present group=nginx
    - name: Install Nginx
      yum: name=nginx state=present
    - name: config
      copy: src=/root/config.txt dest=/etc/nginx/nginx.conf
      notify:
        - Restart Nginx
        - Check Nginx Process
  
  handlers:
    - name: Restart Nginx
      service: name=nginx state=restarted enabled=yes
    - name: Check Nginx process
      shell: killall -0 nginx &gt; /tmp/nginx.log
</code></pre>
<h3 id="playbook中tags使用">Playbook中tags使用</h3>
<pre><code>tage: 添加标签 
可以指定某一个任务添加一个标签,添加标签以后,想执行某个动作可以做出挑选来执行
多个动作可以使用同一个标签

示例：httpd.yml
- hosts: websrvs
  remote_user: root
  
  tasks:
    - name: Install httpd
      yum: name=httpd state=present
      tage: install 
    - name: Install configure file
      copy: src=files/httpd.conf dest=/etc/httpd/conf/
      tags: conf
    - name: start httpd service
      tags: service
      service: name=httpd state=started enabled=yes

ansible-playbook –t install,conf httpd.yml   指定执行install,conf 两个标签
</code></pre>
<h3 id="示例-3">示例</h3>
<pre><code>//heartbeat.yaml
- hosts: hbhosts
  remote_user: root
  
  tasks:
    - name: ensure heartbeat latest version
      yum: name=heartbeat state=present
    - name: authkeys configure file
      copy: src=/root/hb_conf/authkeys dest=/etc/ha.d/authkeys
    - name: authkeys mode 600
      file: path=/etc/ha.d/authkeys mode=600
      notify:
        - restart heartbeat
    - name: ha.cf configure file
      copy: src=/root/hb_conf/ha.cf dest=/etc/ha.d/ha.cf
      notify:
        - restart heartbeat
  handlers:
    - name: restart heartbeat
      service: name=heartbeat state=restarted
</code></pre>
<h3 id="playbook中tags使用-2">Playbook中tags使用</h3>
<pre><code>- hosts: testsrv
  remote_user: root
  tags: inshttpd   针对整个playbook添加tage
  tasks:
    - name: Install httpd
      yum: name=httpd state=present
    - name: Install configure file
      copy: src=files/httpd.conf dest=/etc/httpd/conf/
      tags: rshttpd
      notify: restart httpd
  handlers:
    - name: restart httpd
      service: name=httpd status=restarted
     
ansible-playbook –t rshttpd httpd2.yml
</code></pre>
<h3 id="playbook中变量的使用">Playbook中变量的使用</h3>
<pre><code>变量名：仅能由字母、数字和下划线组成，且只能以字母开头
变量来源：
    1&gt; ansible setup facts 远程主机的所有变量都可直接调用 (系统自带变量)
       setup模块可以实现系统中很多系统信息的显示
                可以返回每个主机的系统信息包括:版本、主机名、cpu、内存
       ansible all -m setup -a 'filter=&quot;ansible_nodename&quot;'     查询主机名
       ansible all -m setup -a 'filter=&quot;ansible_memtotal_mb&quot;'  查询主机内存大小
       ansible all -m setup -a 'filter=&quot;ansible_distribution_major_version&quot;'  查询系统版本
       ansible all -m setup -a 'filter=&quot;ansible_processor_vcpus&quot;' 查询主机cpu个数
    
    2&gt; 在/etc/ansible/hosts(主机清单)中定义变量
        普通变量：主机组中主机单独定义，优先级高于公共变量(单个主机 )
        公共(组)变量：针对主机组中所有主机定义统一变量(一组主机的同一类别)
    
    3&gt; 通过命令行指定变量，优先级最高
       ansible-playbook –e varname=value
    
    4&gt; 在playbook中定义
       vars:
        - var1: value1
        - var2: value2
    
    5&gt; 在独立的变量YAML文件中定义
    
    6&gt; 在role中定义

变量命名:
    变量名仅能由字母、数字和下划线组成，且只能以字母开头

变量定义：key=value
    示例：http_port=80

变量调用方式：
    1&gt; 通过{{ variable_name }} 调用变量，且变量名前后必须有空格，有时用“{{ variable_name }}”才生效

    2&gt; ansible-playbook –e 选项指定
       ansible-playbook test.yml -e &quot;hosts=www user=ansible&quot;
</code></pre>
<pre><code>在主机清单中定义变量,在ansible中使用变量
vim /etc/ansible/hosts
[appsrvs]
192.168.38.17 http_port=817 name=www
192.168.38.27 http_port=827 name=web

调用变量
ansible appsrvs -m hostname -a'name={{name}}'  更改主机名为各自被定义的变量 

针对一组设置变量
[appsrvs:vars]
make=&quot;-&quot;

ansible appsrvs -m hostname -a 'name={{name}}{{mark}}{{http_port}}'  ansible调用变量

</code></pre>
<pre><code>将变量写进单独的配置文件中引用
vim vars.yml
pack: vsftpd
service: vsftpd

引用变量文件
vars_files:
  - vars.yml 
    
</code></pre>
<h3 id="ansible基础元素">Ansible基础元素</h3>
<pre><code>Facts：是由正在通信的远程目标主机发回的信息，这些信息被保存在ansible变量中。
       要获取指定的远程主机所支持的所有facts，可使用如下命令进行
       ansible websrvs -m setup

通过命令行传递变量
    在运行playbook的时候也可以传递一些变量供playbook使用
    示例：
        ansible-playbook test.yml -e &quot;hosts=www user=ansible&quot;
        
register
把任务的输出定义为变量，然后用于其他任务

示例:
tasks:
- shell: /usr/bin/foo
  register: foo_result
  ignore_errors: True
</code></pre>
<h3 id="示例使用setup变量">示例：使用setup变量</h3>
<pre><code>示例：var.yml
- hosts: websrvs
  remote_user: root
  tasks:
    - name: create log file
      file: name=/var/log/ {{ ansible_fqdn }} state=touch

ansible-playbook var.yml
</code></pre>
<h3 id="示例变量">示例：变量</h3>
<pre><code>示例：var.yml
- hosts: websrvs
  remote_user: root
  tasks:
    - name: install package
      yum: name={{ pkname }} state=present
      
ansible-playbook –e pkname=httpd var.yml
</code></pre>
<h3 id="示例变量-2">示例：变量</h3>
<pre><code>示例：var.yml
- hosts: websrvs
  remote_user: root
vars:
  - username: user1
  - groupname: group1
tasks:
  - name: create group
    group: name={{ groupname }} state=present
  - name: create user
    user: name={{ username }} state=present

ansible-playbook var.yml
ansible-playbook -e &quot;username=user2 groupname=group2” var2.yml

</code></pre>
<h3 id="变量">变量</h3>
<pre><code>主机变量
可以在inventory中定义主机时为其添加主机变量以便于在playbook中使用

示例：
[websrvs]
www1.ansible.com http_port=80 maxRequestsPerChild=808
www2.ansible.com http_port=8080 maxRequestsPerChild=909

组变量
组变量是指赋予给指定组内所有主机上的在playbook中可用的变量

示例：
    [websrvs]
    www1.ansible.com
    www2.ansible.com

    [websrvs:vars]
    ntp_server=ntp.ansible.com
    nfs_server=nfs.ansible.com
</code></pre>
<h3 id="示例变量-3">示例：变量</h3>
<pre><code>普通变量
    [websrvs]
    192.168.99.101 http_port=8080 hname=www1
    192.168.99.102 http_port=80 hname=www2

公共（组）变量
    [websvrs:vars]
    http_port=808
    mark=&quot;_&quot;
    [websrvs]
    192.168.99.101 http_port=8080 hname=www1
    192.168.99.102 http_port=80 hname=www2
    ansible websvrs –m hostname –a ‘name={{ hname }}{{ mark }}{{ http_port }}’

命令行指定变量：
    ansible websvrs –e http_port=8000 –m hostname –a'name={{ hname }}{{ mark }}{{ http_port }}'
</code></pre>
<h3 id="使用变量文件">使用变量文件</h3>
<pre><code>cat vars.yml
var1: httpd
var2: nginx

cat var.yml
- hosts: web
  remote_user: root
  vars_files:
    - vars.yml
  tasks:
    - name: create httpd log
      file: name=/app/{{ var1 }}.log state=touch
    - name: create nginx log
      file: name=/app/{{ var2 }}.log state=touch
      
hostname app_81.ansible.com  hostname 不支持&quot;_&quot;,认为&quot;_&quot;是非法字符
hostnamectl set-hostname app_80.ansible.com  可以更改主机名
</code></pre>
<h3 id="变量-2">变量</h3>
<pre><code>组嵌套
inventory中，组还可以包含其它的组，并且也可以向组中的主机指定变量。
这些变量只能在ansible-playbook中使用，而ansible命令不支持

示例：
    [apache]
    httpd1.ansible.com
    httpd2.ansible.com
    
    [nginx]
    ngx1.ansible.com
    ngx2.ansible.com
    
    [websrvs:children]
    apache
    nginx
    
    [webservers:vars]
    ntp_server=ntp.ansible.com
</code></pre>
<h3 id="invertory参数">invertory参数</h3>
<pre><code>invertory参数：用于定义ansible远程连接目标主机时使用的参数，而非传递给playbook的变量
    ansible_ssh_host
    ansible_ssh_port
    ansible_ssh_user
    ansible_ssh_pass
    ansbile_sudo_pass

示例：
    cat /etc/ansible/hosts
    [websrvs]
    192.168.0.1 ansible_ssh_user=root ansible_ssh_pass=ansible
    192.168.0.2 ansible_ssh_user=root ansible_ssh_pass=ansible
</code></pre>
<h3 id="invertory参数-2">invertory参数</h3>
<pre><code>inventory参数
ansible基于ssh连接inventory中指定的远程主机时，还可以通过参数指定其交互方式；
这些参数如下所示：
ansible_ssh_host
The name of the host to connect to, if different from the alias you wishto give to it.

ansible_ssh_port
The ssh port number, if not 22

ansible_ssh_user
The default ssh user name to use.

ansible_ssh_pass
The ssh password to use (this is insecure, we strongly recommendusing --ask-pass or SSH keys)

ansible_sudo_pass
The sudo password to use (this is insecure, we strongly recommendusing --ask-sudo-pass)

ansible_connection
Connection type of the host. Candidates are local, ssh or paramiko.
The default is paramiko before Ansible 1.2, and 'smart' afterwards which
detects whether usage of 'ssh' would be feasible based on whether
ControlPersist is supported.

ansible_ssh_private_key_file
Private key file used by ssh. Useful if using multiple keys and you don't want to use SSH agent.

ansible_shell_type
The shell type of the target system. By default commands are formatted
using 'sh'-style syntax by default. Setting this to 'csh' or 'fish' will cause
commands executed on target systems to follow those shell's syntax instead.

ansible_python_interpreter
The target host python path. This is useful for systems with more
than one Python or not located at &quot;/usr/bin/python&quot; such as \*BSD, or where /usr/bin/python

is not a 2.X series Python. We do not use the &quot;/usr/bin/env&quot; mechanism as that requires the remote user's

path to be set right and also assumes the &quot;python&quot; executable is named python,where the executable might

be named something like &quot;python26&quot;.
ansible\_\*\_interpreter

Works for anything such as ruby or perl and works just like ansible_python_interpreter.

This replaces shebang of modules which will run on that host.
</code></pre>
<h3 id="模板templates">模板templates</h3>
<pre><code>文本文件，嵌套有脚本（使用模板编程语言编写） 借助模板生成真正的文件
Jinja2语言，使用字面量，有下面形式
    字符串：使用单引号或双引号
    数字：整数，浮点数
    列表：[item1, item2, ...]
    元组：(item1, item2, ...)
    字典：{key1:value1, key2:value2, ...}
    布尔型：true/false
算术运算：+, -, *, /, //, %, **
比较操作：==, !=, &gt;, &gt;=, &lt;, &lt;=
逻辑运算：and，or，not
流表达式：For，If，When
</code></pre>
<h3 id="jinja2相关">Jinja2相关</h3>
<pre><code>字面量
    1&gt; 表达式最简单的形式就是字面量。字面量表示诸如字符串和数值的 Python对象。如“Hello World”
    双引号或单引号中间的一切都是字符串。
    2&gt; 无论何时你需要在模板中使用一个字符串（比如函数调用、过滤器或只是包含或继承一个模板的参数），如4242.23
    3&gt; 数值可以为整数和浮点数。如果有小数点，则为浮点数，否则为整数。在Python 里， 42 和 42.0 是不一样的
</code></pre>
<h3 id="jinja2算术运算">Jinja2:算术运算</h3>
<pre><code>算术运算
Jinja 允许你用计算值。这在模板中很少用到，但为了完整性允许其存在
支持下面的运算符
    +：把两个对象加到一起。
       通常对象是素质，但是如果两者是字符串或列表，你可以用这 种方式来衔接它们。
       无论如何这不是首选的连接字符串的方式！连接字符串见 ~ 运算符。 {{ 1 + 1 }} 等于 2
    -：用第一个数减去第二个数。 {{ 3 - 2 }} 等于 1
    /：对两个数做除法。返回值会是一个浮点数。 {{ 1 / 2 }} 等于 {{ 0.5 }}
    //：对两个数做除法，返回整数商。 {{ 20 // 7 }} 等于 2
    %：计算整数除法的余数。 {{ 11 % 7 }} 等于 4
    *：用右边的数乘左边的操作数。 {{ 2 * 2 }} 会返回 4 。
       也可以用于重 复一个字符串多次。{{ ‘=’ * 80 }} 会打印 80 个等号的横条
    **：取左操作数的右操作数次幂。 {{ 2**3 }} 会返回 8
</code></pre>
<h3 id="jinja2">Jinja2</h3>
<pre><code>比较操作符
== 比较两个对象是否相等
!= 比较两个对象是否不等
&gt; 如果左边大于右边，返回 true
&gt;= 如果左边大于等于右边，返回 true
&lt; 如果左边小于右边，返回 true
&lt;= 如果左边小于等于右边，返回 true

逻辑运算符
对于 if 语句，在 for 过滤或 if 表达式中，它可以用于联合多个表达式
and
    如果左操作数和右操作数同为真，返回 true
or
    如果左操作数和右操作数有一个为真，返回 true
not
    对一个表达式取反（见下）
(expr)
    表达式组

['list', 'of', 'objects']:
一对中括号括起来的东西是一个列表。列表用于存储和迭代序列化的数据。
例如 你可以容易地在 for循环中用列表和元组创建一个链接的列表
    &lt;ul&gt;
    {% for href, caption in [('index.html', 'Index'), ('about.html', 'About'), ('downloads.html',
'Downloads')] %}
        &lt;li&gt;&lt;a href=&quot;{{ href }}&quot;&gt;{{ caption }}&lt;/a&gt;&lt;/li&gt;
    {% endfor %}
    &lt;/ul&gt;
    ('tuple', 'of', 'values'):

元组与列表类似，只是你不能修改元组。
如果元组中只有一个项，你需要以逗号结尾它。
元组通常用于表示两个或更多元素的项。更多细节见上面的例子
    {'dict': 'of', 'key': 'and', 'value': 'pairs'}:

Python 中的字典是一种关联键和值的结构。
键必须是唯一的，并且键必须只有一个 值。
字典在模板中很少使用，罕用于诸如 xmlattr() 过滤器之类
    true / false:
    true 永远是 true ，而 false 始终是 false
</code></pre>
<h3 id="template-的使用">template 的使用</h3>
<pre><code>template功能：根据模块文件动态生成对应的配置文件
   &gt; template文件必须存放于templates目录下，且命名为 .j2 结尾
   &gt; yaml/yml 文件需和templates目录平级，目录结构如下：
    ./
     ├── temnginx.yml
     └── templates
        └── nginx.conf.j2
</code></pre>
<h3 id="template示例">template示例</h3>
<pre><code>示例：利用template 同步nginx配置文件
准备templates/nginx.conf.j2文件
vim temnginx.yml
- hosts: websrvs
  remote_user: root
  
  tasks:
    - name: template config to remote hosts
      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf

ansible-playbook temnginx.yml
</code></pre>
<h3 id="playbook中template变更替换">Playbook中template变更替换</h3>
<pre><code>修改文件nginx.conf.j2 下面行为
worker_processes {{ ansible_processor_vcpus }};

cat temnginx2.yml
- hosts: websrvs
  remote_user: root
  tasks:
    - name: template config to remote hosts
      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf

ansible-playbook temnginx2.yml
</code></pre>
<h3 id="playbook中template算术运算">Playbook中template算术运算</h3>
<pre><code>算法运算：
示例：
    vim nginx.conf.j2
    worker_processes {{ ansible_processor_vcpus**2 }};
    worker_processes {{ ansible_processor_vcpus+2 }};
</code></pre>
<h3 id="when-实现条件判断">when  实现条件判断</h3>
<pre><code>条件测试:如果需要根据变量、facts或此前任务的执行结果来做为某task执行与否的前提时要用到条件测试,
通过when语句实现，在task中使用，jinja2的语法格式

when语句
    在task后添加when子句即可使用条件测试；when语句支持Jinja2表达式语法
示例：
tasks:
  - name: &quot;shutdown RedHat flavored systems&quot;
    command: /sbin/shutdown -h now
    when: ansible_os_family == &quot;RedHat&quot;  当系统属于红帽系列,执行command模块 
 
when语句中还可以使用Jinja2的大多&quot;filter&quot;，
例如要忽略此前某语句的错误并基于其结果(failed或者success)运行后面指定的语句，
可使用类似如下形式：
tasks:
  - command: /bin/false
    register: result
    ignore_errors: True
  - command: /bin/something
    when: result|failed
  - command: /bin/something_else
    when: result|success
  - command: /bin/still/something_else
    when: result|skipped

此外，when语句中还可以使用facts或playbook中定义的变量
</code></pre>
<h3 id="示例when条件判断">示例：when条件判断</h3>
<pre><code>- hosts: websrvs
  remote_user: root
  tasks:
    - name: add group nginx
      tags: user
      user: name=nginx state=present
    - name: add user nginx
      user: name=nginx state=present group=nginx
    - name: Install Nginx
      yum: name=nginx state=present
    - name: restart Nginx
      service: name=nginx state=restarted
      when: ansible_distribution_major_version == &quot;6&quot;
</code></pre>
<h3 id="示例when条件判断-2">示例：when条件判断</h3>
<pre><code>示例：
tasks:
  - name: install conf file to centos7
    template: src=nginx.conf.c7.j2 dest=/etc/nginx/nginx.conf
    when: ansible_distribution_major_version == &quot;7&quot;
  - name: install conf file to centos6
    template: src=nginx.conf.c6.j2 dest=/etc/nginx/nginx.conf
    when: ansible_distribution_major_version == &quot;6&quot;
</code></pre>
<h3 id="playbook中when条件判断">Playbook中when条件判断</h3>
<pre><code>---
- hosts: srv120
  remote_user: root
  tasks:
    - name:
      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf
      when: ansible_distribution_major_version == &quot;7&quot;
</code></pre>
<figure data-type="image" tabindex="8"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302012029.png" alt="" loading="lazy"></figure>
<h3 id="迭代with_items">迭代：with_items</h3>
<pre><code>迭代：当有需要重复性执行的任务时，可以使用迭代机制
    &gt; 对迭代项的引用，固定变量名为&quot;item&quot;
    &gt; 要在task中使用with_items给定要迭代的元素列表
    &gt; 列表格式：
         字符串
         字典
</code></pre>
<h3 id="示例-4">示例</h3>
<pre><code>示例： 创建用户
- name: add several users
  user: name={{ item }} state=present groups=wheel   #{{ item }} 系统自定义变量
  with_items:       # 定义{{ item }} 的值和个数
    - testuser1
    - testuser2

上面语句的功能等同于下面的语句：
- name: add user testuser1
  user: name=testuser1 state=present groups=wheel
- name: add user testuser2
  user: name=testuser2 state=present groups=wheel
  
with_items中可以使用元素还可为hashes
示例：
- name: add several users
  user: name={{ item.name }} state=present groups={{ item.groups }}
  with_items:
    - { name: 'testuser1', groups: 'wheel' }
    - { name: 'testuser2', groups: 'root' }

ansible的循环机制还有更多的高级功能，具体请参见官方文档
http://docs.ansible.com/playbooks_loops.html
</code></pre>
<h3 id="示例迭代">示例：迭代</h3>
<pre><code>示例：将多个文件进行copy到被控端
---
- hosts: testsrv
  remote_user: root
  tasks
  - name: Create rsyncd config
    copy: src={{ item }} dest=/etc/{{ item }}
    with_items:
  - rsyncd.secrets
  - rsyncd.conf
</code></pre>
<h3 id="示例迭代-2">示例：迭代</h3>
<pre><code>- hosts: websrvs
  remote_user: root
  tasks:
    - name: copy file
      copy: src={{ item }} dest=/tmp/{{ item }}
      with_items:
    - file1
    - file2
    - file3
- name: yum install httpd
  yum: name={{ item }} state=present
  with_items:
    - apr
    - apr-util
    - httpd
</code></pre>
<h3 id="示例迭代-3">示例：迭代</h3>
<pre><code>- hosts：websrvs
  remote_user: root
  tasks
    - name: install some packages
      yum: name={{ item }} state=present
      with_items:
        - nginx
        - memcached
        - php-fpm
</code></pre>
<h3 id="示例迭代嵌套子变量">示例：迭代嵌套子变量</h3>
<pre><code>- hosts：websrvs
  remote_user: root
  
  tasks:
    - name: add some groups
      group: name={{ item }} state=present
      with_items:
        - group1
        - group2
        - group3
    - name: add some users
      user: name={{ item.name }} group={{ item.group }} state=present
      with_items:
        - { name: 'user1', group: 'group1' }
        - { name: 'user2', group: 'group2' }
        - { name: 'user3', group: 'group3' }
</code></pre>
<h3 id="with_itmes-嵌套子变量">with_itmes 嵌套子变量</h3>
<pre><code>with_itmes 嵌套子变量
示例
---
- hosts: testweb
  remote_user: root
  tasks:
    - name: add several users
      user: name={{ item.name }} state=present groups={{ item.groups }}
      with_items:
    - { name: 'testuser1' , groups: 'wheel'}
    - { name: 'testuser2' , groups: 'root'}
</code></pre>
<h3 id="playbook字典-with_items">Playbook字典 with_items</h3>
<pre><code>- name: 使用ufw模块来管理哪些端口需要开启
  ufw:
  rule: “{{ item.rule }}”
  port: “{{ item.port }}”
  proto: “{{ item.proto }}”
  with_items:
    - { rule: 'allow', port: 22, proto: 'tcp' }
    - { rule: 'allow', port: 80, proto: 'tcp' }
    - { rule: 'allow', port: 123, proto: 'udp' }

- name: 配置网络进出方向的默认规则
  ufw:
  direction: &quot;{{ item.direction }}&quot;
  policy: &quot;{{ item.policy }}&quot;
  state: enabled
  with_items:
    - { direction: outgoing, policy: allow }
    - { direction: incoming, policy: deny }
</code></pre>
<h3 id="playbook中template-for-if-when循环">Playbook中template for if  when循环</h3>
<pre><code>{% for vhost in nginx_vhosts %}

server {    #重复执行server代码
listen {{ vhost.listen | default('80 default_server') }};

{% if vhost.server_name is defined %}
server_name {{ vhost.server_name }};
{% endif %}

{% if vhost.root is defined %}
root {{ vhost.root }};
{% endif %}

{% endfor %}
</code></pre>
<h3 id="示例-5">示例</h3>
<pre><code>// temnginx.yml
---
- hosts: testweb
  remote_user: root
  vars:      # 调用变量
    nginx_vhosts:
      - listen: 8080  #列表 键值对


//templates/nginx.conf.j2
{% for vhost in nginx_vhosts %}  
server {
  listen {{ vhost.listen }}
}
{% endfor %}

生成的结果
server {
  listen 8080
}
</code></pre>
<h3 id="示例-6">示例</h3>
<pre><code>// temnginx.yml
---
- hosts: ansibleweb
  remote_user: root
  vars:
    nginx_vhosts:
      - web1
      - web2
      - web3
  tasks:
    - name: template config
      template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf

// templates/nginx.conf.j2
{% for vhost in nginx_vhosts %}
server {
    listen {{ vhost }}
}
{% endfor %}

生成的结果：
server {
    listen web1
}
server {
    listen web2
}
server {
    listen web3
}
</code></pre>
<h3 id="roles">roles</h3>
<pre><code>roles
    ansible自1.2版本引入的新特性，用于层次性、结构化地组织playbook。
    roles能够根据层次型结构自动装载变量文件、tasks以及handlers等。
    要使用roles只需要在playbook中使用include指令即可。
    简单来讲，roles就是通过分别将变量、文件、任务、模板及处理器放置于单独的目录中，
    并可以便捷地include它们的一种机制。
    角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中

复杂场景：建议使用roles，代码复用度高
    变更指定主机或主机组
    如命名不规范维护和传承成本大
    某些功能需多个Playbook，通过includes即可实现
</code></pre>
<h3 id="roles-2">Roles</h3>
<pre><code>角色(roles)：角色集合
roles/
    mysql/
    httpd/
    nginx/
    memcached/
    
可以互相调用
</code></pre>
<h3 id="ansible-roles目录编排">Ansible Roles目录编排</h3>
<figure data-type="image" tabindex="9"><img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302012051.png" alt="" loading="lazy"></figure>
<h3 id="roles目录结构">roles目录结构</h3>
<pre><code>每个角色，以特定的层级目录结构进行组织
roles目录结构：

playbook.yml  调用角色
roles/
  project/ (角色名称)
    tasks/
    files/
    vars/
    templates/
    handlers/
    default/ 不常用
    meta/    不常用
</code></pre>
<h3 id="roles各目录作用">Roles各目录作用</h3>
<pre><code>/roles/project/ :项目名称,有以下子目录
    files/ ：存放由copy或script模块等调用的文件
    templates/：template模块查找所需要模板文件的目录
    tasks/：定义task,role的基本元素，至少应该包含一个名为main.yml的文件；
            其它的文件需要在此文件中通过include进行包含
    handlers/：至少应该包含一个名为main.yml的文件；
               其它的文件需要在此文件中通过include进行包含
    vars/：定义变量，至少应该包含一个名为main.yml的文件；
           其它的文件需要在此文件中通过include进行包含
    meta/：定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为main.yml的文件，
           其它文件需在此文件中通过include进行包含
    default/：设定默认变量时使用此目录中的main.yml文件
    
roles/appname 目录结构
    tasks目录：至少应该包含一个名为main.yml的文件，其定义了此角色的任务列表；
               此文件可以使用include包含其它的位于此目录中的task文件
    files目录：存放由copy或script等模块调用的文件；
    templates目录：template模块会自动在此目录中寻找Jinja2模板文件
    handlers目录：此目录中应当包含一个main.yml文件，用于定义此角色用到的各handler；
                  在handler中使用include包含的其它的handler文件也应该位于此目录中；
    vars目录：应当包含一个main.yml文件，用于定义此角色用到的变量；
    meta目录：应当包含一个main.yml文件，用于定义此角色的特殊设定及其依赖关系；
              ansible1.3及其以后的版本才支持；
    default目录：为当前角色设定默认变量时使用此目录；应当包含一个main.yml文件

roles/example_role/files/             所有文件，都将可存放在这里
roles/example_role/templates/         所有模板都存放在这里
roles/example_role/tasks/main.yml：   主函数，包括在其中的所有任务将被执行
roles/example_role/handlers/main.yml：所有包括其中的 handlers 将被执行
roles/example_role/vars/main.yml：    所有包括在其中的变量将在roles中生效
roles/example_role/meta/main.yml：    roles所有依赖将被正常登入

</code></pre>
<h3 id="创建role">创建role</h3>
<pre><code>创建role的步骤
(1) 创建以roles命名的目录
(2) 在roles目录中分别创建以各角色名称命名的目录，如webservers等
(3) 在每个角色命名的目录中分别创建files、handlers、meta、tasks、templates和vars目录；
    用不到的目录可以创建为空目录，也可以不创建
(4) 在playbook文件中，调用各角色
</code></pre>
<h3 id="实验-创建httpd角色">实验: 创建httpd角色</h3>
<pre><code>1&gt; 创建roles目录
   mkdir roles/{httpd,mysql,redis}/tasks -pv
   mkdir  roles/httpd/{handlers,files}

查看目录结构
tree roles/
    roles/
    ├── httpd
    │   ├── files
    │   ├── handlers
    │   └── tasks
    ├── mysql
    │   └── tasks
    └── redis
        └── tasks

2&gt; 创建目标文件
   cd roles/httpd/tasks/
   touch install.yml config.yml service.yml

3&gt; vim install.yml
   - name: install httpd package
     yum: name=httpd
     
   vim config.yml
   - name: config file  
     copy: src=httpd.conf dest=/etc/httpd/conf/ backup=yes 
   
   vim service.yml
   - name: start service 
     service: name=httpd state=started enabled=yes
     
4&gt; 创建main.yml主控文件,调用以上单独的yml文件,
   main.yml定义了谁先执行谁后执行的顺序
   vim main.yml
   - include: install.yml
   - include: config.yml
   - include: service.yml
   
5&gt; 准备httpd.conf文件,放到httpd单独的文件目录下
   cp /app/ansible/flies/httpd.conf ../files/
   
6&gt; 创建一个网页
   vim flies/index.html
   &lt;h1&gt; welcome to weixiaodong home &lt;\h1&gt;

7&gt; 创建网页的yml文件
   vim tasks/index.yml
   - name: index.html
     copy: src=index.html dest=/var/www/html 

8&gt; 将网页的yml文件写进mian.yml文件中
   vim mian.yml
   - include: install.yml
   - include: config.yml
   - include: index.yml
   - include: service.yml

9&gt; 在handlers目录下创建handler文件mian.yml
   vim handlers/main.yml
   - name: restart service httpd
     service: name=httpd state=restarted

10&gt; 创建文件调用httpd角色
    cd /app/ansidle/roles
    vim role_httpd.yml
    ---
    # httpd role
    - hosts: appsrvs
      remote_user: root 

      roles:       #调用角色
        - role: httpd  
        
11&gt; 查看目录结构
    tree 
    .
    httpd
    ├── files
    │   ├── httpd.conf
    │   └── index.html
    ├── handlers
    │   └── main.yml
    └── tasks
        ├── config.yml
        ├── index.yml
        ├── install.yml
        ├── main.yml
        └── service.yml

12&gt; ansible-playbook role_httpd.yml
</code></pre>
<h3 id="针对大型项目使用roles进行编排">针对大型项目使用Roles进行编排</h3>
<pre><code>roles目录结构：
playbook.yml
roles/
  project/
    tasks/
    files/
    vars/
    templates/
    handlers/
    default/ # 不经常用
    meta/    # 不经常用

示例：
nginx-role.yml
roles/
└── nginx
    ├── files
    │ └── main.yml
    ├── tasks
    │ ├── groupadd.yml
    │ ├── install.yml
    │ ├── main.yml
    │ ├── restart.yml
    │ └── useradd.yml
    └── vars
        └── main.yml
</code></pre>
<h3 id="示例-7">示例</h3>
<pre><code>roles的示例如下所示：
site.yml
webservers.yml
dbservers.yml
roles/
  common/
    files/
    templates/
    tasks/
    handlers/
    vars/
    meta/
  webservers/
    files/
    templates/
    tasks/
  handlers/
    vars/
    meta/
</code></pre>
<h3 id="实验-创建一个nginx角色">实验： 创建一个nginx角色</h3>
<pre><code>建立nginx角色在多台主机上来部署nginx需要安装 创建账号
1&gt; 创建nginx角色目录
     cd /app/ansible/role
     mkdir nginx{tesks,templates,hanslers} -pv

2&gt; 创建任务目录
     cd tasks/
     touch insatll.yml config.yml service.yml file.yml user.yml
   创建main.yml文件定义任务执行顺序
     vim main.yml
     - include: user.yml
     - include: insatll.yml
     - include: config.yml
     - include: file.yml
     - include: service.yml

  
3&gt; 准备配置文件(centos7、8)
   ll /app/ansible/role/nginx/templates/
   nginx7.conf.j2
   nginx8.conf.j2


4&gt; 定义任务
   vim tasks/install.yml
   - name: install
     yum: name=nginx
     
   vim tasks/config.yml
    - name: config file
      template: src=nginx7.conf.j2 dest=/etc/nginx/nginx.conf
      when: ansible_distribution_major_version==&quot;7&quot;
      notify: restrat
      
    - name: config file
      template: src=nginx8.conf.j2 dest=/etc/nginx/nginx.conf
      when: ansible_distribution_major_version==&quot;8&quot;
      notify: restrat
      
    vim tasks/file.yml   跨角色调用file.yum文件,实现文件复用
    - name: index.html
      copy: src=roles/httpd/files/index.html dest=/usr/share/nginx/html/ 
   
    vim tasks/service.yml
    - nmae: start service
      service: name=nginx state=started enabled=yes
      
    vim handlers/main.yml
    - name: restrat
      service: name=nginx state=restarted
      
    vim roles/role_nginix.yml
    --- 
    #test rcle
    - hosts: appsrvs
    
      roles: 
        - role: nginx
        
5&gt; 测试安装
   ansible-playbook role_nginx.yml
</code></pre>
<h3 id="roles案例">Roles案例</h3>
<p>Roles目录编排<br>
<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302012146.png" alt="" loading="lazy"><br>
Playbook中调用<br>
<img src="https://raw.githubusercontent.com/OrochW/picGo/master/20230302012156.png" alt="" loading="lazy"></p>
<h3 id="playbook调用角色">playbook调用角色</h3>
<pre><code>调用角色方法1：
- hosts: websrvs
  remote_user: root
  
  roles:
    - mysql
    - memcached
    - nginx
    
调用角色方法2：
传递变量给角色
- hosts:
  remote_user:
  roles:
    - mysql
    - { role: nginx, username: nginx }   #不同的角色调用不同的变量  
    键role用于指定角色名称
    后续的k/v用于传递变量给角色

调用角色方法3：还可基于条件测试实现角色调用
roles:
  - { role: nginx, username: nginx, when: ansible_distribution_major_version == '7' }
</code></pre>
<h3 id="通过roles传递变量">通过roles传递变量</h3>
<pre><code>通过roles传递变量
当给一个主机应用角色的时候可以传递变量，然后在角色内使用这些变量
示例：
- hosts: webservers
  roles:
    - common
    - { role: foo_app_instance, dir: '/web/htdocs/a.com', port: 8080 }
</code></pre>
<h3 id="向roles传递参数">向roles传递参数</h3>
<pre><code>而在playbook中，可以这样使用roles：
---
- hosts: webservers
  roles:
    - common
    - webservers

也可以向roles传递参数
示例：
---
- hosts: webservers
  roles:
    - common
    - { role: foo_app_instance, dir: '/opt/a', port: 5000 }
    - { role: foo_app_instance, dir: '/opt/b', port: 5001 }
</code></pre>
<h3 id="条件式地使用roles">条件式地使用roles</h3>
<pre><code>甚至也可以条件式地使用roles
示例：
---
- hosts: webservers
  roles:
    - { role: some_role, when: &quot;ansible_os_family == 'RedHat'&quot; }
</code></pre>
<h3 id="roles条件及变量等案例">Roles条件及变量等案例</h3>
<pre><code>When条件
    roles:
      - {role: nginx, when: &quot;ansible_distribution_major_version == '7' &quot; ,username: nginx }
变量调用
- hosts: zabbix-proxy
  sudo: yes
  roles:
    - { role: geerlingguy.php-mysql }
    - { role: dj-wasabi.zabbix-proxy, zabbix_server_host: 192.168.37.167 }
</code></pre>
<h3 id="完整的roles架构">完整的roles架构</h3>
<pre><code>// nginx-role.yml 顶层任务调用yml文件
---
- hosts: testweb
  remote_user: root
  roles:
    - role: nginx
    - role: httpd 可执行多个role

cat roles/nginx/tasks/main.yml
---
- include: groupadd.yml
- include: useradd.yml
- include: install.yml
- include: restart.yml
- include: filecp.yml

// roles/nginx/tasks/groupadd.yml
---
- name: add group nginx
  user: name=nginx state=present

cat roles/nginx/tasks/filecp.yml
---
- name: file copy
  copy: src=tom.conf dest=/tmp/tom.conf

以下文件格式类似：
useradd.yml,install.yml,restart.yml

ls roles/nginx/files/
tom.conf
</code></pre>
<h3 id="roles-playbook-tags使用">roles playbook tags使用</h3>
<pre><code>roles playbook tags使用
    ansible-playbook --tags=&quot;nginx,httpd,mysql&quot; nginx-role.yml  对标签进行挑选执行

// nginx-role.yml
---
- hosts: testweb
  remote_user: root
  roles:
    - { role: nginx ,tags: [ 'nginx', 'web' ] ,when: ansible_distribution_major_version == &quot;6“ }
    - { role: httpd ,tags: [ 'httpd', 'web' ] }
    - { role: mysql ,tags: [ 'mysql', 'db' ] }
    - { role: marridb ,tags: [ 'mysql', 'db' ] }
    - { role: php }
</code></pre>
<h3 id="实验-创建角色memcached">实验: 创建角色memcached</h3>
<pre><code>memcacched 当做缓存用,会在内存中开启一块空间充当缓存
cat /etc/sysconfig/memcached 
    PORT=&quot;11211&quot;
    USER=&quot;memcached&quot;
    MAXCONN=&quot;1024&quot;
    CACHESIZE=&quot;64&quot;    # 缓存空间默认64M 
    OPTIONS=&quot;&quot;


1&gt; 创建对用目录
   cd /app/ansible
   mkdir roles/memcached/{tasks,templates} -pv
   
2&gt; 拷贝memcached配置文件模板
   cp /etc/sysconfig/memcached  templates/memcached.j2
   vim templates/memcached.j2
   CACHESIZE=&quot;{{ansible_memtotal_mb//4}}&quot;   #物理内存的1/4用做缓存
   
3&gt; 创建对应yml文件,并做相应配置
   cd tasks/
   touch install.yml config.yml service.yml
   创建main.yml文件定义任务执行顺序
   vim main.yml
   - include: install.yml
   - include: config.yml
   - include: service.yml  
   
   vim install.yml
   - name: install 
     yum: name=memcached
     
   vim config.yml
   - name: config file
     template: src=memcached.j2 dets=/etc/sysconfig/memcached

   vim service.yml
   - name: service
     service: name=memcached state=started enabled=yes

4&gt; 创建调用角色文件
   cd /app/ansible/roles/
   vim role_memcached.yml
    ---
    - hosts: appsrvs
    
      roles: 
        - role: memcached

5&gt; 安装
   ansible-playbook  role_memcached.yml 
   memcached端口号11211
</code></pre>
<h3 id="其它功能">其它功能</h3>
<pre><code>委任（指定某一台机器做某一个task）
    delegate_to
    local_action (专指针对ansible命令执行的机器做的变更操作)
交互提示
    prompt
*暂停（java）
    wait_for
Debug
    debug: msg=&quot;This always executes.&quot;
Include
Template 多值合并
Template 动态变量配置
</code></pre>
<h3 id="ansible-roles">Ansible Roles</h3>
<pre><code>委任
    delegate_to
交互提示
    prompt
暂停
    wait_for
Debug
    debug: msg=&quot;This always executes.&quot;
Include
Template 多值合并
Template 动态变量配置
</code></pre>
<h3 id="推荐资料">推荐资料</h3>
<pre><code>http://galaxy.ansible.com
https://galaxy.ansible.com/explore#/
http://github.com/
http://ansible.com.cn/
https://github.com/ansible/ansible
https://github.com/ansible/ansible-examples
</code></pre>
<h3 id="实验-实现二进制安装mysql的卸载">实验: 实现二进制安装mysql的卸载</h3>
<pre><code>cat remove_mysql.yml 
---
# install mariadb server 
- hosts: appsrvs:!192.168.38.108
  remote_user: root

  tasks:
    - name: stop service 
      shell: /etc/init.d/mysqld stop
    - name: delete user 
      user: name=mysql state=absent remove=yes
    - name: delete
      file: path={{item}} state=absent
      with_items: 
        - /usr/local/mysql
        - /usr/local/mariadb-10.2.27-linux-x86_64
        - /etc/init.d/mysqld
        - /etc/profile.d/mysql.sh
        - /etc/my.cnf
        - /data/mysql

ansible-playbook  remove_mysql.yml
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s centos7系统初始化]]></title>
        <id>https://orochw.github.io/post/k8s-centos7-xi-tong-chu-shi-hua/</id>
        <link href="https://orochw.github.io/post/k8s-centos7-xi-tong-chu-shi-hua/">
        </link>
        <updated>2023-02-23T08:32:51.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1设置系统主机名以及-host-文件的相互解析">1.设置系统主机名以及 Host 文件的相互解析</h2>
<pre><code class="language-bash">hostnamectl  set-hostname  k8s-master01
</code></pre>
<h2 id="2安装依赖包">2.安装依赖包</h2>
<pre><code class="language-bash">yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wgetvimnet-tools git
</code></pre>
<h2 id="3设置防火墙为-iptables-并设置空规则">3.设置防火墙为 Iptables 并设置空规则</h2>
<pre><code class="language-bash">systemctl  stop firewalld  &amp;&amp;  systemctl  disable firewalldyum -y install iptables-services  &amp;&amp;  systemctl  start iptables  &amp;&amp;  systemctl  enable iptables&amp;&amp;  iptables -F  &amp;&amp;  service iptables save
</code></pre>
<h2 id="4关闭-selinux">4.关闭 SELINUX</h2>
<pre><code class="language-bash">swapoff -a &amp;&amp; sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstabsetenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
</code></pre>
<h2 id="5调整内核参数对于-k8s">5.调整内核参数，对于 K8S</h2>
<pre><code class="language-bash">cat &gt; kubernetes.conf &lt;&lt;EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
net.ipv4.tcp_tw_recycle=0
vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它
vm.overcommit_memory=1 # 不检查物理内存是否够用
vm.panic_on_oom=0 # 开启 OOM
fs.inotify.max_user_instances=8192
fs.inotify.max_user_watches=1048576
fs.file-max=52706963fs.nr_open=52706963
net.ipv6.conf.all.disable_ipv6=1
net.netfilter.nf_conntrack_max=2310720
EOF
cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf
sysctl -p /etc/sysctl.d/kubernetes.conf
</code></pre>
<h2 id="6调整系统时区">6.调整系统时区</h2>
<h1 id="设置系统时区为中国上海">设置系统时区为中国/上海</h1>
<pre><code class="language-bash">timedatectl set-timezone Asia/Shanghai
</code></pre>
<h2 id="7将当前的-utc-时间写入硬件时钟">7.将当前的 UTC 时间写入硬件时钟</h2>
<pre><code class="language-bash">timedatectl set-local-rtc 0
</code></pre>
<h2 id="8重启依赖于系统时间的服务">8.重启依赖于系统时间的服务</h2>
<pre><code class="language-bash">systemctl restart rsyslog
systemctl restart crond
</code></pre>
<h2 id="9关闭系统不需要服务">9.关闭系统不需要服务</h2>
<pre><code>systemctl stop postfix &amp;&amp; systemctl disable postfix
</code></pre>
<h2 id="10设置-rsyslogd-和-systemd-journald">10.设置 rsyslogd 和 systemd journald</h2>
<pre><code>mkdir /var/log/journal 
</code></pre>
<h2 id="11持久化保存日志的目录">11.持久化保存日志的目录</h2>
<pre><code class="language-bash">mkdir /etc/systemd/journald.conf.d
cat &gt; /etc/systemd/journald.conf.d/99-prophet.conf &lt;&lt;EOF
[Journal]
# 持久化保存到磁盘Storage=persistent
# 压缩历史日志Compress=yesSyncIntervalSec=5m
RateLimitInterval=30sRateLimitBurst=1000
# 最大占用空间 10GSystemMaxUse=10G
# 单日志文件最大 200MSystemMaxFileSize=200M
# 日志保存时间 2 周MaxRetentionSec=2week
# 不将日志转发到 
syslogForwardToSyslog=no
EOF
systemctl restart systemd-journald
</code></pre>
<h2 id="12升级系统内核为-444">12.升级系统内核为 4.44</h2>
<p>升级系统内核为 4.44CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定，例如： rpm -Uvh<br>
http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</p>
<pre><code class="language-bash">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
</code></pre>
<p>安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！</p>
<pre><code class="language-bash">yum --enablerepo=elrepo-kernel install -y kernel-lt
</code></pre>
<p>设置开机从新内核启动</p>
<pre><code class="language-bash">grub2-set-default 'CentOS Linux (4.4.189-1.el7.elrepo.x86_64) 7 (Core)'
</code></pre>
]]></content>
    </entry>
</feed>